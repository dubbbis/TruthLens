{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Code\n",
    "\n",
    "1Ô∏è‚É£ **Fetch news from Mediastack**  \n",
    "   - Fetches articles based on your API key and fetch limit (e.g., 10 articles).\n",
    "\n",
    "2Ô∏è‚É£ **Check for paywalled articles**  \n",
    "   - Skips articles from known paywalled domains (e.g., New York Times).\n",
    "\n",
    "3Ô∏è‚É£ **Extract full article text**  \n",
    "   - Attempts to extract text using `newspaper3k`, `Unstructured`, and `BeautifulSoup`.\n",
    "\n",
    "4Ô∏è‚É£ **Store articles in JSON**  \n",
    "   - Saves the articles in a JSON file (`news.json`).\n",
    "\n",
    "5Ô∏è‚É£ **Convert text to embeddings**  \n",
    "   - Uses the `SentenceTransformer` to generate embeddings for each article's text.\n",
    "\n",
    "6Ô∏è‚É£ **Store embeddings in ChromaDB**  \n",
    "   - Adds the generated embeddings into ChromaDB for semantic search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to install\n",
    "\n",
    "pip install fake-useragent && pip install newspaper3k && pip install lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç [1/10] Processing: https://www.ndtvprofit.com/technology/gen-ai-adoption-not-moving-at-speed-of-technology-agentic-ai-no-silver-bullet-finds-deloitte-survey\n",
      "üîç [2/10] Processing: https://www.wral.com/story/mules-that-provided-aid-after-hurricane-helene-struck-down-on-road/21863436/\n",
      "üîç [3/10] Processing: https://www.argophilia.com/news/lavrio-agios-efstratios-lemnos-kavala/240341/\n",
      "üîç [4/10] Processing: https://www.scotsman.com/recommended/oodie-has-just-launched-football-based-jackets-blankets-and-coats-4963229\n",
      "üîç [5/10] Processing: https://www.burnleyexpress.net/recommended/oodie-has-just-launched-football-based-jackets-blankets-and-coats-4963229\n",
      "üîç [6/10] Processing: https://www.deccanchronicle.com/sports/australian-team-arrives-in-pakistan-for-champions-trophy-1861721\n",
      "üîç [7/10] Processing: https://www.wigantoday.net/recommended/oodie-has-just-launched-football-based-jackets-blankets-and-coats-4963229\n",
      "üîç [8/10] Processing: https://famagusta-gazette.com/over-2800-traffic-violations-recorded-in-cyprus-over-just-six-days/\n",
      "üîç [9/10] Processing: https://www.zeebiz.com/personal-finance/news-return-comparison-sip-vs-ppf-which-investment-can-build-larger-corpus-for-rs-130000-annual-contribution-calculations-inside-347543\n",
      "üîç [10/10] Processing: https://www.ft.com/content/9fb2c865-8183-4b2c-86a0-bd55e86728da\n",
      "‚úÖ Articles saved in 'news.json'.\n",
      "üîÑ Converting articles to embeddings and storing them in ChromaDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: https://famagusta-gazette.com/over-2800-traffic-violations-recorded-in-cyprus-over-just-six-days/\n",
      "Insert of existing embedding ID: https://famagusta-gazette.com/over-2800-traffic-violations-recorded-in-cyprus-over-just-six-days/\n",
      "Add of existing embedding ID: https://www.zeebiz.com/personal-finance/news-return-comparison-sip-vs-ppf-which-investment-can-build-larger-corpus-for-rs-130000-annual-contribution-calculations-inside-347543\n",
      "Insert of existing embedding ID: https://www.zeebiz.com/personal-finance/news-return-comparison-sip-vs-ppf-which-investment-can-build-larger-corpus-for-rs-130000-annual-contribution-calculations-inside-347543\n",
      "Add of existing embedding ID: https://www.ft.com/content/9fb2c865-8183-4b2c-86a0-bd55e86728da\n",
      "Insert of existing embedding ID: https://www.ft.com/content/9fb2c865-8183-4b2c-86a0-bd55e86728da\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Articles converted to embeddings and stored in ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unstructured.partition.html import partition_html\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from newspaper import Article\n",
    "\n",
    "# üîπ CONFIGURATION: Define the Mediastack API and the number of articles to fetch\n",
    "API_KEY = \"356bb7cd80f02083d604ba6ba1dfadd8\"\n",
    "MAX_ARTICLES = 10  # You can change this to 5, 10, etc.\n",
    "\n",
    "# Mediastack Base URL\n",
    "BASE_URL = f\"http://api.mediastack.com/v1/news?access_key={API_KEY}&countries=us&limit={MAX_ARTICLES}\"\n",
    "\n",
    "# List of known paywalled domains (to avoid scraping content)\n",
    "paywalled_domains = [\"nytimes.com\", \"washingtonpost.com\", \"theatlantic.com\", \"bloomberg.com\"]\n",
    "\n",
    "# User-Agent Rotator\n",
    "ua = UserAgent()\n",
    "\n",
    "def is_paywalled(url):\n",
    "    \"\"\"Check if the article is from a paywalled domain.\"\"\"\n",
    "    return any(domain in url for domain in paywalled_domains)\n",
    "\n",
    "def extract_full_text(url):\n",
    "    \"\"\"Extract full article text using newspaper3k, Unstructured, and BeautifulSoup.\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        page = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        if page.status_code != 200:\n",
    "            return f\"Error: Page returned status code {page.status_code}\"\n",
    "\n",
    "        # Attempt 1: newspaper3k (best for full-text extraction)\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        if len(article.text) > 500:\n",
    "            return article.text\n",
    "\n",
    "        # Attempt 2: Unstructured (fallback)\n",
    "        elements = partition_html(text=page.text)\n",
    "        extracted_text = \"\\n\".join([el.text for el in elements if el.text.strip()])\n",
    "        if len(extracted_text) > 500:\n",
    "            return extracted_text\n",
    "\n",
    "        # Attempt 3: BeautifulSoup (last resort)\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        extracted_text = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "        return extracted_text if len(extracted_text) > 500 else \"Content could not be extracted.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting content: {str(e)}\"\n",
    "\n",
    "# üîπ Fetch news from Mediastack\n",
    "response = requests.get(BASE_URL)\n",
    "news_data = response.json().get(\"data\", [])[:MAX_ARTICLES]  # Limit articles\n",
    "\n",
    "articles_list = []\n",
    "\n",
    "# üîπ Process each article\n",
    "for i, article in enumerate(news_data):\n",
    "    url = article.get(\"url\", \"\")\n",
    "\n",
    "    if not url or is_paywalled(url):\n",
    "        print(f\"üö´ Skipping paywalled article: {url}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üîç [{i+1}/{MAX_ARTICLES}] Processing: {url}\")\n",
    "    full_text = extract_full_text(url)\n",
    "\n",
    "    articles_list.append({\n",
    "        \"title\": article.get(\"title\", \"Unknown title\"),\n",
    "        \"url\": url,\n",
    "        \"content\": full_text\n",
    "    })\n",
    "\n",
    "    time.sleep(2)  # Avoid being blocked by rate limits\n",
    "\n",
    "# üîπ Save articles in JSON format\n",
    "with open(\"news.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles_list, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Articles saved in 'news.json'.\")\n",
    "\n",
    "# üîπ INTEGRATION WITH CHROMADB (Embeddings)\n",
    "print(\"üîÑ Converting articles to embeddings and storing them in ChromaDB...\")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_or_create_collection(\"news_articles\")\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert each article to embeddings and store them in ChromaDB\n",
    "for article in articles_list:\n",
    "    text = article[\"title\"] + \" \" + article[\"content\"]\n",
    "    embedding = embedding_model.encode(text).tolist()\n",
    "\n",
    "    collection.add(\n",
    "        ids=[article[\"url\"]],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[{\"title\": article[\"title\"], \"url\": article[\"url\"]}],\n",
    "        documents=[text]\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Articles converted to embeddings and stored in ChromaDB.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
