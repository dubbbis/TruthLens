{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def construct_fact_checking_prompt(enriched_entry):\n",
    "    \"\"\"\n",
    "    Constructs a well-formatted prompt for fact-checking using DeepSeek LLM.\n",
    "    \n",
    "    Args:\n",
    "        enriched_entry (dict): A dictionary containing article metadata, enriched content, and linguistic analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string prompt for the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract relevant fields from enriched_entry\n",
    "    title = enriched_entry.get(\"title\", \"Unknown Title\")\n",
    "    url = enriched_entry.get(\"url\", \"Unknown URL\")\n",
    "    published_date = enriched_entry.get(\"published_date\", \"Unknown Date\")\n",
    "    source_name = enriched_entry.get(\"source\", \"Unknown Source\")\n",
    "    author = enriched_entry.get(\"author\", \"Unknown Author\")\n",
    "    category = enriched_entry.get(\"category\", \"Unknown Category\")\n",
    "    summary = enriched_entry.get(\"enriched_content\", \"No summary available\")[:500]  # Truncate if too long\n",
    "    \n",
    "    tfidf_outliers = json.loads(enriched_entry.get(\"TF-IDF Outliers\", \"[]\"))  # Convert back to list\n",
    "    tfidf_outliers_str = \", \".join(tfidf_outliers) if tfidf_outliers else \"None\"\n",
    "    \n",
    "    grammar_errors = enriched_entry.get(\"Grammar Errors\", 0)\n",
    "    sentence_count = enriched_entry.get(\"Sentence Count\", 0)\n",
    "    sentiment_polarity = enriched_entry.get(\"Sentiment Polarity\", \"Unknown\")\n",
    "    sentiment_subjectivity = enriched_entry.get(\"Sentiment Subjectivity\", \"Unknown\")\n",
    "    fact_checking_summary = enriched_entry.get(\"fact_checking_summary\", \"No fact-checking data available.\")\n",
    "    \n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
    "    \n",
    "    üì∞ **Article Information:**\n",
    "    - **Title:** {title}\n",
    "    - **URL:** {url}\n",
    "    - **Published Date:** {published_date}\n",
    "    - **Source:** {source_name}\n",
    "    - **Author:** {author}\n",
    "    - **Category:** {category}\n",
    "\n",
    "    üîπ **Article Summary (Extracted via AI):**\n",
    "    \"{summary}\"\n",
    "    \n",
    "    üìä **Linguistic Analysis:**\n",
    "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** {tfidf_outliers_str}\n",
    "    - **Grammar Issues:** {grammar_errors} errors\n",
    "    - **Sentence Count:** {sentence_count}\n",
    "    - **Sentiment Analysis:** \n",
    "        - **Polarity (Scale -1 to 1):** {sentiment_polarity}\n",
    "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** {sentiment_subjectivity}\n",
    "\n",
    "    üéØ **Your Task:**\n",
    "    1Ô∏è‚É£ **Assess the credibility of this article** based on the provided content and linguistic analysis.  \n",
    "    2Ô∏è‚É£ **Use the TF-IDF outlier words** to determine if the article contains **unusual phrasing or misleading language.**  \n",
    "    3Ô∏è‚É£ **Analyze sentiment:** Does the emotional tone suggest bias, fear-mongering, or objectivity?  \n",
    "    4Ô∏è‚É£ **Evaluate readability & grammar:** Is the article professionally written, or does it contain errors typical of misinformation?  \n",
    "    5Ô∏è‚É£ **Compare against reliable sources** if possible, to determine factual accuracy.  \n",
    "    \n",
    "    üèÜ **Final Response Format:**\n",
    "    - **Credibility Score:** (Scale 0-100, where 100 = totally credible, 0 = completely false)\n",
    "    - **Verdict:** (Choose one: \"True\", \"False\", or \"Misleading\")\n",
    "    - **Explanation:** (2-3 sentences summarizing why you assigned this rating)\n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example Usage\n",
    "example_entry = {\n",
    "    \"title\": \"Breaking News: AI Solves World Hunger\",\n",
    "    \"url\": \"https://news.example.com/ai-hunger\",\n",
    "    \"published_date\": \"2025-02-18\",\n",
    "    \"source\": \"Example News\",\n",
    "    \"author\": \"John Doe\",\n",
    "    \"category\": \"Technology\",\n",
    "    \"enriched_content\": \"AI has made significant advancements... (summary content here)...\\n\\nFact-Checking Data:\\n- Verified by multiple sources\",\n",
    "    \"TF-IDF Outliers\": json.dumps([\"AI\", \"breakthrough\", \"hunger crisis\"]),\n",
    "    \"Grammar Errors\": 2,\n",
    "    \"Sentence Count\": 25,\n",
    "    \"Sentiment Polarity\": 0.7,\n",
    "    \"Sentiment Subjectivity\": 0.4,\n",
    "    \"fact_checking_summary\": \"Verified by multiple sources.\"\n",
    "}\n",
    "\n",
    "# Generate the prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "print(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, let's tackle this fact-checking task. I'm looking at the provided article and its analysis. The article claims that AI has solved world hunger, which is a pretty bold statement. First, I should assess the credibility based on the content.\n",
      "\n",
      "The summary says AI has made significant advancements, but without specific details or sources backing up these claims, it's hard to take it at face value. News outlets often exaggerate for drama, so this could be a case of sensationalism.\n",
      "\n",
      "Looking at the linguistic analysis: there are TF-IDF outlier keywords like \"AI,\" \"breakthrough,\" and \"hunger crisis.\" These seem common in tech articles but don't necessarily indicate anything unusual on their own. The grammar has 2 errors‚Äîmaybe they were missed by an initial check but possible for a human proofreader.\n",
      "\n",
      "The article has 25 sentences, which seems lengthy, but without citations or context, it's just a string of statements. The sentiment is positive with a polarity of 0.7 and subjectivity of 0.4, suggesting the author is optimistic or opinionated‚Äîcommon in tech articles trying to grab attention.\n",
      "\n",
      "Readability-wise, the article uses technical terms like \"AI\" without much explanation, which could make it seem more credible but might also be confusing for some readers. The mix of technical jargon and sweeping claims about AI solving hunger might indicate a lack of depth or peer review.\n",
      "\n",
      "Comparing against reliable sources, I don't have access to them here, but based on the information given, the article lacks specific evidence and seems to rely more on buzzwords than factual data. The use of \"breakthrough\" without supporting statistics weakens its credibility.\n",
      "\n",
      "Overall, while the article is professionally written in structure with some grammar issues, the lack of concrete evidence and sensational language makes it likely misleading or based on hearsay.\n",
      "</think>\n",
      "\n",
      "**Final Response:**\n",
      "\n",
      "- **Credibility Score:** 20\n",
      "- **Verdict:** False/Misleading\n",
      "- **Explanation:** The article's claim that AI has solved world hunger is unsupported by specific details, relying instead on broad and sensational language. While the structure appears professional with proper grammar, the lack of citations or evidence makes it likely false or misleading. The use of terms like \"breakthrough\" without context adds to its speculative nature, contributing to its unreliability.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Generate the fact-checking prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "\n",
    "# Send the prompt to DeepSeek LLM using Ollama\n",
    "response = ollama.chat(\n",
    "    model=\"deepseek-r1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ollama\n",
    "import chromadb  # Ensure chromadb is installed and properly set up\n",
    "\n",
    "def fetch_entries_from_chromadb(collection_name=\"news_articles\", max_entries=5):\n",
    "    \"\"\"\n",
    "    Fetches the latest enriched entries from ChromaDB.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): The ChromaDB collection to query.\n",
    "        max_entries (int): The maximum number of entries to fetch.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of retrieved entries.\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")  # Update path as needed\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "    \n",
    "    # Retrieve latest X entries\n",
    "    results = collection.get(include=[\"metadatas\", \"documents\"], limit=max_entries)\n",
    "\n",
    "    # Process entries into required format\n",
    "    entries = []\n",
    "    for i in range(len(results[\"documents\"])):\n",
    "        entry = {\n",
    "            \"title\": results[\"metadatas\"][i].get(\"title\", \"Unknown Title\"),\n",
    "            \"url\": results[\"metadatas\"][i].get(\"url\", \"Unknown URL\"),\n",
    "            \"published_date\": results[\"metadatas\"][i].get(\"published_date\", \"Unknown Date\"),\n",
    "            \"source\": results[\"metadatas\"][i].get(\"source\", \"Unknown Source\"),\n",
    "            \"author\": results[\"metadatas\"][i].get(\"author\", \"Unknown Author\"),\n",
    "            \"category\": results[\"metadatas\"][i].get(\"category\", \"Unknown Category\"),\n",
    "            \"enriched_content\": results[\"documents\"][i],\n",
    "            \"TF-IDF Outliers\": json.dumps(results[\"metadatas\"][i].get(\"tfidf_outliers\", [])),\n",
    "            \"Grammar Errors\": results[\"metadatas\"][i].get(\"grammar_errors\", 0),\n",
    "            \"Sentence Count\": results[\"metadatas\"][i].get(\"sentence_count\", 0),\n",
    "            \"Sentiment Polarity\": results[\"metadatas\"][i].get(\"sentiment_polarity\", \"Unknown\"),\n",
    "            \"Sentiment Subjectivity\": results[\"metadatas\"][i].get(\"sentiment_subjectivity\", \"Unknown\"),\n",
    "            \"fact_checking_summary\": results[\"metadatas\"][i].get(\"fact_checking_summary\", \"No fact-checking data available.\"),\n",
    "        }\n",
    "        entries.append(entry)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def analyze_multiple_articles(chroma_entries, model=\"deepseek-r1\", output_file=\"fact_check_results.json\"):\n",
    "    \"\"\"\n",
    "    Sends multiple articles from ChromaDB to the DeepSeek LLM for analysis and saves the results.\n",
    "\n",
    "    Args:\n",
    "        chroma_entries (list): List of enriched article entries from ChromaDB.\n",
    "        model (str): The LLM model to use.\n",
    "        output_file (str): Path to save the JSON file containing results.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of responses from the LLM.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for entry in chroma_entries:\n",
    "        prompt_text = construct_fact_checking_prompt(entry)\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        )\n",
    "        response_content = response['message']['content']\n",
    "        responses.append({\n",
    "            \"title\": entry.get(\"title\", \"Unknown Title\"),\n",
    "            \"url\": entry.get(\"url\", \"Unknown URL\"),\n",
    "            \"published_date\": entry.get(\"published_date\", \"Unknown Date\"),\n",
    "            \"source\": entry.get(\"source\", \"Unknown Source\"),\n",
    "            \"author\": entry.get(\"author\", \"Unknown Author\"),\n",
    "            \"fact_check_result\": response_content\n",
    "        })\n",
    "    \n",
    "    # Save responses to a JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(responses, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return responses\n",
    "\n",
    "# ** Fetch limited entries from ChromaDB and send them for analysis **\n",
    "chroma_entries = fetch_entries_from_chromadb(max_entries=5)\n",
    "results = analyze_multiple_articles(chroma_entries)\n",
    "\n",
    "# Output results\n",
    "print(json.dumps(results, indent=4, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
