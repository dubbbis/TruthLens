{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FACT-CHECKING PROMT (**PROMT ENGINEERING**)\n",
    "#### 01. PROMPT_01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "def construct_fact_checking_prompt(enriched_entry):\n",
    "    \"\"\"\n",
    "    Constructs a well-formatted prompt for fact-checking using DeepSeek LLM.\n",
    "    \n",
    "    Args:\n",
    "        enriched_entry (dict): A dictionary containing article metadata, enriched content, and linguistic analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string prompt for the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract relevant fields from enriched_entry\n",
    "    title = enriched_entry.get(\"title\", \"Unknown Title\")\n",
    "    url = enriched_entry.get(\"url\", \"Unknown URL\")\n",
    "    published_date = enriched_entry.get(\"published_date\", \"Unknown Date\")\n",
    "    source_name = enriched_entry.get(\"source\", \"Unknown Source\")\n",
    "    author = enriched_entry.get(\"author\", \"Unknown Author\")\n",
    "    category = enriched_entry.get(\"category\", \"Unknown Category\")\n",
    "    summary = enriched_entry.get(\"enriched_content\", \"No summary available\")[:500]  # Truncate if too long\n",
    "    \n",
    "    tfidf_outliers = json.loads(enriched_entry.get(\"TF-IDF Outliers\", \"[]\"))  # Convert back to list\n",
    "    tfidf_outliers_str = \", \".join(tfidf_outliers) if tfidf_outliers else \"None\"\n",
    "    \n",
    "    grammar_errors = enriched_entry.get(\"Grammar Errors\", 0)\n",
    "    sentence_count = enriched_entry.get(\"Sentence Count\", 0)\n",
    "    sentiment_polarity = enriched_entry.get(\"Sentiment Polarity\", \"Unknown\")\n",
    "    sentiment_subjectivity = enriched_entry.get(\"Sentiment Subjectivity\", \"Unknown\")\n",
    "    fact_checking_summary = enriched_entry.get(\"fact_checking_summary\", \"No fact-checking data available.\")\n",
    "\n",
    "\n",
    "    # Construct the prompt_01\n",
    "    prompt = f\"\"\"\n",
    "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
    "    \n",
    "    üì∞ **Article Information:**\n",
    "    - **Title:** {title}\n",
    "    - **URL:** {url}\n",
    "    - **Published Date:** {published_date}\n",
    "    - **Source:** {source_name}\n",
    "    - **Author:** {author}\n",
    "    - **Category:** {category}\n",
    "\n",
    "    üîπ **Article Summary (Extracted via AI):**\n",
    "    \"{summary}\"\n",
    "    \n",
    "    üìä **Linguistic Analysis:**\n",
    "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** {tfidf_outliers_str}\n",
    "    - **Grammar Issues:** {grammar_errors} errors\n",
    "    - **Sentence Count:** {sentence_count}\n",
    "    - **Sentiment Analysis:** \n",
    "        - **Polarity (Scale -1 to 1):** {sentiment_polarity}\n",
    "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** {sentiment_subjectivity}\n",
    "\n",
    "    üéØ **Your Task:**\n",
    "    1Ô∏è‚É£ **Assess the credibility of this article** based on the provided content and linguistic analysis.  \n",
    "    2Ô∏è‚É£ **Use the TF-IDF outlier words** to determine if the article contains **unusual phrasing or misleading language.**  \n",
    "    3Ô∏è‚É£ **Analyze sentiment:** Does the emotional tone suggest bias, fear-mongering, or objectivity?  \n",
    "    4Ô∏è‚É£ **Evaluate readability & grammar:** Is the article professionally written, or does it contain errors typical of misinformation?  \n",
    "    5Ô∏è‚É£ **Compare against reliable sources** if possible, to determine factual accuracy.  \n",
    "    \n",
    "    üèÜ **Final Response Format:**\n",
    "    - **Credibility Score:** (Scale 0-100, where 100 = totally credible, 0 = completely false)\n",
    "    - **Verdict:** (Choose one: \"True\", \"False\", or \"Misleading\")\n",
    "    - **Explanation:** (2-3 sentences summarizing why you assigned this rating)\n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "example_entry = {\n",
    "    \"title\": \"Breaking News: AI Solves World Hunger\",\n",
    "    \"url\": \"https://news.example.com/ai-hunger\",\n",
    "    \"published_date\": \"2025-02-18\",\n",
    "    \"source\": \"Example News\",\n",
    "    \"author\": \"John Doe\",\n",
    "    \"category\": \"Technology\",\n",
    "    \"enriched_content\": \"AI has made significant advancements... (summary content here)...\\n\\nFact-Checking Data:\\n- Verified by multiple sources\",\n",
    "    \"TF-IDF Outliers\": json.dumps([\"AI\", \"breakthrough\", \"hunger crisis\"]),\n",
    "    \"Grammar Errors\": 2,\n",
    "    \"Sentence Count\": 25,\n",
    "    \"Sentiment Polarity\": 0.7,\n",
    "    \"Sentiment Subjectivity\": 0.4,\n",
    "    \"fact_checking_summary\": \"Verified by multiple sources.\"\n",
    "}\n",
    "\n",
    "# Generate the prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "print(prompt_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02. PROMPT_02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "def construct_fact_checking_prompt(enriched_entry):\n",
    "    \"\"\"\n",
    "    Constructs a well-formatted prompt for fact-checking using DeepSeek LLM.\n",
    "    \n",
    "    Args:\n",
    "        enriched_entry (dict): A dictionary containing article metadata, enriched content, and linguistic analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string prompt for the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract relevant fields from enriched_entry\n",
    "    title = enriched_entry.get(\"title\", \"Unknown Title\")\n",
    "    url = enriched_entry.get(\"url\", \"Unknown URL\")\n",
    "    published_date = enriched_entry.get(\"published_date\", \"Unknown Date\")\n",
    "    source_name = enriched_entry.get(\"source\", \"Unknown Source\")\n",
    "    author = enriched_entry.get(\"author\", \"Unknown Author\")\n",
    "    category = enriched_entry.get(\"category\", \"Unknown Category\")\n",
    "    summary = enriched_entry.get(\"enriched_content\", \"No summary available\")[:500]  # Truncate if too long\n",
    "    \n",
    "    tfidf_outliers = json.loads(enriched_entry.get(\"TF-IDF Outliers\", \"[]\"))  # Convert back to list\n",
    "    tfidf_outliers_str = \", \".join(tfidf_outliers) if tfidf_outliers else \"None\"\n",
    "    \n",
    "    grammar_errors = enriched_entry.get(\"Grammar Errors\", 0)\n",
    "    sentence_count = enriched_entry.get(\"Sentence Count\", 0)\n",
    "    sentiment_polarity = enriched_entry.get(\"Sentiment Polarity\", \"Unknown\")\n",
    "    sentiment_subjectivity = enriched_entry.get(\"Sentiment Subjectivity\", \"Unknown\")\n",
    "    fact_checking_summary = enriched_entry.get(\"fact_checking_summary\", \"No fact-checking data available.\")\n",
    "\n",
    "\n",
    "    \n",
    "  # Construct the prompt_02\n",
    "    prompt = f\"\"\"\n",
    "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
    "    \n",
    "    üì∞ **Article Information:**\n",
    "    - **Title:** {title}\n",
    "    - **URL:** {url}\n",
    "    - **Published Date:** {published_date}\n",
    "    - **Source:** {source_name}\n",
    "    - **Author:** {author}\n",
    "    - **Category:** {category}\n",
    "\n",
    "    üîπ **Article Summary (Extracted via AI):**\n",
    "    \"{summary}\"\n",
    "    \n",
    "    üìä **Linguistic Analysis:**\n",
    "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** {tfidf_outliers_str}\n",
    "    - **Grammar Issues:** {grammar_errors} errors\n",
    "    - **Sentence Count:** {sentence_count}\n",
    "    - **Sentiment Analysis:** \n",
    "        - **Polarity (Scale -1 to 1):** {sentiment_polarity}\n",
    "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** {sentiment_subjectivity}\n",
    "\n",
    " ## üéØ Task: Evaluate Credibility and Truthfulness  \n",
    "    Based on the provided information, conduct a critical analysis following these points:  \n",
    "\n",
    "    1Ô∏è‚É£ **Credibility Assessment:**  \n",
    "       - Is the source reliable?  \n",
    "       - Does the author have legitimate or recognized credentials in the field?  \n",
    "       - Does the article follow a logical and professional structure, or does it appear poorly written?  \n",
    "\n",
    "    2Ô∏è‚É£ **Detection of Misleading or Sensationalist Language:**  \n",
    "       - Analyze the unusual words detected by TF-IDF. Are these terms uncommon in serious journalism?  \n",
    "       - Does the article use exaggerated language to manipulate the reader‚Äôs emotions?  \n",
    "\n",
    "    3Ô∏è‚É£ **Bias and Subjectivity:**  \n",
    "       - Does the content appear neutral, or does it attempt to influence the reader‚Äôs opinion?  \n",
    "       - Are there phrases that exaggerate, alarm, or contain subjective judgments?  \n",
    "\n",
    "    4Ô∏è‚É£ **Verification with Other Sources:**  \n",
    "       - If a key fact is mentioned, is there verifiable evidence from reliable sources?  \n",
    "       - Are there missing expert citations or solid references?  \n",
    "\n",
    "    5Ô∏è‚É£ **Linguistic Quality:**  \n",
    "       - Does the text contain unusual grammatical errors for legitimate news articles?  \n",
    "       - Does it appear to be an automatically generated or poorly translated text?  \n",
    "\n",
    "    ---  \n",
    "    \n",
    "    ## üìå **Expected Response Format**  \n",
    "    - **Credibility Score (0-100):** (100 = Fully credible, 0 = Completely false)  \n",
    "    - **Final Verdict:** (\"True\", \"False\", or \"Misleading\")  \n",
    "    - **Detailed Explanation (3-5 sentences):** Justify the evaluation based on findings.  \n",
    "\n",
    "    ‚ö†Ô∏è **If the information is insufficient, indicate that more context or additional sources are needed.**  \n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example Usage\n",
    "example_entry = {\n",
    "    \"title\": \"Breaking News: AI Solves World Hunger\",\n",
    "    \"url\": \"https://news.example.com/ai-hunger\",\n",
    "    \"published_date\": \"2025-02-18\",\n",
    "    \"source\": \"Example News\",\n",
    "    \"author\": \"John Doe\",\n",
    "    \"category\": \"Technology\",\n",
    "    \"enriched_content\": \"AI has made significant advancements... (summary content here)...\\n\\nFact-Checking Data:\\n- Verified by multiple sources\",\n",
    "    \"TF-IDF Outliers\": json.dumps([\"AI\", \"breakthrough\", \"hunger crisis\"]),\n",
    "    \"Grammar Errors\": 2,\n",
    "    \"Sentence Count\": 25,\n",
    "    \"Sentiment Polarity\": 0.7,\n",
    "    \"Sentiment Subjectivity\": 0.4,\n",
    "    \"fact_checking_summary\": \"Verified by multiple sources.\"\n",
    "}\n",
    "\n",
    "# Generate the prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "print(prompt_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. PROMPT_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
      "    \n",
      "    üì∞ **Article Information:**\n",
      "    - **Title:** Breaking News: AI Solves World Hunger\n",
      "    - **URL:** https://news.example.com/ai-hunger\n",
      "    - **Published Date:** 2025-02-18\n",
      "    - **Source:** Example News\n",
      "    - **Author:** John Doe\n",
      "    - **Category:** Technology\n",
      "\n",
      "    üîπ **Article Summary (Extracted via AI):**\n",
      "    \"AI has made significant advancements... (summary content here)...\n",
      "\n",
      "Fact-Checking Data:\n",
      "- Verified by multiple sources\"\n",
      "    \n",
      "    üìä **Linguistic Analysis:**\n",
      "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** AI, breakthrough, hunger crisis\n",
      "    - **Grammar Issues:** 2 errors\n",
      "    - **Sentence Count:** 25\n",
      "    - **Sentiment Analysis:** \n",
      "        - **Polarity (Scale -1 to 1):** 0.7\n",
      "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** 0.4\n",
      "\n",
      " ## üéØ  Task: Evaluate Credibility and Truthfulness\n",
      "    Based on the provided information, conduct a critical analysis following these detailed tasks:\n",
      "\n",
      "    1. Source Credibility Analysis:\n",
      "       - Assess the reliability of the source and its reputation in journalism.\n",
      "       - Identify any potential conflicts of interest or biases within the source.\n",
      "       - Determine if the author has known expertise or credentials in the subject matter.\n",
      "\n",
      "    2. Content Verification:\n",
      "       - Cross-check key claims with verifiable and authoritative sources.\n",
      "       - Identify any exaggerations, misleading statements, or unverified claims.\n",
      "       - Evaluate if the article presents evidence and factual backing for its assertions.\n",
      "\n",
      "    3. Detection of Misleading or Sensationalist Language:\n",
      "       - Analyze the tone and wording of the article to identify emotional manipulation.\n",
      "       - Assess the presence of exaggerated or alarmist phrases that may indicate bias.\n",
      "       - Determine whether the article includes balanced viewpoints or only presents one-sided perspectives.\n",
      "\n",
      "    4. Bias and Subjectivity Evaluation:\n",
      "       - Determine whether the article contains subjective language or ideological framing.\n",
      "       - Identify any patterns of bias based on the article's structure, language, and omitted information.\n",
      "       - Consider whether the article serves an agenda beyond objective reporting.\n",
      "\n",
      "    5. External Source Comparison:\n",
      "       - Check if other reputable news sources report on the same topic and whether their coverage aligns.\n",
      "       - Identify inconsistencies in reporting across different sources.\n",
      "       - Evaluate whether primary sources (official statements, research papers, etc.) support the claims made.\n",
      "\n",
      "    6. Quality of Writing and Presentation:\n",
      "       - Analyze the grammatical accuracy and coherence of the text.\n",
      "       - Identify any unusual phrasing that may indicate automatic content generation or poor translation.\n",
      "       - Determine if the article follows standard journalistic practices in terms of citations and formatting.\n",
      "\n",
      "    ---\n",
      "    \n",
      "    Expected Response Format:\n",
      "    - Credibility Score (0-100): (100 = Fully credible, 0 = Completely false)\n",
      "    - Final Verdict: (\"True\", \"False\", or \"Misleading\")\n",
      "    - Detailed Explanation (4-6 sentences): Justify the evaluation based on findings, referencing key factors from the analysis.\n",
      "\n",
      "    If the information is insufficient to determine credibility, specify what additional context or sources would be necessary to reach a conclusive assessment.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def construct_fact_checking_prompt(enriched_entry):\n",
    "    \"\"\"\n",
    "    Constructs a well-formatted prompt for fact-checking using DeepSeek LLM.\n",
    "    \n",
    "    Args:\n",
    "        enriched_entry (dict): A dictionary containing article metadata, enriched content, and linguistic analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string prompt for the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract relevant fields from enriched_entry\n",
    "    title = enriched_entry.get(\"title\", \"Unknown Title\")\n",
    "    url = enriched_entry.get(\"url\", \"Unknown URL\")\n",
    "    published_date = enriched_entry.get(\"published_date\", \"Unknown Date\")\n",
    "    source_name = enriched_entry.get(\"source\", \"Unknown Source\")\n",
    "    author = enriched_entry.get(\"author\", \"Unknown Author\")\n",
    "    category = enriched_entry.get(\"category\", \"Unknown Category\")\n",
    "    summary = enriched_entry.get(\"enriched_content\", \"No summary available\")[:500]  # Truncate if too long\n",
    "    \n",
    "    tfidf_outliers = json.loads(enriched_entry.get(\"TF-IDF Outliers\", \"[]\"))  # Convert back to list\n",
    "    tfidf_outliers_str = \", \".join(tfidf_outliers) if tfidf_outliers else \"None\"\n",
    "    \n",
    "    grammar_errors = enriched_entry.get(\"Grammar Errors\", 0)\n",
    "    sentence_count = enriched_entry.get(\"Sentence Count\", 0)\n",
    "    sentiment_polarity = enriched_entry.get(\"Sentiment Polarity\", \"Unknown\")\n",
    "    sentiment_subjectivity = enriched_entry.get(\"Sentiment Subjectivity\", \"Unknown\")\n",
    "    fact_checking_summary = enriched_entry.get(\"fact_checking_summary\", \"No fact-checking data available.\")\n",
    "\n",
    "\n",
    "    \n",
    "  # Construct the prompt_02\n",
    "    prompt = f\"\"\"\n",
    "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
    "    \n",
    "    üì∞ **Article Information:**\n",
    "    - **Title:** {title}\n",
    "    - **URL:** {url}\n",
    "    - **Published Date:** {published_date}\n",
    "    - **Source:** {source_name}\n",
    "    - **Author:** {author}\n",
    "    - **Category:** {category}\n",
    "\n",
    "    üîπ **Article Summary (Extracted via AI):**\n",
    "    \"{summary}\"\n",
    "    \n",
    "    üìä **Linguistic Analysis:**\n",
    "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** {tfidf_outliers_str}\n",
    "    - **Grammar Issues:** {grammar_errors} errors\n",
    "    - **Sentence Count:** {sentence_count}\n",
    "    - **Sentiment Analysis:** \n",
    "        - **Polarity (Scale -1 to 1):** {sentiment_polarity}\n",
    "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** {sentiment_subjectivity}\n",
    "\n",
    " ## üéØ  Task: Evaluate Credibility and Truthfulness\n",
    "    Based on the provided information, conduct a critical analysis following these detailed tasks:\n",
    "\n",
    "    1. Source Credibility Analysis:\n",
    "       - Assess the reliability of the source and its reputation in journalism.\n",
    "       - Identify any potential conflicts of interest or biases within the source.\n",
    "       - Determine if the author has known expertise or credentials in the subject matter.\n",
    "\n",
    "    2. Content Verification:\n",
    "       - Cross-check key claims with verifiable and authoritative sources.\n",
    "       - Identify any exaggerations, misleading statements, or unverified claims.\n",
    "       - Evaluate if the article presents evidence and factual backing for its assertions.\n",
    "\n",
    "    3. Detection of Misleading or Sensationalist Language:\n",
    "       - Analyze the tone and wording of the article to identify emotional manipulation.\n",
    "       - Assess the presence of exaggerated or alarmist phrases that may indicate bias.\n",
    "       - Determine whether the article includes balanced viewpoints or only presents one-sided perspectives.\n",
    "\n",
    "    4. Bias and Subjectivity Evaluation:\n",
    "       - Determine whether the article contains subjective language or ideological framing.\n",
    "       - Identify any patterns of bias based on the article's structure, language, and omitted information.\n",
    "       - Consider whether the article serves an agenda beyond objective reporting.\n",
    "\n",
    "    5. External Source Comparison:\n",
    "       - Check if other reputable news sources report on the same topic and whether their coverage aligns.\n",
    "       - Identify inconsistencies in reporting across different sources.\n",
    "       - Evaluate whether primary sources (official statements, research papers, etc.) support the claims made.\n",
    "\n",
    "    6. Quality of Writing and Presentation:\n",
    "       - Analyze the grammatical accuracy and coherence of the text.\n",
    "       - Identify any unusual phrasing that may indicate automatic content generation or poor translation.\n",
    "       - Determine if the article follows standard journalistic practices in terms of citations and formatting.\n",
    "\n",
    "    ---\n",
    "    \n",
    "    Expected Response Format:\n",
    "    - Credibility Score (0-100): (100 = Fully credible, 0 = Completely false)\n",
    "    - Final Verdict: (\"True\", \"False\", or \"Misleading\")\n",
    "    - Detailed Explanation (4-6 sentences): Justify the evaluation based on findings, referencing key factors from the analysis.\n",
    "\n",
    "    If the information is insufficient to determine credibility, specify what additional context or sources would be necessary to reach a conclusive assessment.\n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example Usage\n",
    "example_entry = {\n",
    "    \"title\": \"Breaking News: AI Solves World Hunger\",\n",
    "    \"url\": \"https://news.example.com/ai-hunger\",\n",
    "    \"published_date\": \"2025-02-18\",\n",
    "    \"source\": \"Example News\",\n",
    "    \"author\": \"John Doe\",\n",
    "    \"category\": \"Technology\",\n",
    "    \"enriched_content\": \"AI has made significant advancements... (summary content here)...\\n\\nFact-Checking Data:\\n- Verified by multiple sources\",\n",
    "    \"TF-IDF Outliers\": json.dumps([\"AI\", \"breakthrough\", \"hunger crisis\"]),\n",
    "    \"Grammar Errors\": 2,\n",
    "    \"Sentence Count\": 25,\n",
    "    \"Sentiment Polarity\": 0.7,\n",
    "    \"Sentiment Subjectivity\": 0.4,\n",
    "    \"fact_checking_summary\": \"Verified by multiple sources.\"\n",
    "}\n",
    "\n",
    "# Generate the prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEND PROMP TO THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to evaluate the credibility and truthfulness of this news article. Let me go through each task step by step based on the provided data.\n",
      "\n",
      "First, Source Credibility Analysis: The source is \"Example News\" from 2025-02-18. It's an example source, which probably means it's not a real news outlet but used for testing. Since I don't know its reputation or if it has any bias, this could be a red flag. The author is John Doe, who isn't identified as an expert in AI or technology, so that might mean he doesn't have the credentials to speak on such a complex topic.\n",
      "\n",
      "Next, Content Verification: The article claims AI has solved world hunger, which seems too dramatic and likely exaggerated. AI making such a significant breakthrough is probably overhyped unless it's from a peer-reviewed source. I can't verify this because there are no external sources provided or citations given in the summary.\n",
      "\n",
      "Detection of Misleading or Sensationalist Language: The title uses \"Breaking News\" and the lead sentence is strong, which might be to grab attention. Phrases like \"AI Solves World Hunger\" could be sensationalized to catch more clicks. There's also a lack of context on how AI achieved this‚Äîwhether it's through sustainable practices, technological advancements alone, or maybe just simulations.\n",
      "\n",
      "Bias and Subjectivity Evaluation: The article is very opinionated, placing the blame on AI but not considering other factors like political decisions or economic issues. It uses strong positive language without any counterarguments, which makes it subjective. There's no mention of limitations or ethical concerns, so it's quite biased towards AI as the sole solution.\n",
      "\n",
      "External Source Comparison: Since I don't have access to other reputable news sources, I can't compare their reports on this topic. Without seeing what other outlets say, it's hard to assess consistency.\n",
      "\n",
      "Quality of Writing and Presentation: The article has some grammatical errors mentioned‚Äîtwo in total. The language seems a bit formal but might indicate poor proofreading or auto-generated text if there are unusual phrasings related to AI terminology.\n",
      "\n",
      "Putting it all together, the credibility score would be low because of the sensational claims without evidence, potential bias from an unknown author and source, and grammatical issues. The article is misleading with its dramatic headline and lacks necessary verifications or balanced perspectives.\n",
      "</think>\n",
      "\n",
      "**Credibility Score:** 20/100  \n",
      "**Final Verdict:** False/Misleading  \n",
      "**Explanation:** The article's claims about AI solving the hunger crisis are sensationalized without substantial evidence, relying on an unnamed author from a non-reputable source. The lack of verifiable sources and credible citations undermines its truthfulness. Misleading language and biased framing exacerbate concerns, while grammatical errors further detract from its credibility. Additional context, such as reputable sources' coverage and peer-reviewed citations, is needed for a conclusive assessment.\n",
      "\n",
      "If I were to evaluate the credibility of this article, it would be challenging without more information about the author's expertise and the source's reputation. However, based on the provided data, the article seems highly questionable due to its sensational claims, lack of evidence, and potential bias. It's important to rely on multiple reputable sources for accurate reporting.\n",
      "\n",
      "Without further context or sources that corroborate the claims made in this article, it is difficult to determine its truthfulness with confidence.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Generate the fact-checking prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "\n",
    "# Send the prompt to DeepSeek LLM using Ollama\n",
    "response = ollama.chat(\n",
    "    model=\"deepseek-r1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUTOMATES PROCESS AND SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Presidents Are Often Judged By History Through The Lens Of Morality\",\n",
      "        \"url\": \"https://newsone.com/5939034/presidents-are-judged-by-history-through-the-lens-of-morality/\",\n",
      "        \"published_date\": \"2025-02-17T14:33:46+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"George R. Goethals, University of Richmond\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so I need to figure out if this news article is reliable based on some basic checks. Let me start by reading through the content carefully and then see what potential issues or inconsistencies I can spot.\\n\\nThe article talks about how presidents are judged over time through historical surveys, focusing on their moral authority and other aspects. It mentions a few key points: historical rankings have changed as values shift, examples of specific presidents like Lincoln and Grant moving up and down the rankings, and the role of moral authority in these evaluations. \\n\\nFirst off, I notice that the article is quite technical with references to specific surveys by C-SPAN from 2009, 2017, and 2021, as well as an earlier one in 1948. It also cites a professor's analysis using Dean Keith Simonton's model of evaluating presidents based on how well they fit the image of a strong leader.\\n\\nOne potential issue is that it doesn't provide a direct source for these specific surveys or any peer-reviewed studies, which might make its claims seem less credible without more backing. Also, it mentions how certain presidents like James Buchanan and Andrew Johnson have consistently ranked low, but I don't recall reading about that in my previous studies‚Äîmaybe the article is adding some unique data here.\\n\\nAnother point is about the moral authority of leaders like Richard Nixon and Warren Harding being criticized due to scandals. The article says that these scandals lower their rankings, which seems reasonable given past analyses I've seen where such issues affect evaluations.\\n\\nThe mention of how the image of a leader evolves over time could be a valid point, but without concrete data or references from multiple studies, it's hard to say for sure if this analysis is accurate. Also, the article's length is quite limited; it doesn't delve deeply into each survey result or provide enough context on why certain presidents have moved up or down.\\n\\nI should also check if there are any known issues with similar articles. The fact that the article references a professor from the University of Richmond might suggest it's part of an academic discussion, but without knowing if this exact analysis has been published before, I can't confirm its originality.\\n\\nLastly, the conclusion at the end says the article is republished under Creative Commons, which is fine as long as the content itself isn't factually incorrect. Without reading more or having access to the original sources of those surveys, it's hard to verify every detail.\\n\\nOverall, while the article presents a logical argument based on existing survey data and a professor's analysis, its reliance on specific studies that aren't widely available might raise some doubts about its reliability without further evidence.\\n</think>\\n\\n**Analysis of the News Article's Reliability**\\n\\nThe article in question discusses how presidential rankings change over time due to evolving values and historical context. Here's a structured evaluation based on key points:\\n\\n1. **Survey References**: The article cites specific surveys by C-SPAN from 2009, 2017, and 2021, as well as an earlier one in 1948. However, it does not provide direct sources or references to these studies, which could weaken its credibility without additional backing.\\n\\n2. **Mention of Specific Presidencies**: It mentions presidents like James Buchanan and Andrew Johnson consistently ranking low, but this information isn't widely corroborated in previous studies, making the article's uniqueness worth noting.\\n\\n3. **Scandal Impact**: The article correctly points out that scandals lower presidential rankings, aligning with common analyses such as those of Nixon and Harding.\\n\\n4. **Moral Authority Focus**: The emphasis on moral authority is valid but might not delve deeply enough into each survey's results without broader context or multiple study references.\\n\\n5. **Academic Context**: Referencing a professor from the University of Richmond suggests an academic angle, but without knowing if this specific analysis has been published elsewhere, its originality remains uncertain.\\n\\n6. **Republishing Information**: The article is correctly noted as republished under Creative Commons, ensuring it's available for free without violating copyright laws.\\n\\n**Conclusion**: The article presents a logical argument based on existing survey data and academic analysis but lacks detailed references to specific studies or broader context. Its reliability could be enhanced with more evidence or access to the original sources of the surveys cited.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"The Prince and Princess of Wales' ¬£33k-per-week Mustique villa has a 60ft pool\",\n",
      "        \"url\": \"https://www.hellomagazine.com/travel/814676/kate-middleton-prince-william-villa-antilles-mustique/\",\n",
      "        \"published_date\": \"2025-02-17T14:33:35+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Georgia Brown\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so I have to analyze whether the given news article about Prince and Princess of Wales' villa is reliable based on a 404 error. Hmm, let's break this down.\\n\\nFirst, the title says there's an article in The Sun about them having a ¬£33k-per-week villa with a 60ft pool. But when I check the source link, it returns a 404, which means the page doesn't exist on their site.\\n\\nI know that when articles have 404 errors, it often means they were either moved to a different URL or deleted entirely. Sometimes, if there was an issue during publication, like a server crash, but that's less common now with auto-renditions. \\n\\nThe newspaper is The Sun, which I'm familiar with; they do have multiple sections and sometimes have online-only content. Maybe the article isn't on their main website anymore or has been moved to another section under a different title.\\n\\nI also consider the possibility of user error‚Äîmaybe someone pasted an incorrect URL into the source field. Or perhaps it's from a draft that wasn't properly published yet.\\n\\nTo verify reliability, I can look for the exact details elsewhere. If there's any other information about their villa, like if another media outlet covered it with similar or different details, that could help assess its credibility.\\n\\nIn this case, though, since the source is The Sun and the page isn't found, the article might be either moved, deleted, or not published yet. Without more evidence from reputable sources, it's hard to confirm its reliability.\\n</think>\\n\\nThe news article in question appears to have encountered a 404 error on The Sun's website, indicating that the content may no longer be accessible at the provided URL. Here's a structured analysis of the situation:\\n\\n1. **Issue Identification**: The article about Prince and Princess of Wales' villa is linked via a broken (404) status code, suggesting the page is either deleted, moved, or unavailable.\\n\\n2. **Possible Causes**:\\n   - **Moved/Deleted**: The content might have been relocated to another URL or removed entirely.\\n   - **Technical Issues**: Potential server errors, though less common nowadays with auto-rendition systems.\\n   - **User Error**: Incorrect URL provided when embedding the article.\\n\\n3. **Verification Efforts**: Without access to alternative sources confirming the details (e.g., via other media outlets), it's challenging to assess the reliability definitively.\\n\\n**Conclusion**: The article is likely no longer available on The Sun, but without corroborating evidence from reputable sources, its reliability cannot be confirmed.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Scottish museum to undergo 'bold transformation' after ¬£2.6m government funding boost\",\n",
      "        \"url\": \"https://www.scotsman.com/news/scottish-news/scottish-museum-to-undergo-bold-transformation-after-ps26m-government-funding-boost-4994958\",\n",
      "        \"published_date\": \"2025-02-17T14:32:22+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Jane Bradley\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so I need to figure out if the news article about the Scottish museum undergoing a 'bold transformation' with ¬£2.6m funding is reliable. The user mentioned that when they tried accessing it, they got a 403 status code, which means the page was encrypted but not authenticated. Hmm, I remember from previous knowledge that sometimes websites use encryption to protect data, and if you don't have an authentication token, you can't access their content. So maybe this article is behind a paywall or requires authentication.\\n\\nThe title suggests it's about a museum getting significant funding from the government, which could be positive for its operations and possibly improvements. But without knowing where exactly this information comes from, I can't verify its accuracy beyond what's given. \\n\\nI should consider possible sources that might have such an article. Well-known museums or news outlets like the Guardian, BBC, or The Times often cover such topics with credible articles. However, if it's behind a paywall and there's no way to access the content for free, then I can't check its reliability directly.\\n\\nAnother angle is checking the date mentioned‚ÄîFebruary 17, 2025. That seems recent enough that someone might have written about it around that time. But again, without access to the actual article or external sources citing it, I can't be certain.\\n\\nI also think about the possibility of a fake article or a phishing attempt, which sometimes happens online. If there's an error in accessing the page, maybe the user fell victim to such tactics. \\n\\nIn conclusion, while the article seems plausible based on the information given and possible sources, without being able to access it myself, I can't confirm its reliability beyond what's stated.\\n</think>\\n\\nThe news article titled \\\"Scottish museum to undergo 'bold transformation' after ¬£2.6m government funding boost\\\" is mentioned with a positive context regarding potential funding benefits for a museum. However, the lack of access due to a 403 error (unauthenticated) means direct verification of its reliability is not possible.\\n\\nKey considerations include:\\n1. **Source Verification**: The article's credibility could stem from reputable outlets like The Guardian or BBC, but without access, this remains speculative.\\n2. **Relevance and Context**: The transformation suggests potential improvements, which is a positive development if accurate.\\n3. **Access Issues**: The 403 error indicates the article may be behind a paywall or require authentication to access, hindering direct verification.\\n\\nIn summary, while the scenario presents a plausible narrative, the absence of access limits definitive evaluation of the article's reliability.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"The Lathums bring forward release of new album ‚ÄòMatter Does Not Define‚Äô\",\n",
      "        \"url\": \"https://www.nme.com/news/music/the-lathums-bring-forward-release-of-new-album-matter-does-not-define-3838653?utm_source=rss&utm_medium=rss&utm_campaign=the-lathums-bring-forward-release-of-new-album-matter-does-not-define\",\n",
      "        \"published_date\": \"2025-02-17T14:32:14+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Laura Molloy\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so I've been asked to analyze whether the provided news article is reliable. Hmm, let's break this down step by step.\\n\\nFirst, I'll read through the article carefully. The title mentions that \\\"The Lathums bring forward release of new album 'Matter Does Not Define'.\\\" They're a Wigan rock band, and their third album was initially set for March 7 but is now coming out on February 28. They pulled some strings, which I think means they manipulated schedules.\\n\\nLooking at the content, they shared news via Instagram with excitement about the album dropping a week early. They mentioned pre-saves from stores giving fans more time to learn lyrics before the tour, so that's good for fans who like advance preparation.\\n\\nThey also talked about their previous releases, like 'No Direction' and their 2023 album. Frontman Alex Moore explained how recording the LP felt like it brought them back to their roots with John Kettle, a friend and old college tutor, which was a positive experience. They included an interesting note about using a frying pan in one of their songs.\\n\\nThe tour dates are listed for March and April 2025, including major cities like Glasgow, Newcastle, and London, as well as festivals TRNSMT and Neighbourhood Weekender. They also mention tickets being on sale now. \\n\\nNow, evaluating reliability: I check the source‚Äîit's from an unknown source. The date is February 17, 2025, so it's pretty recent, which adds to its currentness. However, without knowing if it's a press release or another verified outlet, there might be a lack of credibility.\\n\\nThe article doesn't cite any previous albums except the 2023 one and their single from last October. That could make it less credible because it's only building on recent successes. The Instagram post is direct but doesn't have a link, so we can‚Äôt verify its authenticity directly.\\n\\nAlex Moore's quote comes from a press release, which seems credible since he's the frontman. His comment about working with John Kettle adds some legitimacy to their creative process. Including details like using a frying pan in a song gives it an authentic touch that might make the article more engaging but doesn't necessarily add to its credibility.\\n\\nThe tour dates are spread out across several cities and include upcoming festivals, which is common for bands to announce tours close to album release to build anticipation. The fact that tickets are on sale now suggests they're confident in their album's reception, which adds to its credibility.\\n\\nHowever, the article's structure could be improved by including more details about previous albums or other artists in the same genre to provide context. Also, verifying if it's a press release or just a fan announcement is crucial because the tone and source can affect reliability.\\n\\nIn conclusion, while there are some positive signs like current date of publication, direct quotes from credible sources (like Alex Moore), and well-structured tour dates, the lack of detailed background information and the unknown source make this article less reliable. It's important to look for additional corroborating information before giving a full endorsement.\\n</think>\\n\\n**Analysis of the News Article Reliability**\\n\\nThe news article about The Lathums releasing their new album 'Matter Does Not Define' presents several positive indicators, such as current publication date (February 17, 2025), direct quotes from Alex Moore via press release, and detailed tour dates. These elements suggest genuine effort and attention to fan anticipation.\\n\\nHowever, the article's credibility is somewhat hindered by:\\n\\n1. **Source Uncertainty**: The source is unknown, adding a layer of doubt about its reliability.\\n2. **Lack of Context**: The article only references their 2023 album but lacks information about previous releases or broader industry context, which could enhance credibility.\\n3. **Authentic Details**: While the use of creative elements like the frying pan in 'Stellar Cast' is engaging, it doesn't directly contribute to verifying the band's authenticity.\\n\\n**Conclusion**: The article demonstrates effort with timely release, direct quotes from a credible artist, and well-structured tour dates. Yet, its lack of detailed background information and unknown source temper its reliability. Further corroborating evidence would enhance trustworthiness before fully endorsing the article.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Trump begins firings of FAA air traffic control staff just weeks after fatal DC plane crash\",\n",
      "        \"url\": \"https://www.toledoblade.com/news/nation/2025/02/17/trump-begins-firings-of-faa-air-traffic-control-staff-after-fatal-dc-plane-crash/stories/20250217084\",\n",
      "        \"published_date\": \"2025-02-17T14:31:22+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Unknown author\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, I need to figure out if the news article titled \\\"Trump begins firings of FAA air traffic control staff just weeks after fatal DC plane crash\\\" is reliable. Let me start by breaking down the content.\\n\\nFirst, the article mentions that Donald Trump fired FAA employees related to air traffic control shortly after a tragic plane crash in Washington D.C. I know that in 2017 there was a crash of a Boeing 737 MAX near Lion Air Flight 610, which led to several incidents and changes in aviation regulations. The crash didn't happen just weeks before the event described here; it actually happened years ago.\\n\\nSo right off the bat, the timing seems off. If this is about the Lion Air crash, that event occurred around August 2017, so several months after Trump's time as president. It makes me question whether the article is mixing up different events or inaccurately reporting a recent incident.\\n\\nNext, checking if it's published by a reputable source. The user mentioned \\\"Unknown source,\\\" which is a red flag because well-known news outlets don't usually publish such specific and possibly false information without verification.\\n\\nLooking at the content sections like obituaries, jobs, etc., there doesn't seem to be any connection to the actual event. The mention of \\\"Enter-To-Win\\\" and other sections typical on a website might suggest it's a promotional site or not focused on serious news topics, which could indicate it's not a reliable source.\\n\\nI should also consider if there's any evidence supporting this claim. For example, checking if Trump fired FAA staff around the time in question. From what I recall, Trump was president from 2017 to 2021, and the Lion Air crash happened later that year. So if the article says it happened just weeks after a crash that actually took place years earlier, that's inconsistent.\\n\\nMoreover, the mention of the DC plane crash might refer to something else entirely. The U.S. has had other commercial flights since then, but none have been fatal unless there's another recent crash I'm not aware of. Without specific details from reliable sources confirming this event, it's hard to take the article at face value.\\n\\nLastly, considering the structure and sections in the article, which include things like obituaries and job listings, it doesn't seem to be a serious news source but more like a promotional or perhaps even a scam site. High-quality news articles typically cover major events with depth and context from multiple sources, which this one lacks.\\n\\nPutting it all together, the article seems unreliable due to incorrect timing, lack of reputable sourcing, and content that doesn't align with known historical events.\\n</think>\\n\\nThe article titled \\\"Trump begins firings of FAA air traffic control staff just weeks after fatal DC plane crash\\\" is not reliable. Here's why:\\n\\n1. **Timing Inconsistency**: The article incorrectly attributes an event to just weeks after a 2017 Lion Air crash, which actually occurred in August 2017. This temporal discrepancy suggests the information may be fabricated or based on a misunderstanding.\\n\\n2. **Source Credibility**: The article is attributed to an \\\"Unknown source,\\\" which is problematic as reputable news outlets typically wouldn't publish such specific and potentially false claims without verification.\\n\\n3. **Content Irrelevance**: The content sections (e.g., obituaries, jobs) indicate it's not a serious news source but rather a promotional or possibly scamsite, lacking the depth and context typical of reliable reporting.\\n\\n4. **Lack of Verification**: There is no evidence to support the claim that Trump fired FAA staff following an event that already occurred years prior, making the article's factual basis questionable.\\n\\nIn conclusion, the article is unreliable due to its inconsistent timing, lack of credible sources, and content that doesn't align with known historical events.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "import chromadb  # Ensure chromadb is installed and properly set up\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_entries_from_chromadb(collection_name=\"news_articles\", max_entries=5):\n",
    "    \"\"\"\n",
    "    Fetches the latest enriched entries from ChromaDB.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): The ChromaDB collection to query.\n",
    "        max_entries (int): The maximum number of entries to fetch.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of retrieved entries.\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")  # Update path as needed\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "    \n",
    "    # Retrieve latest X entries\n",
    "    results = collection.get(include=[\"metadatas\", \"documents\"], limit=max_entries)\n",
    "\n",
    "    # Process entries into required format\n",
    "    entries = []\n",
    "    for i in range(len(results[\"documents\"])):\n",
    "        entry = {\n",
    "            \"title\": results[\"metadatas\"][i].get(\"title\", \"Unknown Title\"),\n",
    "            \"url\": results[\"metadatas\"][i].get(\"url\", \"Unknown URL\"),\n",
    "            \"published_date\": results[\"metadatas\"][i].get(\"published_date\", \"Unknown Date\"),\n",
    "            \"source\": results[\"metadatas\"][i].get(\"source\", \"Unknown Source\"),\n",
    "            \"author\": results[\"metadatas\"][i].get(\"author\", \"Unknown Author\"),\n",
    "            \"category\": results[\"metadatas\"][i].get(\"category\", \"Unknown Category\"),\n",
    "            \"enriched_content\": results[\"documents\"][i],\n",
    "            \"TF-IDF Outliers\": json.dumps(results[\"metadatas\"][i].get(\"tfidf_outliers\", [])),\n",
    "            \"Grammar Errors\": results[\"metadatas\"][i].get(\"grammar_errors\", 0),\n",
    "            \"Sentence Count\": results[\"metadatas\"][i].get(\"sentence_count\", 0),\n",
    "            \"Sentiment Polarity\": results[\"metadatas\"][i].get(\"sentiment_polarity\", \"Unknown\"),\n",
    "            \"Sentiment Subjectivity\": results[\"metadatas\"][i].get(\"sentiment_subjectivity\", \"Unknown\"),\n",
    "            \"fact_checking_summary\": results[\"metadatas\"][i].get(\"fact_checking_summary\", \"No fact-checking data available.\"),\n",
    "        }\n",
    "        entries.append(entry)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def construct_fact_checking_prompt(entry):\n",
    "    \"\"\"\n",
    "    Constructs a fact-checking prompt for the LLM.\n",
    "\n",
    "    Args:\n",
    "        entry (dict): A dictionary containing article details.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt.\n",
    "    \"\"\"\n",
    "    return f\"Analyze the following news article:\\n\\nTitle: {entry['title']}\\nSource: {entry['source']}\\nPublished: {entry['published_date']}\\nContent: {entry['enriched_content']}\\n\\nIs this news article reliable? Provide a fact-checking analysis.\"\n",
    "\n",
    "def get_unique_filename(base_path=\"PROMT_RESULTS\"):\n",
    "    \"\"\"\n",
    "    Generates a unique filename with the format: Prompt_Return_YYYY-MM-DD_N.json.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The directory where files will be stored.\n",
    "\n",
    "    Returns:\n",
    "        str: The full path of the unique filename.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Get current date\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    file_number = 1\n",
    "\n",
    "    while True:\n",
    "        filename = f\"Prompt_Return_{date_str}_{file_number}.json\"\n",
    "        full_path = os.path.join(base_path, filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            return full_path\n",
    "        file_number += 1\n",
    "\n",
    "def analyze_multiple_articles(chroma_entries, model=\"deepseek-r1\"):\n",
    "    \"\"\"\n",
    "    Sends multiple articles from ChromaDB to the DeepSeek LLM for analysis and saves the results.\n",
    "\n",
    "    Args:\n",
    "        chroma_entries (list): List of enriched article entries from ChromaDB.\n",
    "        model (str): The LLM model to use.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of responses from the LLM.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for entry in chroma_entries:\n",
    "        prompt_text = construct_fact_checking_prompt(entry)\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        )\n",
    "        response_content = response['message']['content']\n",
    "        responses.append({\n",
    "            \"title\": entry.get(\"title\", \"Unknown Title\"),\n",
    "            \"url\": entry.get(\"url\", \"Unknown URL\"),\n",
    "            \"published_date\": entry.get(\"published_date\", \"Unknown Date\"),\n",
    "            \"source\": entry.get(\"source\", \"Unknown Source\"),\n",
    "            \"author\": entry.get(\"author\", \"Unknown Author\"),\n",
    "            \"fact_check_result\": response_content\n",
    "        })\n",
    "    \n",
    "    # Generate unique filename\n",
    "    output_file = get_unique_filename()\n",
    "\n",
    "    # Save responses to a JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(responses, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return responses\n",
    "\n",
    "# ** Fetch limited entries from ChromaDB and send them for analysis **\n",
    "chroma_entries = fetch_entries_from_chromadb(max_entries=5)\n",
    "results = analyze_multiple_articles(chroma_entries)\n",
    "\n",
    "# Output results\n",
    "print(json.dumps(results, indent=4, ensure_ascii=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental Differences Between the Three Prompts  \n",
    "\n",
    "## Prompt 5  \n",
    "- Focuses on **historical context** and **content analysis** but lacks a deep evaluation of **source credibility**.  \n",
    "- Provides **basic fact-checking** without structured methodologies for bias detection.  \n",
    "- Does not fully consider **technical issues** like **404 errors, paywalls, or misinformation tactics**.  \n",
    "\n",
    "## Prompt 6  \n",
    "- Introduces **source credibility analysis**, evaluating **bias and methodological clarity**.  \n",
    "- Improves **transparency evaluation** and **fact-checking reliability**.  \n",
    "- Begins considering **digital accessibility issues**, such as **content removal or URL changes**.  \n",
    "\n",
    "## Prompt 7  \n",
    "- Expands **source credibility verification**, including **peer-reviewed sources and institutional transparency**.  \n",
    "- Evaluates **misleading language**, **tone manipulation**, and **sensationalism**.  \n",
    "- Includes **fact-checking cross-references** with **official statements and authoritative sources**.  \n",
    "- Considers **technical barriers** like **paywalls, 404 errors, and online content manipulation**.  \n",
    "- Analyzes **media influence and marketing tactics** in news presentation.  \n",
    "\n",
    "---\n",
    "\n",
    "# Comparative Summary of Results  \n",
    "\n",
    "## Presidential Rankings Article  \n",
    "- **Prompt 5**: Evaluates **historical context** and **survey methodology**.  \n",
    "- **Prompt 6**: Examines **bias in presidential rankings** and **missing survey details**.  \n",
    "- **Prompt 7**: Enhances scrutiny of **source transparency** and **lack of peer-reviewed studies**.  \n",
    "- **Best Performance**: **Prompt 7**, due to stronger verification of **sources and methodology**.  \n",
    "\n",
    "## Prince and Princess of Wales' Mustique Villa  \n",
    "- **Prompt 5**: Identifies **lack of reputable sources**.  \n",
    "- **Prompt 6**: Adds **404 error analysis** and considers **content removal**.  \n",
    "- **Prompt 7**: Investigates **potential online content manipulation** and **technical barriers**.  \n",
    "- **Best Performance**: **Prompt 7**, for deeper **digital forensic evaluation**.  \n",
    "\n",
    "## Scottish Museum Government Funding  \n",
    "- **Prompt 5**: Discusses **general credibility of government funding news**.  \n",
    "- **Prompt 6**: Notes that **government sources are typically reliable** but lacks detailed verification.  \n",
    "- **Prompt 7**: Evaluates **paywall restrictions and source accessibility issues**.  \n",
    "- **Best Performance**: **Prompt 7**, due to analysis of **information accessibility**.  \n",
    "\n",
    "## The Lathums Album Release  \n",
    "- **Prompt 5**: Checks **content consistency** but **does not verify sources**.  \n",
    "- **Prompt 6**: Fact-checks **band announcements** but lacks external verification.  \n",
    "- **Prompt 7**: Analyzes **fan engagement tactics, media influence, and marketing strategies**.  \n",
    "- **Best Performance**: **Prompt 7**, due to **evaluation of media influence on reporting**.  \n",
    "\n",
    "## Trump and FAA Air Traffic Control Firings  \n",
    "- **Prompt 5**: Identifies **timeline inconsistencies**.  \n",
    "- **Prompt 6**: Improves **historical fact-checking** but lacks **government source verification**.  \n",
    "- **Prompt 7**: Expands on **government accountability, misinformation sources, and aviation industry fact-checking**.  \n",
    "- **Best Performance**: **Prompt 7**, due to **comprehensive verification and cross-referencing**.  \n",
    "\n",
    "---\n",
    "\n",
    "# Final Verdict: Best Prompt Performance  \n",
    "The **updated prompt used in File 7** is the most effective because it:  \n",
    "- **Enhances source credibility analysis** with **peer-reviewed references and institutional transparency**.  \n",
    "- **Evaluates bias, misleading language, and tone manipulation**.  \n",
    "- **Incorporates fact-checking methodologies with cross-references to authoritative sources**.  \n",
    "- **Considers digital accessibility issues** such as **paywalls, content removals, and misinformation tactics**.  \n",
    "- **Analyzes the influence of media narratives and marketing strategies** on reporting.  \n",
    "\n",
    "Would you like to refine the prompt further based on these findings?  \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
