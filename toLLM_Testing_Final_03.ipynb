{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FACT-CHECKING PROMPT (**PROMPT ENGINEERING**)\n",
    "#### 01. PROMPT_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
      "    \n",
      "    üì∞ **Article Information:**\n",
      "    - **Title:** Breaking News: AI Solves World Hunger\n",
      "    - **URL:** https://news.example.com/ai-hunger\n",
      "    - **Published Date:** 2025-02-18\n",
      "    - **Source:** Example News\n",
      "    - **Author:** John Doe\n",
      "    - **Category:** Technology\n",
      "\n",
      "    üîπ **Article Summary (Extracted via AI):**\n",
      "    \"AI has made significant advancements... (summary content here)...\n",
      "\n",
      "Fact-Checking Data:\n",
      "- Verified by multiple sources\"\n",
      "    \n",
      "    üìä **Linguistic Analysis:**\n",
      "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** AI, breakthrough, hunger crisis\n",
      "    - **Grammar Issues:** 2 errors\n",
      "    - **Sentence Count:** 25\n",
      "    - **Sentiment Analysis:** \n",
      "        - **Polarity (Scale -1 to 1):** 0.7\n",
      "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** 0.4\n",
      "\n",
      "    üéØ **Your Task:**\n",
      "    1Ô∏è‚É£ **Assess the credibility of this article** based on the provided content and linguistic analysis.  \n",
      "    2Ô∏è‚É£ **Use the TF-IDF outlier words** to determine if the article contains **unusual phrasing or misleading language.**  \n",
      "    3Ô∏è‚É£ **Analyze sentiment:** Does the emotional tone suggest bias, fear-mongering, or objectivity?  \n",
      "    4Ô∏è‚É£ **Evaluate readability & grammar:** Is the article professionally written, or does it contain errors typical of misinformation?  \n",
      "    5Ô∏è‚É£ **Compare against reliable sources** if possible, to determine factual accuracy.  \n",
      "    \n",
      "    üèÜ **Final Response Format:**\n",
      "    - **Credibility Score:** (Scale 0-100, where 100 = totally credible, 0 = completely false)\n",
      "    - **Verdict:** (Choose one: \"True\", \"False\", or \"Misleading\")\n",
      "    - **Explanation:** (2-3 sentences summarizing why you assigned this rating)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def construct_fact_checking_prompt(enriched_entry):\n",
    "    \"\"\"\n",
    "    Constructs a well-formatted prompt for fact-checking using DeepSeek LLM.\n",
    "    \n",
    "    Args:\n",
    "        enriched_entry (dict): A dictionary containing article metadata, enriched content, and linguistic analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string prompt for the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract relevant fields from enriched_entry\n",
    "    title = enriched_entry.get(\"title\", \"Unknown Title\")\n",
    "    url = enriched_entry.get(\"url\", \"Unknown URL\")\n",
    "    published_date = enriched_entry.get(\"published_date\", \"Unknown Date\")\n",
    "    source_name = enriched_entry.get(\"source\", \"Unknown Source\")\n",
    "    author = enriched_entry.get(\"author\", \"Unknown Author\")\n",
    "    category = enriched_entry.get(\"category\", \"Unknown Category\")\n",
    "    summary = enriched_entry.get(\"enriched_content\", \"No summary available\")[:500]  # Truncate if too long\n",
    "    \n",
    "    tfidf_outliers = json.loads(enriched_entry.get(\"TF-IDF Outliers\", \"[]\"))  # Convert back to list\n",
    "    tfidf_outliers_str = \", \".join(tfidf_outliers) if tfidf_outliers else \"None\"\n",
    "    \n",
    "    grammar_errors = enriched_entry.get(\"Grammar Errors\", 0)\n",
    "    sentence_count = enriched_entry.get(\"Sentence Count\", 0)\n",
    "    sentiment_polarity = enriched_entry.get(\"Sentiment Polarity\", \"Unknown\")\n",
    "    sentiment_subjectivity = enriched_entry.get(\"Sentiment Subjectivity\", \"Unknown\")\n",
    "    fact_checking_summary = enriched_entry.get(\"fact_checking_summary\", \"No fact-checking data available.\")\n",
    "\n",
    "\n",
    "    # Construct the prompt_05\n",
    "    prompt = f\"\"\"\n",
    "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
    "    \n",
    "    üì∞ **Article Information:**\n",
    "    - **Title:** {title}\n",
    "    - **URL:** {url}\n",
    "    - **Published Date:** {published_date}\n",
    "    - **Source:** {source_name}\n",
    "    - **Author:** {author}\n",
    "    - **Category:** {category}\n",
    "\n",
    "    üîπ **Article Summary (Extracted via AI):**\n",
    "    \"{summary}\"\n",
    "    \n",
    "    üìä **Linguistic Analysis:**\n",
    "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** {tfidf_outliers_str}\n",
    "    - **Grammar Issues:** {grammar_errors} errors\n",
    "    - **Sentence Count:** {sentence_count}\n",
    "    - **Sentiment Analysis:** \n",
    "        - **Polarity (Scale -1 to 1):** {sentiment_polarity}\n",
    "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** {sentiment_subjectivity}\n",
    "\n",
    "    üéØ **Your Task:**\n",
    "    1Ô∏è‚É£ **Assess the credibility of this article** based on the provided content and linguistic analysis.  \n",
    "    2Ô∏è‚É£ **Use the TF-IDF outlier words** to determine if the article contains **unusual phrasing or misleading language.**  \n",
    "    3Ô∏è‚É£ **Analyze sentiment:** Does the emotional tone suggest bias, fear-mongering, or objectivity?  \n",
    "    4Ô∏è‚É£ **Evaluate readability & grammar:** Is the article professionally written, or does it contain errors typical of misinformation?  \n",
    "    5Ô∏è‚É£ **Compare against reliable sources** if possible, to determine factual accuracy.  \n",
    "    \n",
    "    üèÜ **Final Response Format:**\n",
    "    - **Credibility Score:** (Scale 0-100, where 100 = totally credible, 0 = completely false)\n",
    "    - **Verdict:** (Choose one: \"True\", \"False\", or \"Misleading\")\n",
    "    - **Explanation:** (2-3 sentences summarizing why you assigned this rating)\n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "example_entry = {\n",
    "    \"title\": \"Breaking News: AI Solves World Hunger\",\n",
    "    \"url\": \"https://news.example.com/ai-hunger\",\n",
    "    \"published_date\": \"2025-02-18\",\n",
    "    \"source\": \"Example News\",\n",
    "    \"author\": \"John Doe\",\n",
    "    \"category\": \"Technology\",\n",
    "    \"enriched_content\": \"AI has made significant advancements... (summary content here)...\\n\\nFact-Checking Data:\\n- Verified by multiple sources\",\n",
    "    \"TF-IDF Outliers\": json.dumps([\"AI\", \"breakthrough\", \"hunger crisis\"]),\n",
    "    \"Grammar Errors\": 2,\n",
    "    \"Sentence Count\": 25,\n",
    "    \"Sentiment Polarity\": 0.7,\n",
    "    \"Sentiment Subjectivity\": 0.4,\n",
    "    \"fact_checking_summary\": \"Verified by multiple sources.\"\n",
    "}\n",
    "\n",
    "# Generate the prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "print(prompt_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02. PROMPT_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
      "    \n",
      "    üì∞ **Article Information:**\n",
      "    - **Title:** Breaking News: AI Solves World Hunger\n",
      "    - **URL:** https://news.example.com/ai-hunger\n",
      "    - **Published Date:** 2025-02-18\n",
      "    - **Source:** Example News\n",
      "    - **Author:** John Doe\n",
      "    - **Category:** Technology\n",
      "\n",
      "    üîπ **Article Summary (Extracted via AI):**\n",
      "    \"AI has made significant advancements... (summary content here)...\n",
      "\n",
      "Fact-Checking Data:\n",
      "- Verified by multiple sources\"\n",
      "    \n",
      "    üìä **Linguistic Analysis:**\n",
      "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** AI, breakthrough, hunger crisis\n",
      "    - **Grammar Issues:** 2 errors\n",
      "    - **Sentence Count:** 25\n",
      "    - **Sentiment Analysis:** \n",
      "        - **Polarity (Scale -1 to 1):** 0.7\n",
      "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** 0.4\n",
      "\n",
      " ## üéØ Task: Evaluate Credibility and Truthfulness  \n",
      "    Based on the provided information, conduct a critical analysis following these points:  \n",
      "\n",
      "    1Ô∏è‚É£ **Credibility Assessment:**  \n",
      "       - Is the source reliable?  \n",
      "       - Does the author have legitimate or recognized credentials in the field?  \n",
      "       - Does the article follow a logical and professional structure, or does it appear poorly written?  \n",
      "\n",
      "    2Ô∏è‚É£ **Detection of Misleading or Sensationalist Language:**  \n",
      "       - Analyze the unusual words detected by TF-IDF. Are these terms uncommon in serious journalism?  \n",
      "       - Does the article use exaggerated language to manipulate the reader‚Äôs emotions?  \n",
      "\n",
      "    3Ô∏è‚É£ **Bias and Subjectivity:**  \n",
      "       - Does the content appear neutral, or does it attempt to influence the reader‚Äôs opinion?  \n",
      "       - Are there phrases that exaggerate, alarm, or contain subjective judgments?  \n",
      "\n",
      "    4Ô∏è‚É£ **Verification with Other Sources:**  \n",
      "       - If a key fact is mentioned, is there verifiable evidence from reliable sources?  \n",
      "       - Are there missing expert citations or solid references?  \n",
      "\n",
      "    5Ô∏è‚É£ **Linguistic Quality:**  \n",
      "       - Does the text contain unusual grammatical errors for legitimate news articles?  \n",
      "       - Does it appear to be an automatically generated or poorly translated text?  \n",
      "\n",
      "    ---  \n",
      "    \n",
      "    ## üìå **Expected Response Format**  \n",
      "    - **Credibility Score (0-100):** (100 = Fully credible, 0 = Completely false)  \n",
      "    - **Final Verdict:** (\"True\", \"False\", or \"Misleading\")  \n",
      "    - **Detailed Explanation (3-5 sentences):** Justify the evaluation based on findings.  \n",
      "\n",
      "    ‚ö†Ô∏è **If the information is insufficient, indicate that more context or additional sources are needed.**  \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def construct_fact_checking_prompt(enriched_entry):\n",
    "    \"\"\"\n",
    "    Constructs a well-formatted prompt for fact-checking using DeepSeek LLM.\n",
    "    \n",
    "    Args:\n",
    "        enriched_entry (dict): A dictionary containing article metadata, enriched content, and linguistic analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string prompt for the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract relevant fields from enriched_entry\n",
    "    title = enriched_entry.get(\"title\", \"Unknown Title\")\n",
    "    url = enriched_entry.get(\"url\", \"Unknown URL\")\n",
    "    published_date = enriched_entry.get(\"published_date\", \"Unknown Date\")\n",
    "    source_name = enriched_entry.get(\"source\", \"Unknown Source\")\n",
    "    author = enriched_entry.get(\"author\", \"Unknown Author\")\n",
    "    category = enriched_entry.get(\"category\", \"Unknown Category\")\n",
    "    summary = enriched_entry.get(\"enriched_content\", \"No summary available\")[:500]  # Truncate if too long\n",
    "    \n",
    "    tfidf_outliers = json.loads(enriched_entry.get(\"TF-IDF Outliers\", \"[]\"))  # Convert back to list\n",
    "    tfidf_outliers_str = \", \".join(tfidf_outliers) if tfidf_outliers else \"None\"\n",
    "    \n",
    "    grammar_errors = enriched_entry.get(\"Grammar Errors\", 0)\n",
    "    sentence_count = enriched_entry.get(\"Sentence Count\", 0)\n",
    "    sentiment_polarity = enriched_entry.get(\"Sentiment Polarity\", \"Unknown\")\n",
    "    sentiment_subjectivity = enriched_entry.get(\"Sentiment Subjectivity\", \"Unknown\")\n",
    "    fact_checking_summary = enriched_entry.get(\"fact_checking_summary\", \"No fact-checking data available.\")\n",
    "\n",
    "\n",
    "    \n",
    "  # Construct the prompt_06\n",
    "    prompt = f\"\"\"\n",
    "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
    "    \n",
    "    üì∞ **Article Information:**\n",
    "    - **Title:** {title}\n",
    "    - **URL:** {url}\n",
    "    - **Published Date:** {published_date}\n",
    "    - **Source:** {source_name}\n",
    "    - **Author:** {author}\n",
    "    - **Category:** {category}\n",
    "\n",
    "    üîπ **Article Summary (Extracted via AI):**\n",
    "    \"{summary}\"\n",
    "    \n",
    "    üìä **Linguistic Analysis:**\n",
    "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** {tfidf_outliers_str}\n",
    "    - **Grammar Issues:** {grammar_errors} errors\n",
    "    - **Sentence Count:** {sentence_count}\n",
    "    - **Sentiment Analysis:** \n",
    "        - **Polarity (Scale -1 to 1):** {sentiment_polarity}\n",
    "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** {sentiment_subjectivity}\n",
    "\n",
    " ## üéØ Task: Evaluate Credibility and Truthfulness  \n",
    "    Based on the provided information, conduct a critical analysis following these points:  \n",
    "\n",
    "    1Ô∏è‚É£ **Credibility Assessment:**  \n",
    "       - Is the source reliable?  \n",
    "       - Does the author have legitimate or recognized credentials in the field?  \n",
    "       - Does the article follow a logical and professional structure, or does it appear poorly written?  \n",
    "\n",
    "    2Ô∏è‚É£ **Detection of Misleading or Sensationalist Language:**  \n",
    "       - Analyze the unusual words detected by TF-IDF. Are these terms uncommon in serious journalism?  \n",
    "       - Does the article use exaggerated language to manipulate the reader‚Äôs emotions?  \n",
    "\n",
    "    3Ô∏è‚É£ **Bias and Subjectivity:**  \n",
    "       - Does the content appear neutral, or does it attempt to influence the reader‚Äôs opinion?  \n",
    "       - Are there phrases that exaggerate, alarm, or contain subjective judgments?  \n",
    "\n",
    "    4Ô∏è‚É£ **Verification with Other Sources:**  \n",
    "       - If a key fact is mentioned, is there verifiable evidence from reliable sources?  \n",
    "       - Are there missing expert citations or solid references?  \n",
    "\n",
    "    5Ô∏è‚É£ **Linguistic Quality:**  \n",
    "       - Does the text contain unusual grammatical errors for legitimate news articles?  \n",
    "       - Does it appear to be an automatically generated or poorly translated text?  \n",
    "\n",
    "    ---  \n",
    "    \n",
    "    ## üìå **Expected Response Format**  \n",
    "    - **Credibility Score (0-100):** (100 = Fully credible, 0 = Completely false)  \n",
    "    - **Final Verdict:** (\"True\", \"False\", or \"Misleading\")  \n",
    "    - **Detailed Explanation (3-5 sentences):** Justify the evaluation based on findings.  \n",
    "\n",
    "    ‚ö†Ô∏è **If the information is insufficient, indicate that more context or additional sources are needed.**  \n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example Usage\n",
    "example_entry = {\n",
    "    \"title\": \"Breaking News: AI Solves World Hunger\",\n",
    "    \"url\": \"https://news.example.com/ai-hunger\",\n",
    "    \"published_date\": \"2025-02-18\",\n",
    "    \"source\": \"Example News\",\n",
    "    \"author\": \"John Doe\",\n",
    "    \"category\": \"Technology\",\n",
    "    \"enriched_content\": \"AI has made significant advancements... (summary content here)...\\n\\nFact-Checking Data:\\n- Verified by multiple sources\",\n",
    "    \"TF-IDF Outliers\": json.dumps([\"AI\", \"breakthrough\", \"hunger crisis\"]),\n",
    "    \"Grammar Errors\": 2,\n",
    "    \"Sentence Count\": 25,\n",
    "    \"Sentiment Polarity\": 0.7,\n",
    "    \"Sentiment Subjectivity\": 0.4,\n",
    "    \"fact_checking_summary\": \"Verified by multiple sources.\"\n",
    "}\n",
    "\n",
    "# Generate the prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "print(prompt_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03. PROMPT_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
      "    \n",
      "    üì∞ **Article Information:**\n",
      "    - **Title:** Breaking News: AI Solves World Hunger\n",
      "    - **URL:** https://news.example.com/ai-hunger\n",
      "    - **Published Date:** 2025-02-18\n",
      "    - **Source:** Example News\n",
      "    - **Author:** John Doe\n",
      "    - **Category:** Technology\n",
      "\n",
      "    üîπ **Article Summary (Extracted via AI):**\n",
      "    \"AI has made significant advancements... (summary content here)...\n",
      "\n",
      "Fact-Checking Data:\n",
      "- Verified by multiple sources\"\n",
      "    \n",
      "    üìä **Linguistic Analysis:**\n",
      "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** AI, breakthrough, hunger crisis\n",
      "    - **Grammar Issues:** 2 errors\n",
      "    - **Sentence Count:** 25\n",
      "    - **Sentiment Analysis:** \n",
      "        - **Polarity (Scale -1 to 1):** 0.7\n",
      "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** 0.4\n",
      "\n",
      " ## üéØ  Task: Evaluate Credibility and Truthfulness\n",
      "    Based on the provided information, conduct a critical analysis following these detailed tasks:\n",
      "\n",
      "    1. Source Credibility Analysis:\n",
      "       - Assess the reliability of the source and its reputation in journalism.\n",
      "       - Identify any potential conflicts of interest or biases within the source.\n",
      "       - Determine if the author has known expertise or credentials in the subject matter.\n",
      "\n",
      "    2. Content Verification:\n",
      "       - Cross-check key claims with verifiable and authoritative sources.\n",
      "       - Identify any exaggerations, misleading statements, or unverified claims.\n",
      "       - Evaluate if the article presents evidence and factual backing for its assertions.\n",
      "\n",
      "    3. Detection of Misleading or Sensationalist Language:\n",
      "       - Analyze the tone and wording of the article to identify emotional manipulation.\n",
      "       - Assess the presence of exaggerated or alarmist phrases that may indicate bias.\n",
      "       - Determine whether the article includes balanced viewpoints or only presents one-sided perspectives.\n",
      "\n",
      "    4. Bias and Subjectivity Evaluation:\n",
      "       - Determine whether the article contains subjective language or ideological framing.\n",
      "       - Identify any patterns of bias based on the article's structure, language, and omitted information.\n",
      "       - Consider whether the article serves an agenda beyond objective reporting.\n",
      "\n",
      "    5. External Source Comparison:\n",
      "       - Check if other reputable news sources report on the same topic and whether their coverage aligns.\n",
      "       - Identify inconsistencies in reporting across different sources.\n",
      "       - Evaluate whether primary sources (official statements, research papers, etc.) support the claims made.\n",
      "\n",
      "    6. Quality of Writing and Presentation:\n",
      "       - Analyze the grammatical accuracy and coherence of the text.\n",
      "       - Identify any unusual phrasing that may indicate automatic content generation or poor translation.\n",
      "       - Determine if the article follows standard journalistic practices in terms of citations and formatting.\n",
      "\n",
      "    ---\n",
      "    \n",
      "    Expected Response Format:\n",
      "    - Credibility Score (0-100): (100 = Fully credible, 0 = Completely false)\n",
      "    - Final Verdict: (\"True\", \"False\", or \"Misleading\")\n",
      "    - Detailed Explanation (4-6 sentences): Justify the evaluation based on findings, referencing key factors from the analysis.\n",
      "\n",
      "    If the information is insufficient to determine credibility, specify what additional context or sources would be necessary to reach a conclusive assessment.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def construct_fact_checking_prompt(enriched_entry):\n",
    "    \"\"\"\n",
    "    Constructs a well-formatted prompt for fact-checking using DeepSeek LLM.\n",
    "    \n",
    "    Args:\n",
    "        enriched_entry (dict): A dictionary containing article metadata, enriched content, and linguistic analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string prompt for the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract relevant fields from enriched_entry\n",
    "    title = enriched_entry.get(\"title\", \"Unknown Title\")\n",
    "    url = enriched_entry.get(\"url\", \"Unknown URL\")\n",
    "    published_date = enriched_entry.get(\"published_date\", \"Unknown Date\")\n",
    "    source_name = enriched_entry.get(\"source\", \"Unknown Source\")\n",
    "    author = enriched_entry.get(\"author\", \"Unknown Author\")\n",
    "    category = enriched_entry.get(\"category\", \"Unknown Category\")\n",
    "    summary = enriched_entry.get(\"enriched_content\", \"No summary available\")[:500]  # Truncate if too long\n",
    "    \n",
    "    tfidf_outliers = json.loads(enriched_entry.get(\"TF-IDF Outliers\", \"[]\"))  # Convert back to list\n",
    "    tfidf_outliers_str = \", \".join(tfidf_outliers) if tfidf_outliers else \"None\"\n",
    "    \n",
    "    grammar_errors = enriched_entry.get(\"Grammar Errors\", 0)\n",
    "    sentence_count = enriched_entry.get(\"Sentence Count\", 0)\n",
    "    sentiment_polarity = enriched_entry.get(\"Sentiment Polarity\", \"Unknown\")\n",
    "    sentiment_subjectivity = enriched_entry.get(\"Sentiment Subjectivity\", \"Unknown\")\n",
    "    fact_checking_summary = enriched_entry.get(\"fact_checking_summary\", \"No fact-checking data available.\")\n",
    "\n",
    "\n",
    "    \n",
    "  # Construct the prompt_07\n",
    "    prompt = f\"\"\"\n",
    "    You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
    "    \n",
    "    üì∞ **Article Information:**\n",
    "    - **Title:** {title}\n",
    "    - **URL:** {url}\n",
    "    - **Published Date:** {published_date}\n",
    "    - **Source:** {source_name}\n",
    "    - **Author:** {author}\n",
    "    - **Category:** {category}\n",
    "\n",
    "    üîπ **Article Summary (Extracted via AI):**\n",
    "    \"{summary}\"\n",
    "    \n",
    "    üìä **Linguistic Analysis:**\n",
    "    - **TF-IDF Outlier Keywords (Unique/Unusual Words):** {tfidf_outliers_str}\n",
    "    - **Grammar Issues:** {grammar_errors} errors\n",
    "    - **Sentence Count:** {sentence_count}\n",
    "    - **Sentiment Analysis:** \n",
    "        - **Polarity (Scale -1 to 1):** {sentiment_polarity}\n",
    "        - **Subjectivity (Scale 0 to 1, higher = opinionated):** {sentiment_subjectivity}\n",
    "\n",
    " ## üéØ  Task: Evaluate Credibility and Truthfulness\n",
    "    Based on the provided information, conduct a critical analysis following these detailed tasks:\n",
    "\n",
    "    1. Source Credibility Analysis:\n",
    "       - Assess the reliability of the source and its reputation in journalism.\n",
    "       - Identify any potential conflicts of interest or biases within the source.\n",
    "       - Determine if the author has known expertise or credentials in the subject matter.\n",
    "\n",
    "    2. Content Verification:\n",
    "       - Cross-check key claims with verifiable and authoritative sources.\n",
    "       - Identify any exaggerations, misleading statements, or unverified claims.\n",
    "       - Evaluate if the article presents evidence and factual backing for its assertions.\n",
    "\n",
    "    3. Detection of Misleading or Sensationalist Language:\n",
    "       - Analyze the tone and wording of the article to identify emotional manipulation.\n",
    "       - Assess the presence of exaggerated or alarmist phrases that may indicate bias.\n",
    "       - Determine whether the article includes balanced viewpoints or only presents one-sided perspectives.\n",
    "\n",
    "    4. Bias and Subjectivity Evaluation:\n",
    "       - Determine whether the article contains subjective language or ideological framing.\n",
    "       - Identify any patterns of bias based on the article's structure, language, and omitted information.\n",
    "       - Consider whether the article serves an agenda beyond objective reporting.\n",
    "\n",
    "    5. External Source Comparison:\n",
    "       - Check if other reputable news sources report on the same topic and whether their coverage aligns.\n",
    "       - Identify inconsistencies in reporting across different sources.\n",
    "       - Evaluate whether primary sources (official statements, research papers, etc.) support the claims made.\n",
    "\n",
    "    6. Quality of Writing and Presentation:\n",
    "       - Analyze the grammatical accuracy and coherence of the text.\n",
    "       - Identify any unusual phrasing that may indicate automatic content generation or poor translation.\n",
    "       - Determine if the article follows standard journalistic practices in terms of citations and formatting.\n",
    "\n",
    "    ---\n",
    "    \n",
    "    Expected Response Format:\n",
    "    - Credibility Score (0-100): (100 = Fully credible, 0 = Completely false)\n",
    "    - Final Verdict: (\"True\", \"False\", or \"Misleading\")\n",
    "    - Detailed Explanation (4-6 sentences): Justify the evaluation based on findings, referencing key factors from the analysis.\n",
    "\n",
    "    If the information is insufficient to determine credibility, specify what additional context or sources would be necessary to reach a conclusive assessment.\n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example Usage\n",
    "example_entry = {\n",
    "    \"title\": \"Breaking News: AI Solves World Hunger\",\n",
    "    \"url\": \"https://news.example.com/ai-hunger\",\n",
    "    \"published_date\": \"2025-02-18\",\n",
    "    \"source\": \"Example News\",\n",
    "    \"author\": \"John Doe\",\n",
    "    \"category\": \"Technology\",\n",
    "    \"enriched_content\": \"AI has made significant advancements... (summary content here)...\\n\\nFact-Checking Data:\\n- Verified by multiple sources\",\n",
    "    \"TF-IDF Outliers\": json.dumps([\"AI\", \"breakthrough\", \"hunger crisis\"]),\n",
    "    \"Grammar Errors\": 2,\n",
    "    \"Sentence Count\": 25,\n",
    "    \"Sentiment Polarity\": 0.7,\n",
    "    \"Sentiment Subjectivity\": 0.4,\n",
    "    \"fact_checking_summary\": \"Verified by multiple sources.\"\n",
    "}\n",
    "\n",
    "# Generate the prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. PROMPT_08 (FINAL)\n",
    "I've enhanced the prompt by adding Chain-of-Thought reasoning and Few-Shot learning examples to guide the model in generating more accurate fact-checking responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a fact-checking AI analyzing the credibility of a news article. Follow the structured process below to evaluate its reliability step by step.\n",
      "    \n",
      "    ## Article Information:\n",
      "    - **Title:** Breaking News: AI Solves World Hunger\n",
      "    - **URL:** https://news.example.com/ai-hunger\n",
      "    - **Published Date:** 2025-02-18\n",
      "    - **Source:** Example News\n",
      "    - **Author:** John Doe\n",
      "    - **Category:** Technology\n",
      "    \n",
      "    ## Article Summary (Extracted via AI):\n",
      "    \"AI has made significant advancements... (summary content here)...\n",
      "\n",
      "Fact-Checking Data:\n",
      "- Verified by multiple sources\"\n",
      "    \n",
      "    ## Linguistic Analysis:\n",
      "    - **TF-IDF Outlier Keywords (Unusual Terms):** AI, breakthrough, hunger crisis\n",
      "    - **Grammar Issues:** 2 errors\n",
      "    - **Sentence Count:** 25\n",
      "    - **Sentiment Analysis:**\n",
      "        - **Polarity (Scale -1 to 1):** 0.7\n",
      "        - **Subjectivity (Scale 0 to 1, where 1 = highly opinionated):** 0.4\n",
      "    \n",
      "    ---\n",
      "    \n",
      "    ## Step-by-Step Fact-Checking Process:\n",
      "    ### 1. Source Credibility Analysis\n",
      "    - Is the source reputable, unbiased, and known for accurate reporting?\n",
      "    - Does the author have expertise in this subject?\n",
      "    - Are there any known biases or conflicts of interest?\n",
      "    \n",
      "    ### 2. Content Verification\n",
      "    - Identify key factual claims in the article.\n",
      "    - Cross-check these claims with authoritative sources (official reports, academic papers, reputable news organizations).\n",
      "    - Are there missing citations or unverifiable claims?\n",
      "    \n",
      "    ### 3. Misleading or Sensationalist Language Detection\n",
      "    - Does the article use emotionally charged or manipulative language?\n",
      "    - Are there exaggerated statements, alarmist phrases, or one-sided narratives?\n",
      "    - Compare the tone to neutral, fact-based reporting.\n",
      "    \n",
      "    ### 4. Bias and Subjectivity Evaluation\n",
      "    - Assess if the article favors a particular viewpoint.\n",
      "    - Does it omit relevant facts that would provide a balanced perspective?\n",
      "    - Look for patterns of ideological framing.\n",
      "    \n",
      "    ### 5. External Source Comparison\n",
      "    - Are other reputable sources reporting on the same topic?\n",
      "    - If so, do their accounts align with this article?\n",
      "    - Are primary sources (official documents, expert opinions) cited and accurately represented?\n",
      "    \n",
      "    ### 6. Quality of Writing and Presentation\n",
      "    - Does the article follow proper journalistic standards?\n",
      "    - Are there spelling, grammar, or formatting issues that suggest a lack of editorial oversight?\n",
      "    - Is there evidence of automated content generation or poor translation?\n",
      "    \n",
      "    ---\n",
      "    \n",
      "    ## Expected Response Format (Use Chain-of-Thought Reasoning)\n",
      "    \n",
      "    ### Example Analysis (Few-Shot Learning Reference)\n",
      "    **Article Title:** \"Company XYZ Announces Revolutionary Cancer Treatment\"\n",
      "    \n",
      "    **Step 1 - Source Credibility:**\n",
      "    - The article is from \"HealthNewsNow.com,\" which is not a well-known medical journal.\n",
      "    - The author has no medical credentials.\n",
      "    \n",
      "    **Step 2 - Content Verification:**\n",
      "    - The claim that \"XYZ's drug cures cancer in 90% of cases\" is not supported by any clinical trial data.\n",
      "    - No references to peer-reviewed research were found.\n",
      "    \n",
      "    **Step 3 - Sensationalist Language:**\n",
      "    - The article repeatedly uses phrases like \"miracle cure\" and \"breakthrough,\" suggesting hype over factual accuracy.\n",
      "    \n",
      "    **Step 4 - Bias and Framing:**\n",
      "    - The piece heavily promotes the company's stock, indicating a financial motive.\n",
      "    \n",
      "    **Step 5 - External Source Comparison:**\n",
      "    - No major medical journals or regulatory agencies have published similar findings.\n",
      "    \n",
      "    **Step 6 - Writing Quality:**\n",
      "    - Several grammar errors and an informal tone reduce credibility.\n",
      "    \n",
      "    **Final Verdict:** \"Misleading\"\n",
      "    **Credibility Score:** 25/100\n",
      "    **Explanation:** The article lacks verifiable sources, contains promotional bias, and exaggerates claims without evidence.\n",
      "    \n",
      "    ---\n",
      "    \n",
      "    Now, apply this step-by-step method to the given article and provide your evaluation.\n",
      "    \n",
      "    **Final Expected Output:**\n",
      "    - **Credibility Score (0-100):** (100 = Fully credible, 0 = Completely false)\n",
      "    - **Final Verdict:** (\"True\", \"False\", or \"Misleading\")\n",
      "    - **Detailed Explanation (4-6 sentences)** referencing key findings from each fact-checking step.\n",
      "    - **If insufficient information is available, specify what additional sources or context are needed.**\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def construct_fact_checking_prompt(enriched_entry):\n",
    "    \"\"\"\n",
    "    Constructs a well-formatted prompt for fact-checking using DeepSeek LLM.\n",
    "    \n",
    "    Args:\n",
    "        enriched_entry (dict): A dictionary containing article metadata, enriched content, and linguistic analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string prompt for the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract relevant fields from enriched_entry\n",
    "    title = enriched_entry.get(\"title\", \"Unknown Title\")\n",
    "    url = enriched_entry.get(\"url\", \"Unknown URL\")\n",
    "    published_date = enriched_entry.get(\"published_date\", \"Unknown Date\")\n",
    "    source_name = enriched_entry.get(\"source\", \"Unknown Source\")\n",
    "    author = enriched_entry.get(\"author\", \"Unknown Author\")\n",
    "    category = enriched_entry.get(\"category\", \"Unknown Category\")\n",
    "    summary = enriched_entry.get(\"enriched_content\", \"No summary available\")[:500]  # Truncate if too long\n",
    "    \n",
    "    tfidf_outliers = json.loads(enriched_entry.get(\"TF-IDF Outliers\", \"[]\"))  # Convert back to list\n",
    "    tfidf_outliers_str = \", \".join(tfidf_outliers) if tfidf_outliers else \"None\"\n",
    "    \n",
    "    grammar_errors = enriched_entry.get(\"Grammar Errors\", 0)\n",
    "    sentence_count = enriched_entry.get(\"Sentence Count\", 0)\n",
    "    sentiment_polarity = enriched_entry.get(\"Sentiment Polarity\", \"Unknown\")\n",
    "    sentiment_subjectivity = enriched_entry.get(\"Sentiment Subjectivity\", \"Unknown\")\n",
    "    fact_checking_summary = enriched_entry.get(\"fact_checking_summary\", \"No fact-checking data available.\")\n",
    "\n",
    "\n",
    "    \n",
    "  # Construct the prompt_08\n",
    "    prompt = f\"\"\"\n",
    "    You are a fact-checking AI analyzing the credibility of a news article. Follow the structured process below to evaluate its reliability step by step.\n",
    "    \n",
    "    ## Article Information:\n",
    "    - **Title:** {title}\n",
    "    - **URL:** {url}\n",
    "    - **Published Date:** {published_date}\n",
    "    - **Source:** {source_name}\n",
    "    - **Author:** {author}\n",
    "    - **Category:** {category}\n",
    "    \n",
    "    ## Article Summary (Extracted via AI):\n",
    "    \"{summary}\"\n",
    "    \n",
    "    ## Linguistic Analysis:\n",
    "    - **TF-IDF Outlier Keywords (Unusual Terms):** {tfidf_outliers_str}\n",
    "    - **Grammar Issues:** {grammar_errors} errors\n",
    "    - **Sentence Count:** {sentence_count}\n",
    "    - **Sentiment Analysis:**\n",
    "        - **Polarity (Scale -1 to 1):** {sentiment_polarity}\n",
    "        - **Subjectivity (Scale 0 to 1, where 1 = highly opinionated):** {sentiment_subjectivity}\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    ## Step-by-Step Fact-Checking Process:\n",
    "    ### 1. Source Credibility Analysis\n",
    "    - Is the source reputable, unbiased, and known for accurate reporting?\n",
    "    - Does the author have expertise in this subject?\n",
    "    - Are there any known biases or conflicts of interest?\n",
    "    \n",
    "    ### 2. Content Verification\n",
    "    - Identify key factual claims in the article.\n",
    "    - Cross-check these claims with authoritative sources (official reports, academic papers, reputable news organizations).\n",
    "    - Are there missing citations or unverifiable claims?\n",
    "    \n",
    "    ### 3. Misleading or Sensationalist Language Detection\n",
    "    - Does the article use emotionally charged or manipulative language?\n",
    "    - Are there exaggerated statements, alarmist phrases, or one-sided narratives?\n",
    "    - Compare the tone to neutral, fact-based reporting.\n",
    "    \n",
    "    ### 4. Bias and Subjectivity Evaluation\n",
    "    - Assess if the article favors a particular viewpoint.\n",
    "    - Does it omit relevant facts that would provide a balanced perspective?\n",
    "    - Look for patterns of ideological framing.\n",
    "    \n",
    "    ### 5. External Source Comparison\n",
    "    - Are other reputable sources reporting on the same topic?\n",
    "    - If so, do their accounts align with this article?\n",
    "    - Are primary sources (official documents, expert opinions) cited and accurately represented?\n",
    "    \n",
    "    ### 6. Quality of Writing and Presentation\n",
    "    - Does the article follow proper journalistic standards?\n",
    "    - Are there spelling, grammar, or formatting issues that suggest a lack of editorial oversight?\n",
    "    - Is there evidence of automated content generation or poor translation?\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    ## Expected Response Format (Use Chain-of-Thought Reasoning)\n",
    "    \n",
    "    ### Example Analysis (Few-Shot Learning Reference)\n",
    "    **Article Title:** \"Company XYZ Announces Revolutionary Cancer Treatment\"\n",
    "    \n",
    "    **Step 1 - Source Credibility:**\n",
    "    - The article is from \"HealthNewsNow.com,\" which is not a well-known medical journal.\n",
    "    - The author has no medical credentials.\n",
    "    \n",
    "    **Step 2 - Content Verification:**\n",
    "    - The claim that \"XYZ's drug cures cancer in 90% of cases\" is not supported by any clinical trial data.\n",
    "    - No references to peer-reviewed research were found.\n",
    "    \n",
    "    **Step 3 - Sensationalist Language:**\n",
    "    - The article repeatedly uses phrases like \"miracle cure\" and \"breakthrough,\" suggesting hype over factual accuracy.\n",
    "    \n",
    "    **Step 4 - Bias and Framing:**\n",
    "    - The piece heavily promotes the company's stock, indicating a financial motive.\n",
    "    \n",
    "    **Step 5 - External Source Comparison:**\n",
    "    - No major medical journals or regulatory agencies have published similar findings.\n",
    "    \n",
    "    **Step 6 - Writing Quality:**\n",
    "    - Several grammar errors and an informal tone reduce credibility.\n",
    "    \n",
    "    **Final Verdict:** \"Misleading\"\n",
    "    **Credibility Score:** 25/100\n",
    "    **Explanation:** The article lacks verifiable sources, contains promotional bias, and exaggerates claims without evidence.\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    Now, apply this step-by-step method to the given article and provide your evaluation.\n",
    "    \n",
    "    **Final Expected Output:**\n",
    "    - **Credibility Score (0-100):** (100 = Fully credible, 0 = Completely false)\n",
    "    - **Final Verdict:** (\"True\", \"False\", or \"Misleading\")\n",
    "    - **Detailed Explanation (4-6 sentences)** referencing key findings from each fact-checking step.\n",
    "    - **If insufficient information is available, specify what additional sources or context are needed.**\n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example Usage\n",
    "example_entry = {\n",
    "    \"title\": \"Breaking News: AI Solves World Hunger\",\n",
    "    \"url\": \"https://news.example.com/ai-hunger\",\n",
    "    \"published_date\": \"2025-02-18\",\n",
    "    \"source\": \"Example News\",\n",
    "    \"author\": \"John Doe\",\n",
    "    \"category\": \"Technology\",\n",
    "    \"enriched_content\": \"AI has made significant advancements... (summary content here)...\\n\\nFact-Checking Data:\\n- Verified by multiple sources\",\n",
    "    \"TF-IDF Outliers\": json.dumps([\"AI\", \"breakthrough\", \"hunger crisis\"]),\n",
    "    \"Grammar Errors\": 2,\n",
    "    \"Sentence Count\": 25,\n",
    "    \"Sentiment Polarity\": 0.7,\n",
    "    \"Sentiment Subjectivity\": 0.4,\n",
    "    \"fact_checking_summary\": \"Verified by multiple sources.\"\n",
    "}\n",
    "\n",
    "# Generate the prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEND PROMP TO THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_Deepseek R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out the credibility of this news article that says AI has solved world hunger. The title is pretty dramatic: \"AI Solves World Hunger.\" Let's break down the process step by step.\n",
      "\n",
      "First off, the source is \"Example News,\" which seems a bit generic and not very reputable. I don't recognize it as a well-known news outlet, so that raises some red flags for me. Usually, high-quality news sources have names like The New York Times or BBC News. This one might be a fake or a lesser-known site.\n",
      "\n",
      "The author is John Doe, who just did a quick check on his LinkedIn‚Äîit seems he's an AI expert but doesn't have any notable credentials in global hunger research either. His background might not add much credibility since it's focused on AI without expertise in food distribution systems.\n",
      "\n",
      "Looking at the date of publication, it's from 2025-02-18. That's pretty recent, so I wonder if this is a newly released piece that hasn't been widely vetted yet. Sometimes breakthroughs get hyped up quickly, especially with AI being such a hot topic these days.\n",
      "\n",
      "Now, the summary says AI has made significant advancements, but without specifics on how exactly they've solved hunger‚Äîlike through crop optimization or supply chain improvements‚Äîit's hard to take this at face value. AI can be applied in various ways, so it's not clear if there are concrete results here.\n",
      "\n",
      "The linguistic analysis mentions TF-IDF outlier keywords like \"AI,\" \"breakthrough,\" and \"hunger crisis.\" That might make the article sound more sensational than accurate. Words like these often lead to exaggerated claims without solid evidence.\n",
      "\n",
      "There are also two grammar errors, which isn't great for a fact-checking process. Typos or incorrect sentence structures can undermine an article's credibility because it shows that editing was done with carelessness.\n",
      "\n",
      "The article has 25 sentences, but they might all be repeating the same information or using overly complex language without adding new insights. Without citations to peer-reviewed studies or official reports, it's hard to verify if these claims are backed by evidence.\n",
      "\n",
      "Looking at the sentiment analysis, it's positive with a high polarity and moderate subjectivity. This could mean the article is pushing a message that's somewhat opinionated but doesn't outright lie‚Äîit might be trying to create an emotional response by highlighting AI's potential.\n",
      "\n",
      "Now, thinking about external sources, I don't know if other reputable news outlets have covered this same topic recently. If they have, do their stories align with this one? For example, has Time Magazine or The Washington Post reported something similar?\n",
      "\n",
      "The quality of writing could use improvement too. Some articles are more engaging than others based on the language used and how information is presented. If there's a lot of fluff without clear data points, it might not be as credible.\n",
      "\n",
      "So putting it all together: The source isn't reputable, the author lacks relevant expertise, grammar issues exist, key facts aren't verified with sources, and the article uses sensationalist language. All these factors point towards the article being misleading or at least lacking solid evidence to back up its claims.\n",
      "</think>\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "- **Credibility Score:** 20/100\n",
      "- **Final Verdict:** Misleading\n",
      "- **Detailed Explanation:**  \n",
      "The article from \"Example News\" asserts that AI has solved world hunger, which is a highly sensational claim. The source lacks credibility as it's not a reputable news organization and the author, John Doe, doesn't have expertise in global hunger research. The article uses sensationalist language with terms like \"breakthrough\" and employs grammar errors and a complex structure without sufficient evidence or citations to verify its claims. External sources are unclear, and the narrative lacks balance and factual support.\n",
      "\n",
      "- **Additional Information Needed:** More context from reputable news outlets covering similar topics would be necessary to assess for alignment and credibility of the AI's role in addressing hunger.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Generate the fact-checking prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "\n",
    "# Send the prompt to DeepSeek LLM using Ollama\n",
    "response = ollama.chat(\n",
    "    model=\"deepseek-r1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOMATE PROCESS AND SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Presidents Are Often Judged By History Through The Lens Of Morality\",\n",
      "        \"url\": \"https://newsone.com/5939034/presidents-are-judged-by-history-through-the-lens-of-morality/\",\n",
      "        \"published_date\": \"2025-02-17T14:33:46+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"George R. Goethals, University of Richmond\",\n",
      "        \"fact_check_result\": \"<think>\\nAlright, so I need to figure out if the given news article is reliable by doing a fact-checking analysis. Let me start by reading through the article carefully and then verifying each piece of information against credible sources or existing knowledge.\\n\\nFirst, the article's title mentions that presidents are judged by history through the lens of morality. It says that Lincoln is highly rated because of his leadership during the Civil War. The source is an unknown entity from 2025, which already seems a bit odd since news articles from that future date aren't available yet. Maybe it's just placeholder information.\\n\\nLooking at the content, the article discusses historical surveys where presidents are ranked based on various criteria, including public persuasion, economic management, moral authority, etc. It cites an example of C-SPAN conducting surveys every time there was a new administration since 2000, which includes 2009, 2017, and 2021, plus the upcoming 2025 survey.\\n\\nThe article also mentions that Lincoln consistently ranks at the top in each survey. The presidents immediately before him, Franklin Pierce and James Buchanan, are at the bottom, along with Andrew Johnson. Then it talks about Donald Trump's ranking near the bottom in 2021, though I know he was president from 2017 to 2021. The article also brings up specific examples like Wilson's handling of segregation affecting his moral authority, and Jackson'sTrail of Tears making him lose rank.\\n\\nI should check if these survey results are accurate. From what I remember, the C-SPAN surveys do exist. They started in 1962 with Arthur Schlesinger Jr., so that part is plausible. The specific rankings for each president align somewhat with my knowledge‚ÄîLincoln being highly regarded for moral authority and economic policies.\\n\\nHowever, I'm a bit unsure about the exact positions of every president mentioned, like Grant moving up significantly in 2021 versus his earlier ratings. I might need to look that up to confirm if his ranking indeed jumped from near the bottom in previous surveys to sixth place now.\\n\\nAlso, the article attributes the rise in Grant's moral authority ranking due to equal justice being central these days. That makes sense because equal justice is a significant issue nowadays with so many Supreme Court cases affecting civil rights and equality.\\n\\nAnother point is about Jackson dropping rank due to Indian policies leading to the Trail of Tears. I know that his treatment of Native Americans was brutal, but I'm not certain if this led to such a significant drop in multiple surveys. It might be worth checking if historical analyses support such a large change.\\n\\nThe mention of moral authority being crucial is also something I can verify. If presidents' moral commitments are key, then the drops and rises make sense based on their policies, especially regarding civil rights, equality, etc.\\n\\nI should also consider whether the article's structure makes logical sense. It starts with an analysis, moves into specific examples, and then discusses implications. That flow seems coherent for a news piece explaining historical evaluation methods.\\n\\nOverall, while most points seem plausible given my existing knowledge about U.S. history and presidential evaluations, there are areas where I might need to verify the exact rankings and how they've changed over time. The presence of credible sources like C-SPAN surveys supports some parts, but without live data from 2025, it's a bit speculative.\\n</think>\\n\\nThe article in question presents an analysis of how historians evaluate presidential leadership through historical surveys conducted by C-SPAN since 2000. Here is the fact-checking analysis:\\n\\n1. **Source and Date**: The article mentions a source as unknown from 2025, which is impractical because it refers to future events that haven't occurred yet.\\n\\n2. **Survey Existence**: The article cites C-SPAN conducting surveys since 1962 with Arthur Schlesinger Jr., which aligns with known historical data about such evaluations.\\n\\n3. **Presidential Rankings**: The article states that Lincoln consistently ranks at the top, while Franklin Pierce and James Buchanan are at the bottom along with Andrew Johnson. This is plausible based on historical contexts of their policies.\\n\\n4. ** Donald Trump's Ranking**: The article mentions Trump ranking near the bottom in 2021, which is consistent with his administration's handling of civil rights issues.\\n\\n5. **Specific Examples**:\\n   - **Wilson**: His handling of segregation affecting moral authority is plausible.\\n   - **Jackson**: The Trail of Tears leading to a drop in rank is supported by historical evidence.\\n\\n6. **Grant's Rising Rank**: The article notes Grant moving up significantly, which could be due to current societal emphasis on equal justice, though specific data would be needed for verification.\\n\\n7. **Moral Authority Importance**: This is a key factor in evaluations, as evidenced by the significant drops and rises of presidents based on their policies.\\n\\n8. **Structure**: The article's logical flow makes sense, explaining historical evaluation methods before providing examples and implications.\\n\\n**Conclusion**: While most points are plausible and supported by existing knowledge, areas requiring verification include exact rankings over time and specific historical data about presidential evaluations. Overall, the article presents a reasonable overview with some speculative elements due to its futuristic context.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"The Prince and Princess of Wales' ¬£33k-per-week Mustique villa has a 60ft pool\",\n",
      "        \"url\": \"https://www.hellomagazine.com/travel/814676/kate-middleton-prince-william-villa-antilles-mustique/\",\n",
      "        \"published_date\": \"2025-02-17T14:33:35+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Georgia Brown\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so I need to figure out whether the news article about Prince and Princess of Wales' ¬£33k-per-week villa in Mustique with a 60ft pool is reliable. The user provided some details: the title, an unknown source, published on February 17, 2025 at 14:33:35, and the content was a 404 error.\\n\\nFirst, I'll start by looking at each component of this information to assess reliability.\\n\\nThe title mentions the Prince and Princess of Wales' villa in Mustique. That's a specific location known for luxury properties, so that's a good sign. The price is ¬£33k per week, which seems reasonable for such a place but not extremely high. It also notes there's a 60ft pool, which adds to the exclusivity.\\n\\nThe source is unknown, which raises some red flags because without knowing who published it or if they're reputable, it's harder to assess reliability. If it's from a well-known news outlet, that would add credibility. An unknown source could mean it's from a smaller publication or maybe even a rumor mill.\\n\\nThe date is February 17, 2025. That seems recent enough for a news item, but I should check if there are any corroborating sources. If the information came directly from the couple themselves or a trusted agent, that would help verify the details.\\n\\nThe content was a 404 error when trying to access it. A 404 means the page doesn't exist on the website where it's supposed to be. This could mean the article never went live, was moved off the site, or perhaps it's just not available publicly anymore. Without more information, I can't tell why the page isn't found.\\n\\nNow, considering all this: The specific location is credible, but without a known source, it's hard to trust the report. The 404 error suggests that if the article existed online before, maybe it's been taken down or moved. The price and pool size are plausible for a luxury property in Mustique.\\n\\nI should also think about where such information might come from. Maybe the couple listed it on their own website? Or perhaps it was shared by a trusted contact. Without more context, it's difficult to assess further. I don't have access to check if this information is corroborated elsewhere because I can't look up external sources.\\n\\nIn conclusion, while some details are credible, the lack of a known source and the 404 error make the article's reliability questionable. It might be worth checking multiple sources or contacting someone familiar with the couple for verification.\\n</think>\\n\\n**Analysis of the News Article Reliability**\\n\\nThe news article in question provides specific details about the Prince and Princess of Wales' ¬£33k-per-week villa in Mustique, which includes a 60ft pool. The analysis considers several factors to assess reliability:\\n\\n1. **Source Credibility**: The source is unknown, which raises concerns about credibility. Reputable news outlets typically provide their sources or have verifiable information.\\n\\n2. **Date of Publication**: The article was published on February 17, 2025. This recent date suggests it might be timely, but without corroborating evidence, its existence is uncertain.\\n\\n3. **Content Availability**: A 404 error indicates the page may no longer be accessible or has been removed from the website hosting it.\\n\\n4. **Location and Details**: Mustique is a known luxury destination, making the location credible. The price and pool size are plausible for such a property.\\n\\n5. **Verification Methods**: Without access to external sources or corroborating information, verifying the article's authenticity is challenging.\\n\\n**Conclusion**: While the specific details about the villa seem credible due to its location and features, the lack of a known source and the 404 error make the article's reliability questionable. It may be advisable to seek additional verification from trusted contacts or sources.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Scottish museum to undergo 'bold transformation' after ¬£2.6m government funding boost\",\n",
      "        \"url\": \"https://www.scotsman.com/news/scottish-news/scottish-museum-to-undergo-bold-transformation-after-ps26m-government-funding-boost-4994958\",\n",
      "        \"published_date\": \"2025-02-17T14:32:22+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Jane Bradley\",\n",
      "        \"fact_check_result\": \"<think>\\nAlright, so I need to figure out if the given news article is reliable based on the information provided. Let me start by looking at the details given.\\n\\nThe title says that a Scottish museum is undergoing a \\\"bold transformation\\\" after receiving ¬£2.6 million in government funding. The source is unknown, and it was published on February 17, 2025, at 14:32:22 UTC with a status code of 403.\\n\\nHmm, the first thing I notice is that the source isn't named. Without knowing who provided this information, it's hard to assess reliability. Government funding projects often come from reputable departments like HM Treasury or similar, but without that info, we can't be sure.\\n\\nNext, the article mentions a \\\"bold transformation.\\\" That term seems a bit vague. It could mean significant changes in exhibits, layout, technology integration, etc., but it's not specific enough to evaluate if the project is well-conceived or executed.\\n\\nThe funding amount is ¬£2.6 million. I wonder how that compares to previous projects and whether such funding typically results in meaningful transformations. Maybe this isn't a record high, so without context, it's hard to say if the money alone justifies the transformation.\\n\\nThe publication date is 2025-02-17T14:32:22+00:00. That's within days of being published today (assuming this response is in early 2025). Newswires usually get updated with any corrections or clarifications, so a quick publication might indicate it's the latest info without major changes expected soon.\\n\\nThe content status code is 403. I'm not familiar with what 403 means offhand, but maybe it's an error message on some platforms indicating a server-side issue like a firewall blocking access or a timeout. If that's the case, maybe the article isn't accessible through standard sources, so relying solely on the snippet might be unreliable.\\n\\nPutting this all together: The lack of a source makes it hard to verify credibility. The vague description doesn't give detailed information about the project, which is crucial for assessing reliability. A quick publication date might mean it's timely but not necessarily comprehensive. And the 403 status suggests some technical issue with accessing the article.\\n\\nSo, considering these factors, I would say this news article isn't reliable enough to cite without more details and verification from a reputable source.\\n</think>\\n\\nBased on the analysis provided, the news article in question is not deemed reliable due to several factors:\\n\\n1. **Unknown Source**: The absence of a known and credible source makes it difficult to assess the article's credibility.\\n\\n2. **Vague Description**: The term \\\"bold transformation\\\" lacks specificity, hindering evaluation of the project's scope and quality.\\n\\n3. **Technical Issue (403 Status)**: The error code suggests potential access issues, which could affect the accuracy and completeness of the information provided.\\n\\n4. **Quick Publication Date**: While timely publication might indicate relevance, it doesn't provide sufficient context for a thorough assessment.\\n\\nGiven these considerations, the article's reliability is uncertain without further details and verification from reputable sources.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"The Lathums bring forward release of new album ‚ÄòMatter Does Not Define‚Äô\",\n",
      "        \"url\": \"https://www.nme.com/news/music/the-lathums-bring-forward-release-of-new-album-matter-does-not-define-3838653?utm_source=rss&utm_medium=rss&utm_campaign=the-lathums-bring-forward-release-of-new-album-matter-does-not-define\",\n",
      "        \"published_date\": \"2025-02-17T14:32:14+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Laura Molloy\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so the user provided a news article about The Lathums releasing their new album earlier than expected. They asked if it's reliable and want a fact-checking analysis.\\n\\nFirst, I'll read through the article carefully to note any details that might be unusual or suspect. The release date is February 28 instead of March 7‚Äîthat‚Äôs two weeks early. I know record labels often require advance notice for pre-orders, so maybe this isn't an issue with the band but perhaps a strategic move.\\n\\nThey mentioned pre-savings and orders from the store getting the album a week sooner. That makes sense as promotions are common to entice purchases. The album is their third, which isn‚Äôt uncommon, especially if they've been active in the scene since 2023.\\n\\nThe single 'No Direction' was released last summer before the new album, so that's consistent with typical album drops. Their previous album had a UK Number One, which is impressive but not unheard of for rock bands. The tour dates are listed clearly and cover multiple cities, including festivals like TRNSMT and Neighbourhood Weekender‚Äîthose are well-known events.\\n\\nThe quotes from Alex Moore about recording with John Kettle and the creative process seem genuine, given that it's a press release. He also mentions the album being \\\"real, raw,\\\" which is typical for bands aiming to be authentic.\\n\\nI don't see any red flags here. The timing aligns with promotional strategies, the tour dates are plausible, and there's no information suggesting any irregularities in their process or performance. So, considering all these points, the article seems reliable.\\n</think>\\n\\nThe article appears to be a well-crafted press release from what appears to be an independent source for *The Lathums*. The reporting aligns with typical promotional practices, including details about pre-order incentives and tour scheduling. There are no red flags or inconsistencies that would suggest the information is unreliable.\\n\\nFact-checking criteria applied:\\n\\n1. **Release Timing**: The advance notice of a release date being pulled forward (from March 7 to February 28) aligns with common promotional practices, where record labels often require advance notice for pre-orders and marketing campaigns.\\n\\n2. **Single Release**: The mention of the single \\\"No Direction\\\" being released last summer before the album's official drop is consistent with typical album promotion strategies.\\n\\n3. **Tour Dates**: The listed tour dates are spread across major cities in the UK and include notable festivals, which is plausible for a band's promotional schedule.\\n\\n4. **Artist Quotes**: The quotes from Alex Moore provide insight into their creative process and align with the typical tone of an artist statement.\\n\\n5. **Consistency**: There are no inconsistencies or contradictory details that would raise suspicion about the article's reliability.\\n\\nOverall, based on the information provided, the article is likely reliable as it follows standard promotional practices and does not contain any anomalies.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Trump begins firings of FAA air traffic control staff just weeks after fatal DC plane crash\",\n",
      "        \"url\": \"https://www.toledoblade.com/news/nation/2025/02/17/trump-begins-firings-of-faa-air-traffic-control-staff-after-fatal-dc-plane-crash/stories/20250217084\",\n",
      "        \"published_date\": \"2025-02-17T14:31:22+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Unknown author\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so I need to figure out if the given news article is reliable by doing a fact check. Let me start by looking at what's provided.\\n\\nThe title says Trump started firing FAA air traffic control staff just after a fatal DC plane crash, which happened weeks before. That seems pretty concerning because it connects a major accident with job losses in a critical field like aviation safety. I know that in the past, there have been high-profile firings related to crashes, but this is the first time I'm hearing about such a direct link.\\n\\nLooking at the content section, the user input just says \\\"Enter-To-Win\\\" and some sections like Obituaries, Jobs, etc., but it's mostly navigation links. It doesn't actually provide any details or context beyond the main header line. So, there's very little information to go on besides the title.\\n\\nNow, I need to assess reliability. First, is the source credible? The article says \\\"Source: Unknown source,\\\" which means we don't know where it's coming from. If it's from a reputable news outlet like ABC News or NBC, that would be good. But if it's an unknown or less-known source, that could raise red flags.\\n\\nNext, checking for verifiability. The article claims a specific event: Trump firings after a crash. Without any quotes or citations, I can't verify if this is accurate or from a reliable source. It sounds too sensational and unusual to come from a well-established news outlet without more context.\\n\\nThere's also a lack of context about the crash itself. When did it happen? Was it a recent crash? How many people were on board? What was the cause? Without that information, it's hard to assess if the firing is related or coincidental.\\n\\nAnother point is the immediate response. If a major accident happens and then there are job losses in such a critical field as air traffic control, I would expect media coverage from known outlets. This article doesn't mention any other news about this event, which makes me question its credibility.\\n\\nI also think about whether such an event would be covered by any mainstream media. If it's a rare and specific event, maybe it isn't widely reported yet. But the combination of a crash and subsequent firings seems like something that should get attention from at least some news outlets, especially if it's a significant number of people affected.\\n\\nThe fact that there are no details about the fires or how many staff were let go makes it hard to assess whether this is normal or unusual in such a situation. Typically, during an accident, agencies might conduct investigations and then decide on personnel actions based on the findings, but without more info, I can't tell if they did that.\\n\\nAlso, checking for bias: if Trump is involved, even if he's unrelated, could there be political motives? But the article doesn't specify any connection to Trump beyond the firings. However, given the timing just after a crash, it might seem like an attempt to downplay the incident by linking it to personnel changes.\\n\\nIn summary, without more context, details about the crash, and citations from reputable sources, this article is hard to verify as reliable. It's possible that the information is correct but comes from an unknown source, or maybe it's just sensationalized for a story.\\n</think>\\n\\nThe news article in question makes a significant claim by linking Trump's firings of FAA air traffic control staff directly to a fatal DC plane crash weeks prior. However, upon fact-checking, several factors suggest that the article may not be reliable:\\n\\n1. **Source Credibility**: The article cites an \\\"Unknown source,\\\" which raises concerns about its credibility as reputable news outlets typically provide bylines for their articles.\\n\\n2. **Lack of Context and Verifiability**: Without detailed information about the crash‚Äîsuch as dates, causes, or numbers affected‚Äîit is difficult to verify the accuracy of the claim. The article lacks quotes or citations from credible sources, making it hard to assess veracity.\\n\\n3. **Sequencing of Events**: The connection between the crash and the firings seems coincidental without additional context on immediate actions taken by authorities post-crash.\\n\\n4. **Media Coverage**: Such an event involving a critical field like aviation control typically draws media attention, especially from well-established outlets, which is absent here.\\n\\n5. **Potential Bias**: While no bias towards Trump is mentioned, the timing of the firings could be seen as an attempt to downplay the crash's impact, though this speculation lacks evidence.\\n\\nIn conclusion, due to insufficient information and lack of corroborating evidence from reputable sources, the article's reliability remains uncertain.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "import chromadb  # Ensure chromadb is installed and properly set up\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_entries_from_chromadb(collection_name=\"news_articles\", max_entries=5):\n",
    "    \"\"\"\n",
    "    Fetches the latest enriched entries from ChromaDB.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): The ChromaDB collection to query.\n",
    "        max_entries (int): The maximum number of entries to fetch.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of retrieved entries.\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")  # Update path as needed\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "    \n",
    "    # Retrieve latest X entries\n",
    "    results = collection.get(include=[\"metadatas\", \"documents\"], limit=max_entries)\n",
    "\n",
    "    # Process entries into required format\n",
    "    entries = []\n",
    "    for i in range(len(results[\"documents\"])):\n",
    "        entry = {\n",
    "            \"title\": results[\"metadatas\"][i].get(\"title\", \"Unknown Title\"),\n",
    "            \"url\": results[\"metadatas\"][i].get(\"url\", \"Unknown URL\"),\n",
    "            \"published_date\": results[\"metadatas\"][i].get(\"published_date\", \"Unknown Date\"),\n",
    "            \"source\": results[\"metadatas\"][i].get(\"source\", \"Unknown Source\"),\n",
    "            \"author\": results[\"metadatas\"][i].get(\"author\", \"Unknown Author\"),\n",
    "            \"category\": results[\"metadatas\"][i].get(\"category\", \"Unknown Category\"),\n",
    "            \"enriched_content\": results[\"documents\"][i],\n",
    "            \"TF-IDF Outliers\": json.dumps(results[\"metadatas\"][i].get(\"tfidf_outliers\", [])),\n",
    "            \"Grammar Errors\": results[\"metadatas\"][i].get(\"grammar_errors\", 0),\n",
    "            \"Sentence Count\": results[\"metadatas\"][i].get(\"sentence_count\", 0),\n",
    "            \"Sentiment Polarity\": results[\"metadatas\"][i].get(\"sentiment_polarity\", \"Unknown\"),\n",
    "            \"Sentiment Subjectivity\": results[\"metadatas\"][i].get(\"sentiment_subjectivity\", \"Unknown\"),\n",
    "            \"fact_checking_summary\": results[\"metadatas\"][i].get(\"fact_checking_summary\", \"No fact-checking data available.\"),\n",
    "        }\n",
    "        entries.append(entry)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def construct_fact_checking_prompt(entry):\n",
    "    \"\"\"\n",
    "    Constructs a fact-checking prompt for the LLM.\n",
    "\n",
    "    Args:\n",
    "        entry (dict): A dictionary containing article details.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt.\n",
    "    \"\"\"\n",
    "    return f\"Analyze the following news article:\\n\\nTitle: {entry['title']}\\nSource: {entry['source']}\\nPublished: {entry['published_date']}\\nContent: {entry['enriched_content']}\\n\\nIs this news article reliable? Provide a fact-checking analysis.\"\n",
    "\n",
    "def get_unique_filename(base_path=\"PROMPT_RESULTS\"):\n",
    "    \"\"\"\n",
    "    Generates a unique filename with the format: Prompt_Return_YYYY-MM-DD_N.json.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The directory where files will be stored.\n",
    "\n",
    "    Returns:\n",
    "        str: The full path of the unique filename.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Get current date\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    file_number = 1\n",
    "\n",
    "    while True:\n",
    "        filename = f\"Prompt_Return_{date_str}_{file_number}.json\"\n",
    "        full_path = os.path.join(base_path, filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            return full_path\n",
    "        file_number += 1\n",
    "\n",
    "def analyze_multiple_articles(chroma_entries, model=\"deepseek-r1\"):\n",
    "    \"\"\"\n",
    "    Sends multiple articles from ChromaDB to the DeepSeek LLM for analysis and saves the results.\n",
    "\n",
    "    Args:\n",
    "        chroma_entries (list): List of enriched article entries from ChromaDB.\n",
    "        model (str): The LLM model to use.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of responses from the LLM.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for entry in chroma_entries:\n",
    "        prompt_text = construct_fact_checking_prompt(entry)\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        )\n",
    "        response_content = response['message']['content']\n",
    "        responses.append({\n",
    "            \"title\": entry.get(\"title\", \"Unknown Title\"),\n",
    "            \"url\": entry.get(\"url\", \"Unknown URL\"),\n",
    "            \"published_date\": entry.get(\"published_date\", \"Unknown Date\"),\n",
    "            \"source\": entry.get(\"source\", \"Unknown Source\"),\n",
    "            \"author\": entry.get(\"author\", \"Unknown Author\"),\n",
    "            \"fact_check_result\": response_content\n",
    "        })\n",
    "    \n",
    "    # Generate unique filename\n",
    "    output_file = get_unique_filename()\n",
    "\n",
    "    # Save responses to a JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(responses, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return responses\n",
    "\n",
    "# ** Fetch limited entries from ChromaDB and send them for analysis **\n",
    "chroma_entries = fetch_entries_from_chromadb(max_entries=5)\n",
    "results = analyze_multiple_articles(chroma_entries)\n",
    "\n",
    "# Output results\n",
    "print(json.dumps(results, indent=4, ensure_ascii=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fundamental Differences Between the Four Prompts  \n",
    "\n",
    "## Prompt 5  \n",
    "- Focuses on **historical context** and **content consistency**.  \n",
    "- Lacks **deep source credibility verification** and **external fact-checking mechanisms**.  \n",
    "- Limited **bias and misleading language analysis**.  \n",
    "\n",
    "## Prompt 6  \n",
    "- Introduces **source credibility evaluation** and **basic bias detection**.  \n",
    "- Begins to analyze **methodological clarity** and **fact-checking transparency**.  \n",
    "- Lacks **structured Chain-of-Thought reasoning**.  \n",
    "\n",
    "## Prompt 7  \n",
    "- Expands **source credibility checks** with **institutional transparency and citation verification**.  \n",
    "- Includes **misleading language detection**, **tone manipulation analysis**, and **media influence evaluation**.  \n",
    "- Integrates **fact-checking cross-references** with **official reports and authoritative sources**.  \n",
    "- Considers **paywalls, 404 errors, and content manipulation**.  \n",
    "- Uses **Chain-of-Thought reasoning** but lacks structured examples.  \n",
    "\n",
    "## Prompt 8  \n",
    "- Fully structured **step-by-step fact-checking framework**.  \n",
    "- Uses **Chain-of-Thought reasoning** and **Few-Shot learning examples**.  \n",
    "- Evaluates **bias, misleading language, external validation, and source credibility**.  \n",
    "- **Most comprehensive prompt**, guiding the model through a **detailed verification process**.  \n",
    "\n",
    "---\n",
    "\n",
    "# Comparative Summary of Results  \n",
    "\n",
    "## Presidential Rankings Article  \n",
    "- **Prompt 5**: Basic historical context and content analysis.  \n",
    "- **Prompt 6**: Adds bias detection and methodological scrutiny.  \n",
    "- **Prompt 7**: Enhances source transparency checks.  \n",
    "- **Prompt 8**: Uses structured fact-checking, citing **historical data validation**.  \n",
    "- **Best Performance**: **Prompt 8**, for its structured evaluation and Chain-of-Thought reasoning.  \n",
    "\n",
    "## Prince and Princess of Wales' Mustique Villa  \n",
    "- **Prompt 5**: Identifies lack of reputable sources.  \n",
    "- **Prompt 6**: Adds **404 error analysis** and considers content removal.  \n",
    "- **Prompt 7**: Investigates **possible content manipulation**.  \n",
    "- **Prompt 8**: **Examines digital forensic aspects and source credibility evaluation**.  \n",
    "- **Best Performance**: **Prompt 8**, for comprehensive **source validation and misinformation detection**.  \n",
    "\n",
    "## Scottish Museum Government Funding  \n",
    "- **Prompt 5**: Checks general **government funding credibility**.  \n",
    "- **Prompt 6**: Adds **external source validation**.  \n",
    "- **Prompt 7**: Evaluates **paywalls and content accessibility**.  \n",
    "- **Prompt 8**: Expands into **cross-referencing with institutional reports**.  \n",
    "- **Best Performance**: **Prompt 8**, for detailed **fact-checking and external validation**.  \n",
    "\n",
    "## The Lathums Album Release  \n",
    "- **Prompt 5**: Checks **content consistency** but lacks source verification.  \n",
    "- **Prompt 6**: Introduces **fact-checking against past media coverage**.  \n",
    "- **Prompt 7**: Examines **marketing influences on media narratives**.  \n",
    "- **Prompt 8**: **Cross-references with promotional strategies and industry standards**.  \n",
    "- **Best Performance**: **Prompt 8**, for evaluating **industry patterns and media accuracy**.  \n",
    "\n",
    "## Trump and FAA Air Traffic Control Firings  \n",
    "- **Prompt 5**: Identifies **timeline inconsistencies**.  \n",
    "- **Prompt 6**: Adds **basic fact-checking against historical events**.  \n",
    "- **Prompt 7**: Expands **government accountability verification**.  \n",
    "- **Prompt 8**: **Uses official aviation reports and cross-referencing techniques**.  \n",
    "- **Best Performance**: **Prompt 8**, for in-depth **fact-checking and misinformation risk analysis**.  \n",
    "\n",
    "---\n",
    "\n",
    "# Final Verdict: Best Prompt for Fake News Detection  \n",
    "The **updated prompt used in File 8** is the most effective because it:  \n",
    "- **Combines Chain-of-Thought reasoning with Few-Shot learning examples**.  \n",
    "- **Includes a structured fact-checking process for source credibility, bias, and misleading language detection**.  \n",
    "- **Incorporates external verification techniques, including cross-referencing authoritative sources**.  \n",
    "- **Considers digital accessibility barriers, such as paywalls, content removals, and misinformation tactics**.  \n",
    "\n",
    "This prompt is the **best fit for an LLM-based fake news detection system**, ensuring **accurate, detailed, and structured verification**.  \n",
    "\n",
    "Would you like additional refinements to further optimize for your specific use case?  \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
