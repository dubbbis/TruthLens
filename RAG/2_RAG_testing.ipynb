{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RAG\n",
    "\n",
    "We will use RAG to enhance the data we send off to the LLM to assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install:\n",
    "\n",
    " pip install nltk spacy textblob scikit-learn\n",
    "\n",
    "\n",
    "### Download English model for Spacy\n",
    " python -m spacy download en_core_web_sm  \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step:\n",
    "Let's look at some of our data in news.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\newpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Run once to download NLTK and Spacy models\n",
    "nltk.download(\"punkt\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load BERT summarizer (once)\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ Total articles in news.json: 49\n",
      "\n",
      "üîπ **Title:** Asking Eric: I get these wild midnight calls from my mother\n",
      "\n",
      "üîπ **Content Length:** 4974 characters\n",
      "\n",
      "üîπ **Content:**\n",
      "\n",
      "Dear Eric: My 78-year-old mother is a very kind and generous person; however, she goes through life \n",
      "looking through a lens of negativity. Whatever can go wrong will go wrong in her eyes. Related Artic\n",
      "les Asking Eric: How do I let strangers know that I‚Äôm not under the influence?\n",
      "\n",
      "Asking Eric: Why did\n",
      " this stranger think I needed help? Was it my clothes?\n",
      "\n",
      "Asking Eric: If she keeps this up, I‚Äôm leavi\n",
      "ng the book club\n",
      "\n",
      "Asking Eric: An ex-friend is trying to get back into our lives, and I‚Äôm afraid my \n",
      "wife will say yes\n",
      "\n",
      "Asking Eric: Should I talk to my sister about her worrisome son?\n",
      "\n",
      "She constantly \n",
      "complains that she has the worst luck, the worst pain and is always on the verge of death. The exagg\n",
      "eration and drama of every situation is a part of her daily living. I get calls in the middle of the\n",
      " night telling me that she‚Äôs going to have a heart attack or a stroke.\n",
      "\n",
      "I live three hours away, so \n",
      "getting to her quickly is a challenge. My siblings no longer want to deal with her.\n",
      "\n",
      "If she calls 91\n",
      "1 and has to go to the hospital, she calls me every hour telling me that they‚Äôre trying to kill her \n",
      "by giving her the wrong medicine, the wrong food or that the room is filthy, and I need to come righ\n",
      "t away to save her.\n",
      "\n",
      "Every situation is grossly exaggerated to the point of being lies. I really can\n",
      "‚Äôt take the drama any longer.\n",
      "\n",
      "I think that she needs to speak to someone that can help her overcome\n",
      " her anxiety and possibly prescribe her beneficial medications, but she won‚Äôt hear of it.\n",
      "\n",
      "She has m\n",
      "issed many family occasions because of her perceived ailments. It‚Äôs sad and frustrating at the same \n",
      "time. What can I do at this point?\n",
      "\n",
      "‚Äì Positivity Is a Choice\n",
      "\n",
      "Dear Positivity: You‚Äôre right that you\n",
      "r mother should speak to someone about her anxiety. It seems unmanageable and is clearly affecting h\n",
      "er well-being. But we can‚Äôt force people to help themselves, which, I know, only adds to your pain a\n",
      "nd frustration.\n",
      "\n",
      "You can, however, set boundaries with your mother that reinforce how important it i\n",
      "s that she finds alternate ways of communicating. Tell her you‚Äôre both going to have to work with wh\n",
      "at‚Äôs possible and effective.\n",
      "\n",
      "For instance, the hourly calls about the hospital trying to kill her a\n",
      "ren‚Äôt effective. It would be more effective for you to get the name of her doctor, speak with that p\n",
      "erson and then set up a schedule of when you‚Äôll check in with your mother.\n",
      "\n",
      "If that‚Äôs not a plan she\n",
      " can accept, even in theory, redirect her. Why does she think the care she receives is bad care? Wha\n",
      "t are other possible solutions? What would be effective?\n",
      "\n",
      "Then, should the situation arise, stick to\n",
      " the boundary that you set. ‚ÄúMom, we talked about this before. I‚Äôm going to talk to the on-call doct\n",
      "or and then I‚Äôll call to check on you. But if you‚Äôre feeling anxious or afraid, we can have a chapla\n",
      "in come visit with you or we can talk to the doctors about medication that will help you relax. What\n",
      " do you want to do?‚Äù\n",
      "\n",
      "Empowering her to seek her own solutions while also holding your boundary is o\n",
      "nly going to get you so far. She has to want to change. But it‚Äôs a first step to keeping some of wha\n",
      "t you describe as the drama at bay.\n",
      "\n",
      "Dear Eric: I have two daughters with special needs. They functi\n",
      "on around the level of a 10-year-old. They are also nonverbal in public.\n",
      "\n",
      "It is very unlikely that t\n",
      "hey will ever have a relationship with anyone, much less have children. So, it is likely that I will\n",
      " never be a grandmother.\n",
      "\n",
      "My question is how do I deal with the grief that causes me?\n",
      "\n",
      "‚Äì Grieving th\n",
      "e Future\n",
      "\n",
      "Dear Future: Talking openly about the way that you‚Äôre feeling ‚Äì with a therapist, with fri\n",
      "ends, with other parents of children with special needs ‚Äì is a good first step.\n",
      "\n",
      "Being able to lay i\n",
      "t all out may not lessen the grief right away, but it will help it to feel more manageable.\n",
      "\n",
      "As a pa\n",
      "rent, you‚Äôre handling a lot. It likely feels isolating. So, reminding yourself that you‚Äôre not alone\n",
      " and you don‚Äôt have to navigate tough feelings alone is important.\n",
      "\n",
      "It‚Äôs also important to remember \n",
      "that there are many kinds of relationships that can offer you the love and emotional connection of g\n",
      "randparenthood. Even if this assurance feels like a cold comfort at the moment, it‚Äôs helpful to thin\n",
      "k through the people in your life and ways that you can be present for each other as life goes on. I\n",
      "t‚Äôs also a good reminder of who you have in your corner.\n",
      "\n",
      "Lastly, think about what you would be hopi\n",
      "ng for as a grandparent. Perhaps it‚Äôs the simple joy of being able to love and care for another chil\n",
      "d (and then return them to their parents at the end of a visit). But perhaps it‚Äôs a more complex des\n",
      "ire.\n",
      "\n",
      "There‚Äôs no wrong way to feel about this. But thinking through what‚Äôs at the root of this grief\n",
      " will help you talk about it, work through it, and find alternatives in the future.\n",
      "\n",
      "Send questions \n",
      "to R. Eric Thomas at eric@askingeric.com or P.O. Box 22474, Philadelphia, PA 19110. Follow him on In\n",
      "stagram @oureric and sign up for his weekly newsletter at rericthomas.com.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load the cleaned dataset\n",
    "with open(\"news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    news_articles = json.load(f)\n",
    "\n",
    "# üîπ CONFIGURATION: Set to True to remove short articles (otherwise, it's OFF)\n",
    "remove_short_articles = False  # Change to True to enable filtering\n",
    "min_content_length = 300  # Change this to filter out articles with content length < X\n",
    "\n",
    "# Count total articles\n",
    "total_articles = len(news_articles)\n",
    "print(f\"üì∞ Total articles in news.json: {total_articles}\")\n",
    "\n",
    "# üîπ Optionally Remove Short Articles\n",
    "if remove_short_articles:\n",
    "    news_articles = [article for article in news_articles if len(article[\"content\"]) >= min_content_length]\n",
    "    print(f\"üóë Removed short articles. Remaining articles: {len(news_articles)}\")\n",
    "\n",
    "    # Save filtered articles back to news.json\n",
    "    with open(\"news.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(news_articles, f, indent=4)\n",
    "    print(\"‚úÖ Filtered news.json saved.\")\n",
    "\n",
    "# Randomly select an article index\n",
    "if len(news_articles) > 0:\n",
    "    article_index = random.randint(0, len(news_articles) - 1)\n",
    "\n",
    "    # Function to format text with line breaks every 100 characters\n",
    "    def format_text(text, max_chars=100):\n",
    "        \"\"\"Inserts line breaks every `max_chars` characters for better readability.\"\"\"\n",
    "        return '\\n'.join([text[i:i+max_chars] for i in range(0, len(text), max_chars)])\n",
    "\n",
    "    # Get the title and content\n",
    "    title = news_articles[article_index]['title']\n",
    "    content = news_articles[article_index]['content']\n",
    "    content_length = len(content)\n",
    "\n",
    "    # Print the formatted article with content length\n",
    "    print(f\"\\nüîπ **Title:** {title}\\n\")\n",
    "    print(f\"üîπ **Content Length:** {content_length} characters\\n\")\n",
    "    print(f\"üîπ **Content:**\\n\")\n",
    "    print(format_text(content))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No articles left after filtering.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning\n",
    "The text inside news.json isn't very clean, let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Removed NULL values. 49 valid articles saved in 'cleaned_news.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the news dataset\n",
    "with open(\"news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    news_articles = json.load(f)\n",
    "\n",
    "# Filter out entries with NULL or empty content\n",
    "cleaned_articles = [\n",
    "    article for article in news_articles\n",
    "    if \"content\" in article and isinstance(article[\"content\"], str) and article[\"content\"].strip()\n",
    "]\n",
    "\n",
    "# Save cleaned dataset\n",
    "with open(\"cleaned_news.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_articles, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Removed NULL values. {len(cleaned_articles)} valid articles saved in 'cleaned_news.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text cleaning complete. `cleaned_news.json` has been updated.\n",
      "‚úÖ We now have 49 valid articles saved in 'cleaned_news.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"cleaned_news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    news_articles = json.load(f)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans and preprocesses text by removing unwanted symbols, metadata, and fixing formatting.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove \"By [Author Name]\" patterns (e.g., \"By JOSH FUNK AP Business Writer\")\n",
    "    text = re.sub(r\"By\\s[A-Z\\s]+(AP|Reuters|BBC|CNN|Business Writer|Correspondent)?\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove Moon Alerts, Horoscope-like structures\n",
    "    text = re.sub(r\"Moon alert\\s.*?\\.\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove \"\\r\\n\" and excessive whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "    # Remove text in square brackets [Like this]\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "\n",
    "    # Remove any remaining excessive punctuation\n",
    "    text = re.sub(r\"[^\\w\\s.,!?'-]\", \"\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning to each article's content\n",
    "for article in news_articles:\n",
    "    if \"content\" in article:\n",
    "        article[\"content\"] = clean_text(article[\"content\"])\n",
    "\n",
    "# Save cleaned data back into cleaned_news.json\n",
    "with open(\"cleaned_news.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_articles, f, indent=4)\n",
    "\n",
    "print(\"‚úÖ Text cleaning complete. `cleaned_news.json` has been updated.\")\n",
    "\n",
    "print(f\"‚úÖ We now have {len(cleaned_articles)} valid articles saved in 'cleaned_news.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing Analysis on the texts again before we keep going..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ Total articles in cleaned_news: 49\n",
      "\n",
      "üîπ **Title:** Sierra Ocean LLC Acquires New Holdings in The Procter & Gamble Company (NYSE:PG)\n",
      "\n",
      "üîπ **Content Length:** 7571 characters\n",
      "\n",
      "üîπ **Content:**\n",
      "\n",
      "Sierra Ocean LLC acquired a new stake in shares of The Procter  Gamble Company NYSEPG - Free Report \n",
      "during the fourth quarter, according to its most recent 13F filing with the Securities  Exchange Com\n",
      "mission. The fund acquired 1,891 shares of the company's stock, valued at approximately 317,000. Get\n",
      " Procter  Gamble alerts Sign Up A number of other hedge funds and other institutional investors have\n",
      " also modified their holdings of PG. Patrick M Sweeney  Associates Inc. grew its stake in Procter  G\n",
      "amble by 1.1 during the 3rd quarter. Patrick M Sweeney  Associates Inc. now owns 5,561 shares of the\n",
      " company's stock valued at 963,000 after acquiring an additional 59 shares in the last quarter. Lake\n",
      "ridge Wealth Management LLC grew its stake in Procter  Gamble by 0.6 during the 4th quarter. Lakerid\n",
      "ge Wealth Management LLC now owns 10,807 shares of the company's stock valued at 1,812,000 after acq\n",
      "uiring an additional 60 shares in the last quarter. Valued Wealth Advisors LLC grew its stake in Pro\n",
      "cter  Gamble by 11.2 during the 3rd quarter. Valued Wealth Advisors LLC now owns 605 shares of the c\n",
      "ompany's stock valued at 105,000 after acquiring an additional 61 shares in the last quarter. Colton\n",
      " Groome Financial Advisors LLC grew its stake in Procter  Gamble by 3.2 during the 4th quarter. Colt\n",
      "on Groome Financial Advisors LLC now owns 1,995 shares of the company's stock valued at 334,000 afte\n",
      "r acquiring an additional 62 shares in the last quarter. Finally, Tevis Investment Management grew i\n",
      "ts stake in Procter  Gamble by 0.6 during the 4th quarter. Tevis Investment Management now owns 11,6\n",
      "33 shares of the company's stock valued at 1,950,000 after acquiring an additional 65 shares in the \n",
      "last quarter. 65.77 of the stock is currently owned . Procter  Gamble Stock Performance PG opened at\n",
      " 162.84 on Monday. The stock has a market cap of 381.84 billion, a PE ratio of 25.93, a PEG ratio of\n",
      " 3.79 and a beta of 0.45. The company has a current ratio of 0.76, a quick ratio of 0.55 and a debt-\n",
      "to-equity ratio of 0.50. The Procter  Gamble Company has a 1 year low of 153.52 and a 1 year high of\n",
      " 180.43. The stock's 50 day simple moving average is 166.51 and its 200-day simple moving average is\n",
      " 169.46. Procter  Gamble NYSEPG - Get Free Report last released its quarterly earnings data on Wedne\n",
      "sday, January 22nd. The company reported 1.88 earnings per share EPS for the quarter, beating analys\n",
      "ts' consensus estimates of 1.86 by 0.02. The business had revenue of 21.88 billion for the quarter, \n",
      "compared to analysts' expectations of 21.66 billion. Procter  Gamble had a return on equity of 33.00\n",
      " and a net margin of 18.35. Procter  Gamble's quarterly revenue was up 2.1 on a year-over-year basis\n",
      ". During the same period in the previous year, the business posted 1.84 earnings per share. Sell-sid\n",
      "e analysts forecast that The Procter  Gamble Company will post 6.91 EPS for the current year. Procte\n",
      "r  Gamble Announces Dividend The business also recently declared a quarterly dividend, which will be\n",
      " paid on Tuesday, February 18th. Shareholders of record on Friday, January 24th will be given a 1.00\n",
      "65 dividend. This represents a 4.03 dividend on an annualized basis and a yield of 2.47. The ex-divi\n",
      "dend date of this dividend is Friday, January 24th. Procter  Gamble's dividend payout ratio DPR is 6\n",
      "4.17. Wall Street Analyst Weigh In PG has been the topic of several research analyst reports. Morgan\n",
      " Stanley lifted their target price on Procter  Gamble from 174.00 to 191.00 and gave the company an \n",
      "overweight rating in a research note on Monday, October 21st. Royal Bank of Canada reaffirmed a sect\n",
      "or perform rating and set a 164.00 target price on shares of Procter  Gamble in a research note on T\n",
      "hursday, January 23rd. Wells Fargo  Company lifted their price target on Procter  Gamble from 176.00\n",
      " to 180.00 and gave the company an overweight rating in a report on Thursday, January 23rd. Evercore\n",
      " ISI lifted their price target on Procter  Gamble from 180.00 to 183.00 and gave the company an outp\n",
      "erform rating in a report on Monday, October 21st. Finally, UBS Group reduced their price target on \n",
      "Procter  Gamble from 196.00 to 189.00 and set a buy rating for the company in a report on Thursday, \n",
      "January 16th. Eight investment analysts have rated the stock with a hold rating, thirteen have assig\n",
      "ned a buy rating and two have issued a strong buy rating to the stock. According to data from Market\n",
      "Beat.com, the stock presently has an average rating of Moderate Buy and a consensus price target of \n",
      "180.53. Read Our Latest Report on PG Insider Buying and Selling at Procter  Gamble In other Procter \n",
      " Gamble news, insider Balaji Purushothaman sold 12,800 shares of the company's stock in a transactio\n",
      "n on Thursday, November 21st. The stock was sold at an average price of 172.73, for a total value of\n",
      " 2,210,944.00. Following the sale, the insider now directly owns 11,595 shares in the company, value\n",
      "d at 2,002,804.35. This represents a 52.47  decrease in their ownership of the stock. The sale was d\n",
      "isclosed in a legal filing with the SEC, which is available at this hyperlink. Also, insider Marc S.\n",
      " Pritchard sold 90,450 shares of the company's stock in a transaction on Thursday, January 23rd. The\n",
      " shares were sold at an average price of 163.84, for a total value of 14,819,328.00. Following the c\n",
      "ompletion of the sale, the insider now owns 172,814 shares in the company, valued at 28,313,845.76. \n",
      "This trade represents a 34.36  decrease in their ownership of the stock. The disclosure for this sal\n",
      "e can be found here. Insiders have sold 151,097 shares of company stock worth 25,635,076 over the la\n",
      "st three months. Company insiders own 0.18 of the company's stock. Procter  Gamble Company Profile P\n",
      "rocter  Gamble Co engages in the provision of branded consumer packaged goods. It operates through t\n",
      "he following segments Beauty, Grooming, Health Care, Fabric and Home Care, and Baby, Feminine and Fa\n",
      "mily Care. The Beauty segment offers hair, skin, and personal care. The Grooming segment consists of\n",
      " shave care like female and male blades and razors, pre and post shave products, and appliances. Fea\n",
      "tured Articles Want to see what other hedge funds are holding PG? Visit HoldingsChannel.com to get t\n",
      "he latest 13F filings and insider trades for The Procter  Gamble Company NYSEPG - Free Report. This \n",
      "instant news alert was generated . This story was reviewed 's editorial team prior to publication. P\n",
      "lease send any questions or comments about this story to contactmarketbeat.com. Before you make your\n",
      " next trade, you'll want to hear this. MarketBeat keeps track of Wall Street's top-rated and best pe\n",
      "rforming research analysts and the stocks they recommend to their clients on a daily basis. Our team\n",
      " has identified the five stocks that top analysts are quietly whispering to their clients to buy now\n",
      " before the broader market catches on... and none of the big name stocks were on the list. They beli\n",
      "eve these five stocks are the five best companies for investors to buy now... See The Five Stocks He\n",
      "re Before you consider Procter  Gamble, you'll want to hear this. MarketBeat keeps track of Wall Str\n",
      "eet's top-rated and best performing research analysts and the stocks they recommend to their clients\n",
      " on a daily basis. MarketBeat has identified the five stocks that top analysts are quietly whisperin\n",
      "g to their clients to buy now before the broader market catches on... and Procter  Gamble wasn't on \n",
      "the list. While Procter  Gamble currently has a Moderate Buy rating among analysts, top-rated analys\n",
      "ts believe these five stocks are better buys. View The Five Stocks Here\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load the cleaned dataset\n",
    "with open(\"cleaned_news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    news_articles = json.load(f)\n",
    "\n",
    "# üîπ CONFIGURATION: Set to True to remove short articles (otherwise, it's OFF)\n",
    "remove_short_articles = False  # Change to True to enable filtering\n",
    "min_content_length = 300  # Change this to filter out articles with content length < X\n",
    "\n",
    "# Count total articles\n",
    "total_articles = len(news_articles)\n",
    "print(f\"üì∞ Total articles in cleaned_news: {total_articles}\")\n",
    "\n",
    "# üîπ Optionally Remove Short Articles\n",
    "if remove_short_articles:\n",
    "    news_articles = [article for article in news_articles if len(article[\"content\"]) >= min_content_length]\n",
    "    print(f\"üóë Removed short articles. Remaining articles: {len(news_articles)}\")\n",
    "\n",
    "    # Save filtered articles back to cleaned_news\n",
    "    with open(\"cleaned_news\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(news_articles, f, indent=4)\n",
    "    print(\"‚úÖ Filtered cleaned_news saved.\")\n",
    "\n",
    "# Randomly select an article index\n",
    "if len(news_articles) > 0:\n",
    "    article_index = random.randint(0, len(news_articles) - 1)\n",
    "\n",
    "    # Function to format text with line breaks every 100 characters\n",
    "    def format_text(text, max_chars=100):\n",
    "        \"\"\"Inserts line breaks every `max_chars` characters for better readability.\"\"\"\n",
    "        return '\\n'.join([text[i:i+max_chars] for i in range(0, len(text), max_chars)])\n",
    "\n",
    "    # Get the title and content\n",
    "    title = news_articles[article_index]['title']\n",
    "    content = news_articles[article_index]['content']\n",
    "    content_length = len(content)\n",
    "\n",
    "    # Print the formatted article with content length\n",
    "    print(f\"\\nüîπ **Title:** {title}\\n\")\n",
    "    print(f\"üîπ **Content Length:** {content_length} characters\\n\")\n",
    "    print(f\"üîπ **Content:**\\n\")\n",
    "    print(format_text(content))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No articles left after filtering.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to summarize text. BERT can help us\n",
    "\n",
    "For state-of-the-art extractive summarization, we can use BERT (transformers).\n",
    "\n",
    "Install it first:\n",
    "\n",
    "pip install transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\newpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summarizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 90\u001b[0m\n\u001b[0;32m     88\u001b[0m     grammar \u001b[38;5;241m=\u001b[39m grammar_analysis(text)\n\u001b[0;32m     89\u001b[0m     sentiment \u001b[38;5;241m=\u001b[39m sentiment_analysis(text)\n\u001b[1;32m---> 90\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m summary_mode \u001b[38;5;28;01melse\u001b[39;00m text  \u001b[38;5;66;03m# creates and saves summary to .json\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     linguistic_features\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: summary,\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfidf_outliers\u001b[39m\u001b[38;5;124m\"\u001b[39m: outliers[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_subjectivity\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentiment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubjectivity\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     99\u001b[0m     })\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Merge analysis into articles\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 72\u001b[0m, in \u001b[0;36mgenerate_summary\u001b[1;34m(text, max_length, min_length)\u001b[0m\n\u001b[0;32m     69\u001b[0m text_chunks \u001b[38;5;241m=\u001b[39m split_text(text)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Step 2: Summarize each chunk separately\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m chunk_summaries \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummary_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Step 3: Merge chunk summaries and summarize the full summary\u001b[39;00m\n\u001b[0;32m     75\u001b[0m merged_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunk_summaries)\n",
      "Cell \u001b[1;32mIn[6], line 72\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     69\u001b[0m text_chunks \u001b[38;5;241m=\u001b[39m split_text(text)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Step 2: Summarize each chunk separately\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m chunk_summaries \u001b[38;5;241m=\u001b[39m [\u001b[43msummarizer\u001b[49m(chunk, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Step 3: Merge chunk summaries and summarize the full summary\u001b[39;00m\n\u001b[0;32m     75\u001b[0m merged_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunk_summaries)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summarizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Toggle: Set to True for summaries, False for full content\n",
    "summary_mode = True  \n",
    "\n",
    "# Load cleaned dataset\n",
    "with open(\"cleaned_news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    news_articles = json.load(f)\n",
    "\n",
    "# Extract valid content\n",
    "documents = [article[\"content\"] for article in news_articles]\n",
    "\n",
    "### üîπ TF-IDF Outlier Analysis\n",
    "def tfidf_outliers(texts, top_n=5):\n",
    "    \"\"\"Finds top N high-TF-IDF words per article.\"\"\"\n",
    "    texts = [text if isinstance(text, str) else \"\" for text in texts]  # Ensure valid text\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    outlier_words = []\n",
    "    for row in tfidf_matrix:\n",
    "        scores = row.toarray()[0]\n",
    "        top_indices = scores.argsort()[-top_n:]\n",
    "        outlier_words.append([feature_names[i] for i in top_indices])\n",
    "\n",
    "    return outlier_words\n",
    "\n",
    "### üîπ Grammar & Readability Analysis\n",
    "def grammar_analysis(text):\n",
    "    \"\"\"Analyzes grammatical complexity and readability.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\"grammar_errors\": 0, \"sentence_count\": 0}  \n",
    "\n",
    "    doc = nlp(text)\n",
    "    errors = sum(1 for token in doc if token.is_oov)  \n",
    "    sentences = len(list(doc.sents))\n",
    "    return {\"grammar_errors\": errors, \"sentence_count\": sentences}\n",
    "\n",
    "### üîπ Sentiment Analysis\n",
    "def sentiment_analysis(text):\n",
    "    \"\"\"Detects sentiment polarity and emotional words.\"\"\"\n",
    "    analysis = TextBlob(text)\n",
    "    return {\"polarity\": analysis.sentiment.polarity, \"subjectivity\": analysis.sentiment.subjectivity}\n",
    "\n",
    "## Long Text summary using BERT\n",
    "\n",
    "def split_text(text, max_tokens=1024):\n",
    "    \"\"\"Splits long text into smaller chunks that fit within BERT's token limit.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunks, current_chunk = [], \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_tokens:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def generate_summary(text, max_length=80, min_length=20):\n",
    "    \"\"\"Generates a summary for large texts efficiently.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"No summary available.\"\n",
    "\n",
    "    # Step 1: Split the text into manageable chunks\n",
    "    text_chunks = split_text(text)\n",
    "\n",
    "    # Step 2: Summarize each chunk separately\n",
    "    chunk_summaries = [summarizer(chunk, max_length=100, min_length=20, do_sample=False)[0][\"summary_text\"] for chunk in text_chunks]\n",
    "\n",
    "    # Step 3: Merge chunk summaries and summarize the full summary\n",
    "    merged_summary = \" \".join(chunk_summaries)\n",
    "\n",
    "    # Step 4: Final summarization to keep it concise\n",
    "    final_summary = summarizer(merged_summary, max_length=100, min_length=20, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Apply linguistic analysis\n",
    "outliers = tfidf_outliers(documents)\n",
    "linguistic_features = []\n",
    "\n",
    "for i, text in enumerate(documents):\n",
    "    grammar = grammar_analysis(text)\n",
    "    sentiment = sentiment_analysis(text)\n",
    "    summary = generate_summary(text) if summary_mode else text  # creates and saves summary to .json\n",
    "\n",
    "    linguistic_features.append({\n",
    "        \"summary\": summary,\n",
    "        \"tfidf_outliers\": outliers[i],\n",
    "        \"grammar_errors\": grammar[\"grammar_errors\"],\n",
    "        \"sentence_count\": grammar[\"sentence_count\"],\n",
    "        \"sentiment_polarity\": sentiment[\"polarity\"],\n",
    "        \"sentiment_subjectivity\": sentiment[\"subjectivity\"],\n",
    "    })\n",
    "\n",
    "# Merge analysis into articles\n",
    "for i, article in enumerate(news_articles):\n",
    "    article[\"linguistic_analysis\"] = linguistic_features[i]\n",
    "\n",
    "# Save the updated dataset\n",
    "with open(\"RAG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_articles, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Linguistic analysis completed. Results saved in 'RAG_news.json'.\")\n",
    "print(f\"üîπ Summary Mode: {'ON' if summary_mode else 'OFF'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to cut down the Ling analysis - separating FULL TEXT to focus on it \\\n",
    "TESTING PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Content removed. Saved 16 entries in 'small_linguistic_news.json'.\n"
     ]
    }
   ],
   "source": [
    "# Load the linguistic analysis dataset\n",
    "with open(\"linguistic_news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    news_articles = json.load(f)\n",
    "\n",
    "# Create a new list without 'content'\n",
    "filtered_articles = []\n",
    "\n",
    "for article in news_articles:\n",
    "    filtered_articles.append({\n",
    "        \"title\": article.get(\"title\", \"No Title\"),\n",
    "        \"linguistic_analysis\": article.get(\"linguistic_analysis\", {})\n",
    "    })\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(\"small_linguistic_news.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_articles, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Content removed. Saved {len(filtered_articles)} entries in 'small_linguistic_news.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
