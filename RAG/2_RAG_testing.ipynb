{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RAG\n",
    "\n",
    "We will use RAG to enhance the data we send off to the LLM to assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install:\n",
    "\n",
    " pip install nltk spacy textblob scikit-learn transformers\n",
    "\n",
    "\n",
    "### Download English model for Spacy\n",
    " python -m spacy download en_core_web_sm  \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "Let's look have our RAG system go through the entries in ChromeDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\newpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Run to download NLTK and Spacy models\n",
    "nltk.download(\"punkt\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load BERT summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve & Load Articles from ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded 69 articles from ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Connect to ChromaDB\n",
    "CHROMA_DB_PATH = \"../chroma_db\"\n",
    "COLLECTION_NAME = \"news_articles\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "# ‚úÖ Load stored articles\n",
    "data = collection.get()\n",
    "documents = data[\"documents\"]\n",
    "metadata = data[\"metadatas\"]\n",
    "\n",
    "# ‚úÖ Convert ChromaDB data into structured format\n",
    "news_articles = []\n",
    "for i in range(len(documents)):\n",
    "    article = metadata[i]\n",
    "    article[\"content\"] = documents[i]\n",
    "    article[\"status\"] = \"raw\"  # Track processing stage\n",
    "    news_articles.append(article)\n",
    "\n",
    "print(f\"üìä Loaded {len(news_articles)} articles from ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean & Preprocess Text\n",
    "The text might not be clean, so let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Smart text cleaning completed for 69 articles.\n"
     ]
    }
   ],
   "source": [
    "def smart_clean_text(text):\n",
    "    \"\"\"Cleans text while preserving meaning, structure, and tone.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # ‚úÖ 1. Normalize spaces and line breaks\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())  # Remove excessive spaces/newlines\n",
    "\n",
    "    # ‚úÖ 2. Remove unnecessary system characters\n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F]\", \"\", text)  # Remove non-printable ASCII characters\n",
    "\n",
    "    # ‚úÖ 3. Remove certain \"breaking\" symbols but keep meaning intact\n",
    "    text = re.sub(r\"[\\u200B-\\u200D\\uFEFF]\", \"\", text)  # Remove zero-width spaces & soft hyphens\n",
    "\n",
    "    # ‚úÖ 4. Preserve quoted text (speech & sources)\n",
    "    # Does NOT remove content inside quotation marks to keep context accurate.\n",
    "\n",
    "    # ‚úÖ 5. Remove unnecessary prefixes without deleting valuable text\n",
    "    text = re.sub(r\"^(By\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*,\\s*\\w+\\s*Writer)\", \"\", text)  # Remove \"By Author, Writer\"\n",
    "    \n",
    "    # ‚úÖ 6. Remove excessive special characters (but keep punctuation & formatting)\n",
    "    text = re.sub(r\"[^\\w\\s.,!?()'\\-\\\"‚Äú‚Äù‚Äò‚Äô]\", \"\", text)  # Keeps normal sentence structure intact\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# ‚úÖ Apply smart cleaning\n",
    "for article in news_articles:\n",
    "    article[\"content\"] = smart_clean_text(article[\"content\"])\n",
    "    article[\"status\"] = \"cleaned\"  # Update processing stage\n",
    "\n",
    "print(f\"‚úÖ Smart text cleaning completed for {len(news_articles)} articles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Smart text cleaning completed for 69 articles.\n"
     ]
    }
   ],
   "source": [
    "#### SECOND OPTION @@@@@ TESTING\n",
    "\n",
    "### CLEANING TEXT\n",
    "\n",
    "import re\n",
    "\n",
    "def smart_clean_text(text):\n",
    "    \"\"\"Cleans text while preserving meaning, structure, and tone.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # ‚úÖ 1. Normalize spaces and line breaks\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())  \n",
    "\n",
    "    # ‚úÖ 2. Remove unnecessary system characters\n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F]\", \"\", text)  \n",
    "\n",
    "    # ‚úÖ 3. Remove certain \"breaking\" symbols but keep meaning intact\n",
    "    text = re.sub(r\"[\\u200B-\\u200D\\uFEFF]\", \"\", text)  \n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# ‚úÖ Apply smart cleaning\n",
    "for article in news_articles:\n",
    "    article[\"content\"] = smart_clean_text(article[\"content\"])\n",
    "    article[\"status\"] = \"cleaned\"  \n",
    "\n",
    "print(f\"‚úÖ Smart text cleaning completed for {len(news_articles)} articles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Linguistic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each metric provides a different perspective on the text:\n",
    "\n",
    "* TF-IDF Outliers ‚Üí Highlights rare but significant words.\n",
    "* Grammar Analysis ‚Üí Assesses complexity and readability.\n",
    "* Sentiment Analysis ‚Üí Identifies bias and emotional intensity.\n",
    "* Summarization ‚Üí Reduces lengthy text while retaining meaning.\n",
    "\n",
    "For state-of-the-art extractive summarization, we can use BERT (transformers).\n",
    "\n",
    "NOTE: THERE'S ROOM HERE FOR IMPROVEMENT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### üîπ TF-IDF Outlier Analysis\n",
    "def tfidf_outliers(texts, top_n=5):\n",
    "    \"\"\"Finds top N high-TF-IDF words per article.\"\"\"\n",
    "    texts = [text if isinstance(text, str) else \"\" for text in texts]\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    outlier_words = []\n",
    "    for row in tfidf_matrix:\n",
    "        scores = row.toarray()[0]\n",
    "        top_indices = scores.argsort()[-top_n:]\n",
    "        outlier_words.append([feature_names[i] for i in top_indices])\n",
    "\n",
    "    return outlier_words\n",
    "\n",
    "### üîπ Grammar & Readability Analysis\n",
    "def grammar_analysis(text):\n",
    "    \"\"\"Analyzes grammatical complexity and readability.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\"grammar_errors\": 0, \"sentence_count\": 0}  \n",
    "\n",
    "    doc = nlp(text)\n",
    "    errors = sum(1 for token in doc if token.is_oov)\n",
    "    sentences = len(list(doc.sents))\n",
    "    return {\"grammar_errors\": errors, \"sentence_count\": sentences}\n",
    "\n",
    "### üîπ Sentiment Analysis\n",
    "def sentiment_analysis(text):\n",
    "    \"\"\"Detects sentiment polarity and emotional words.\"\"\"\n",
    "    analysis = TextBlob(text)\n",
    "    return {\"polarity\": analysis.sentiment.polarity, \"subjectivity\": analysis.sentiment.subjectivity}\n",
    "\n",
    "### üîπ BERT Summarization (Handles Long Texts)\n",
    "def split_text(text, max_tokens=1024):\n",
    "    \"\"\"Splits long text into smaller chunks that fit within BERT's token limit.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunks, current_chunk = [], \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_tokens:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def generate_summary(text, max_length=80, min_length=20):\n",
    "    \"\"\"Generates a summary using BERT (BART Model).\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"No summary available.\"\n",
    "\n",
    "    text_chunks = split_text(text)\n",
    "    chunk_summaries = [summarizer(chunk, max_length=100, min_length=20, do_sample=False)[0][\"summary_text\"] for chunk in text_chunks]\n",
    "    merged_summary = \" \".join(chunk_summaries)\n",
    "\n",
    "    return summarizer(merged_summary, max_length=100, min_length=20, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "# ‚úÖ Process all articles\n",
    "documents = [article[\"content\"] for article in news_articles]\n",
    "outliers = tfidf_outliers(documents)\n",
    "\n",
    "for i, text in enumerate(documents):\n",
    "    grammar = grammar_analysis(text)\n",
    "    sentiment = sentiment_analysis(text)\n",
    "    summary = generate_summary(text)\n",
    "\n",
    "    news_articles[i][\"linguistic_analysis\"] = {\n",
    "        \"summary\": summary,\n",
    "        \"tfidf_outliers\": outliers[i],\n",
    "        \"grammar_errors\": grammar[\"grammar_errors\"],\n",
    "        \"sentence_count\": grammar[\"sentence_count\"],\n",
    "        \"sentiment_polarity\": sentiment[\"polarity\"],\n",
    "        \"sentiment_subjectivity\": sentiment[\"subjectivity\"],\n",
    "    }\n",
    "    news_articles[i][\"status\"] = \"processed\"\n",
    "\n",
    "print(\"‚úÖ Linguistic analysis & summarization completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Summaries\n",
    "\n",
    "$!@$!@#$ NEED TO CHECK AND TEST THIS PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ‚úÖ Load cleaned dataset from ChromaDB (instead of news.json)\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"news_articles\"\n",
    "\n",
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "# ‚úÖ Fetch stored articles from ChromaDB\n",
    "data = collection.get()\n",
    "documents = data[\"documents\"]\n",
    "metadata = data[\"metadatas\"]\n",
    "\n",
    "# ‚úÖ Convert ChromaDB data into structured format\n",
    "news_articles = []\n",
    "for i in range(len(documents)):\n",
    "    article = metadata[i]\n",
    "    article[\"content\"] = documents[i]\n",
    "    article[\"status\"] = \"raw\"\n",
    "    news_articles.append(article)\n",
    "\n",
    "print(f\"üìä Loaded {len(news_articles)} articles from ChromaDB.\")\n",
    "\n",
    "# ‚úÖ Load BERT Summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# ‚úÖ Load NLP models\n",
    "nltk.download(\"punkt\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Processed Articles in ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Update articles in ChromaDB with processed data\n",
    "for article in news_articles:\n",
    "    collection.update(\n",
    "        ids=[article[\"url\"]],\n",
    "        metadatas=[{\n",
    "            \"title\": article[\"title\"],\n",
    "            \"url\": article[\"url\"],\n",
    "            \"published_date\": article[\"published_date\"],\n",
    "            \"source_name\": article[\"source_name\"],\n",
    "            \"author\": article[\"author\"],\n",
    "            \"category\": article[\"category\"],\n",
    "            \"status\": article[\"status\"],  # Mark as \"processed\"\n",
    "            \"linguistic_analysis\": article[\"linguistic_analysis\"]\n",
    "        }]\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Processed articles updated in ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE TO JSON for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Save processed articles to JSON\n",
    "with open(\"RAG_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_articles, f, indent=4)\n",
    "\n",
    "print(\"‚úÖ Processed articles saved to 'RAG_output.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle: Set to True for summaries, False for full content\n",
    "summary_mode = True  \n",
    "\n",
    "# Load cleaned dataset\n",
    "with open(\"cleaned_news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    news_articles = json.load(f)\n",
    "\n",
    "# Extract valid content\n",
    "documents = [article[\"content\"] for article in news_articles]\n",
    "\n",
    "### üîπ TF-IDF Outlier Analysis\n",
    "def tfidf_outliers(texts, top_n=5):\n",
    "    \"\"\"Finds top N high-TF-IDF words per article.\"\"\"\n",
    "    texts = [text if isinstance(text, str) else \"\" for text in texts]  # Ensure valid text\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    outlier_words = []\n",
    "    for row in tfidf_matrix:\n",
    "        scores = row.toarray()[0]\n",
    "        top_indices = scores.argsort()[-top_n:]\n",
    "        outlier_words.append([feature_names[i] for i in top_indices])\n",
    "\n",
    "    return outlier_words\n",
    "\n",
    "### üîπ Grammar & Readability Analysis\n",
    "def grammar_analysis(text):\n",
    "    \"\"\"Analyzes grammatical complexity and readability.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\"grammar_errors\": 0, \"sentence_count\": 0}  \n",
    "\n",
    "    doc = nlp(text)\n",
    "    errors = sum(1 for token in doc if token.is_oov)  \n",
    "    sentences = len(list(doc.sents))\n",
    "    return {\"grammar_errors\": errors, \"sentence_count\": sentences}\n",
    "\n",
    "### üîπ Sentiment Analysis\n",
    "def sentiment_analysis(text):\n",
    "    \"\"\"Detects sentiment polarity and emotional words.\"\"\"\n",
    "    analysis = TextBlob(text)\n",
    "    return {\"polarity\": analysis.sentiment.polarity, \"subjectivity\": analysis.sentiment.subjectivity}\n",
    "\n",
    "## Long Text summary using BERT\n",
    "\n",
    "def split_text(text, max_tokens=1024):\n",
    "    \"\"\"Splits long text into smaller chunks that fit within BERT's token limit.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunks, current_chunk = [], \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_tokens:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def generate_summary(text, max_length=80, min_length=20):\n",
    "    \"\"\"Generates a summary for large texts efficiently.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"No summary available.\"\n",
    "\n",
    "    # Step 1: Split the text into manageable chunks\n",
    "    text_chunks = split_text(text)\n",
    "\n",
    "    # Step 2: Summarize each chunk separately\n",
    "    chunk_summaries = [summarizer(chunk, max_length=100, min_length=20, do_sample=False)[0][\"summary_text\"] for chunk in text_chunks]\n",
    "\n",
    "    # Step 3: Merge chunk summaries and summarize the full summary\n",
    "    merged_summary = \" \".join(chunk_summaries)\n",
    "\n",
    "    # Step 4: Final summarization to keep it concise\n",
    "    final_summary = summarizer(merged_summary, max_length=100, min_length=20, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Apply linguistic analysis\n",
    "outliers = tfidf_outliers(documents)\n",
    "linguistic_features = []\n",
    "\n",
    "for i, text in enumerate(documents):\n",
    "    grammar = grammar_analysis(text)\n",
    "    sentiment = sentiment_analysis(text)\n",
    "    summary = generate_summary(text) if summary_mode else text  # creates and saves summary to .json\n",
    "\n",
    "    linguistic_features.append({\n",
    "        \"summary\": summary,\n",
    "        \"tfidf_outliers\": outliers[i],\n",
    "        \"grammar_errors\": grammar[\"grammar_errors\"],\n",
    "        \"sentence_count\": grammar[\"sentence_count\"],\n",
    "        \"sentiment_polarity\": sentiment[\"polarity\"],\n",
    "        \"sentiment_subjectivity\": sentiment[\"subjectivity\"],\n",
    "    })\n",
    "\n",
    "# Merge analysis into articles\n",
    "for i, article in enumerate(news_articles):\n",
    "    article[\"linguistic_analysis\"] = linguistic_features[i]\n",
    "\n",
    "# Save the updated dataset\n",
    "with open(\"RAG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_articles, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Linguistic analysis completed. Results saved in 'RAG_news.json'.\")\n",
    "print(f\"üîπ Summary Mode: {'ON' if summary_mode else 'OFF'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to cut down the Ling analysis - separating FULL TEXT to focus on it \\\n",
    "TESTING PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Content removed. Saved 16 entries in 'small_linguistic_news.json'.\n"
     ]
    }
   ],
   "source": [
    "# Load the linguistic analysis dataset\n",
    "with open(\"linguistic_news.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    news_articles = json.load(f)\n",
    "\n",
    "# Create a new list without 'content'\n",
    "filtered_articles = []\n",
    "\n",
    "for article in news_articles:\n",
    "    filtered_articles.append({\n",
    "        \"title\": article.get(\"title\", \"No Title\"),\n",
    "        \"linguistic_analysis\": article.get(\"linguistic_analysis\", {})\n",
    "    })\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(\"small_linguistic_news.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_articles, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Content removed. Saved {len(filtered_articles)} entries in 'small_linguistic_news.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
