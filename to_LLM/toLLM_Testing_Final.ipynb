{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FACT-CHECKING PROMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def construct_fact_checking_prompt(enriched_entry):\n",
    "#     \"\"\"\n",
    "#     Constructs a well-formatted prompt for fact-checking using DeepSeek LLM.\n",
    "    \n",
    "#     Args:\n",
    "#         enriched_entry (dict): A dictionary containing article metadata, enriched content, and linguistic analysis.\n",
    "    \n",
    "#     Returns:\n",
    "#         str: A formatted string prompt for the LLM.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Extract relevant fields from enriched_entry\n",
    "#     title = enriched_entry.get(\"title\", \"Unknown Title\")\n",
    "#     url = enriched_entry.get(\"url\", \"Unknown URL\")\n",
    "#     published_date = enriched_entry.get(\"published_date\", \"Unknown Date\")\n",
    "#     source_name = enriched_entry.get(\"source\", \"Unknown Source\")\n",
    "#     author = enriched_entry.get(\"author\", \"Unknown Author\")\n",
    "#     category = enriched_entry.get(\"category\", \"Unknown Category\")\n",
    "#     summary = enriched_entry.get(\"enriched_content\", \"No summary available\")[:500]  # Truncate if too long\n",
    "    \n",
    "#     tfidf_outliers = json.loads(enriched_entry.get(\"TF-IDF Outliers\", \"[]\"))  # Convert back to list\n",
    "#     tfidf_outliers_str = \", \".join(tfidf_outliers) if tfidf_outliers else \"None\"\n",
    "    \n",
    "#     grammar_errors = enriched_entry.get(\"Grammar Errors\", 0)\n",
    "#     sentence_count = enriched_entry.get(\"Sentence Count\", 0)\n",
    "#     sentiment_polarity = enriched_entry.get(\"Sentiment Polarity\", \"Unknown\")\n",
    "#     sentiment_subjectivity = enriched_entry.get(\"Sentiment Subjectivity\", \"Unknown\")\n",
    "#     fact_checking_summary = enriched_entry.get(\"fact_checking_summary\", \"No fact-checking data available.\")\n",
    "    \n",
    "#     # Construct the prompt\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
    "    \n",
    "#     üì∞ **Article Information:**\n",
    "#     - **Title:** {title}\n",
    "#     - **URL:** {url}\n",
    "#     - **Published Date:** {published_date}\n",
    "#     - **Source:** {source_name}\n",
    "#     - **Author:** {author}\n",
    "#     - **Category:** {category}\n",
    "\n",
    "#     üîπ **Article Summary (Extracted via AI):**\n",
    "#     \"{summary}\"\n",
    "    \n",
    "#     üìä **Linguistic Analysis:**\n",
    "#     - **TF-IDF Outlier Keywords (Unique/Unusual Words):** {tfidf_outliers_str}\n",
    "#     - **Grammar Issues:** {grammar_errors} errors\n",
    "#     - **Sentence Count:** {sentence_count}\n",
    "#     - **Sentiment Analysis:** \n",
    "#         - **Polarity (Scale -1 to 1):** {sentiment_polarity}\n",
    "#         - **Subjectivity (Scale 0 to 1, higher = opinionated):** {sentiment_subjectivity}\n",
    "\n",
    "#     üéØ **Your Task:**\n",
    "#     1Ô∏è‚É£ **Assess the credibility of this article** based on the provided content and linguistic analysis.  \n",
    "#     2Ô∏è‚É£ **Use the TF-IDF outlier words** to determine if the article contains **unusual phrasing or misleading language.**  \n",
    "#     3Ô∏è‚É£ **Analyze sentiment:** Does the emotional tone suggest bias, fear-mongering, or objectivity?  \n",
    "#     4Ô∏è‚É£ **Evaluate readability & grammar:** Is the article professionally written, or does it contain errors typical of misinformation?  \n",
    "#     5Ô∏è‚É£ **Compare against reliable sources** if possible, to determine factual accuracy.  \n",
    "    \n",
    "#     üèÜ **Final Response Format:**\n",
    "#     - **Credibility Score:** (Scale 0-100, where 100 = totally credible, 0 = completely false)\n",
    "#     - **Verdict:** (Choose one: \"True\", \"False\", or \"Misleading\")\n",
    "#     - **Explanation:** (2-3 sentences summarizing why you assigned this rating)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     return prompt\n",
    "\n",
    "# # Example Usage\n",
    "# example_entry = {\n",
    "#     \"title\": \"Breaking News: AI Solves World Hunger\",\n",
    "#     \"url\": \"https://news.example.com/ai-hunger\",\n",
    "#     \"published_date\": \"2025-02-18\",\n",
    "#     \"source\": \"Example News\",\n",
    "#     \"author\": \"John Doe\",\n",
    "#     \"category\": \"Technology\",\n",
    "#     \"enriched_content\": \"AI has made significant advancements... (summary content here)...\\n\\nFact-Checking Data:\\n- Verified by multiple sources\",\n",
    "#     \"TF-IDF Outliers\": json.dumps([\"AI\", \"breakthrough\", \"hunger crisis\"]),\n",
    "#     \"Grammar Errors\": 2,\n",
    "#     \"Sentence Count\": 25,\n",
    "#     \"Sentiment Polarity\": 0.7,\n",
    "#     \"Sentiment Subjectivity\": 0.4,\n",
    "#     \"fact_checking_summary\": \"Verified by multiple sources.\"\n",
    "# }\n",
    "\n",
    "# # Generate the prompt\n",
    "# prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "# print(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
      "\n",
      "        üì∞ **Article Information:**\n",
      "        - **Title:** Breaking News: AI Solves World Hunger\n",
      "        - **URL:** https://news.example.com/ai-hunger\n",
      "        - **Published Date:** 2025-02-18\n",
      "        - **Source:** Example News\n",
      "        - **Author:** John Doe\n",
      "        - **Category:** Technology\n",
      "\n",
      "        üîπ **Article Summary (Extracted via AI):**\n",
      "        \"AI has made significant advancements... (summary content here)...\n",
      "\n",
      "Fact-Checking Data:\n",
      "- Verified by multiple sources\"\n",
      "\n",
      "        üìä **Linguistic Analysis:**\n",
      "        - **TF-IDF Outlier Keywords (Unique/Unusual Words):** AI, breakthrough, hunger crisis\n",
      "        - **Grammar Issues:** 2 errors\n",
      "        - **Sentence Count:** 25\n",
      "        - **Sentiment Analysis:** \n",
      "            - **Polarity (Scale -1 to 1):** 0.7\n",
      "            - **Subjectivity (Scale 0 to 1, higher = opinionated):** 0.4\n",
      "\n",
      "        üéØ **Your Task:**\n",
      "        1Ô∏è‚É£ **Assess the credibility of this article** based on the provided content and linguistic analysis. \n",
      "        2Ô∏è‚É£ **Use the TF-IDF outlier words** to determine if the article contains **unusual phrasing or misleading language.** \n",
      "        3Ô∏è‚É£ **Analyze sentiment:** Does the emotional tone suggest bias, fear-mongering, or objectivity? \n",
      "        4Ô∏è‚É£ **Evaluate readability & grammar:** Is the article professionally written, or does it contain errors typical of misinformation? \n",
      "        5Ô∏è‚É£ **Compare against reliable sources** if possible, to determine factual accuracy. \n",
      "\n",
      "        üèÜ **Final Response Format:**\n",
      "        - **Credibility Score:** (Scale 0-100, where 100 = totally credible, 0 = completely false)\n",
      "        - **Verdict:** (Choose one: \"True\", \"False\", or \"Misleading\")\n",
      "        - **Explanation:** (2-3 sentences summarizing why you assigned this rating)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 10:45:58,443 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000021869403010>, 'Connection to us.i.posthog.com timed out. (connect timeout=15)')': /batch/\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "# Configure logging (good practice for debugging and monitoring)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def construct_fact_checking_prompt(enriched_entry):\n",
    "    \"\"\"Constructs a well-formatted prompt for fact-checking using an LLM.\n",
    "\n",
    "    Args:\n",
    "        enriched_entry (dict): A dictionary containing article metadata, \n",
    "                               enriched content, and linguistic analysis.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string prompt for the LLM, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        title = enriched_entry.get(\"title\", \"Unknown Title\")\n",
    "        url = enriched_entry.get(\"url\", \"Unknown URL\")\n",
    "        published_date = enriched_entry.get(\"published_date\", \"Unknown Date\")\n",
    "        source_name = enriched_entry.get(\"source\", \"Unknown Source\")\n",
    "        author = enriched_entry.get(\"author\", \"Unknown Author\")\n",
    "        category = enriched_entry.get(\"category\", \"Unknown Category\")\n",
    "\n",
    "        summary = enriched_entry.get(\"enriched_content\")\n",
    "        if summary is None:\n",
    "            logging.warning(\"No summary available in enriched_entry.\")\n",
    "            summary = \"No summary available\"\n",
    "        summary = summary[:500]  # Truncate if too long\n",
    "\n",
    "        tfidf_outliers_str = enriched_entry.get(\"TF-IDF Outliers\")\n",
    "        if tfidf_outliers_str is None:\n",
    "            logging.warning(\"No TF-IDF Outliers available in enriched_entry.\")\n",
    "            tfidf_outliers_str = \"None\"\n",
    "        else:\n",
    "            try:\n",
    "                tfidf_outliers = json.loads(tfidf_outliers_str)\n",
    "                tfidf_outliers_str = \", \".join(tfidf_outliers) if tfidf_outliers else \"None\"\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(\"Error decoding TF-IDF Outliers JSON.\")\n",
    "                tfidf_outliers_str = \"None\"  # Handle JSON decoding errors gracefully\n",
    "\n",
    "\n",
    "        grammar_errors = enriched_entry.get(\"Grammar Errors\", 0)\n",
    "        sentence_count = enriched_entry.get(\"Sentence Count\", 0)\n",
    "        sentiment_polarity = enriched_entry.get(\"Sentiment Polarity\", \"Unknown\")\n",
    "        sentiment_subjectivity = enriched_entry.get(\"Sentiment Subjectivity\", \"Unknown\")\n",
    "        fact_checking_summary = enriched_entry.get(\"fact_checking_summary\", \"No fact-checking data available.\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a fact-checking AI analyzing the credibility of a news article. Below is the structured information:\n",
    "\n",
    "        üì∞ **Article Information:**\n",
    "        - **Title:** {title}\n",
    "        - **URL:** {url}\n",
    "        - **Published Date:** {published_date}\n",
    "        - **Source:** {source_name}\n",
    "        - **Author:** {author}\n",
    "        - **Category:** {category}\n",
    "\n",
    "        üîπ **Article Summary (Extracted via AI):**\n",
    "        \"{summary}\"\n",
    "\n",
    "        üìä **Linguistic Analysis:**\n",
    "        - **TF-IDF Outlier Keywords (Unique/Unusual Words):** {tfidf_outliers_str}\n",
    "        - **Grammar Issues:** {grammar_errors} errors\n",
    "        - **Sentence Count:** {sentence_count}\n",
    "        - **Sentiment Analysis:** \n",
    "            - **Polarity (Scale -1 to 1):** {sentiment_polarity}\n",
    "            - **Subjectivity (Scale 0 to 1, higher = opinionated):** {sentiment_subjectivity}\n",
    "\n",
    "        üéØ **Your Task:**\n",
    "        1Ô∏è‚É£ **Assess the credibility of this article** based on the provided content and linguistic analysis. \n",
    "        2Ô∏è‚É£ **Use the TF-IDF outlier words** to determine if the article contains **unusual phrasing or misleading language.** \n",
    "        3Ô∏è‚É£ **Analyze sentiment:** Does the emotional tone suggest bias, fear-mongering, or objectivity? \n",
    "        4Ô∏è‚É£ **Evaluate readability & grammar:** Is the article professionally written, or does it contain errors typical of misinformation? \n",
    "        5Ô∏è‚É£ **Compare against reliable sources** if possible, to determine factual accuracy. \n",
    "\n",
    "        üèÜ **Final Response Format:**\n",
    "        - **Credibility Score:** (Scale 0-100, where 100 = totally credible, 0 = completely false)\n",
    "        - **Verdict:** (Choose one: \"True\", \"False\", or \"Misleading\")\n",
    "        - **Explanation:** (2-3 sentences summarizing why you assigned this rating)\n",
    "        \"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error constructing prompt: {e}\")\n",
    "        return None  # Indicate failure by returning None\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage (corrected)\n",
    "example_entry = {\n",
    "    \"title\": \"Breaking News: AI Solves World Hunger\",\n",
    "    \"url\": \"https://news.example.com/ai-hunger\",\n",
    "    \"published_date\": \"2025-02-18\",\n",
    "    \"source\": \"Example News\",\n",
    "    \"author\": \"John Doe\",\n",
    "    \"category\": \"Technology\",\n",
    "    \"enriched_content\": \"AI has made significant advancements... (summary content here)...\\n\\nFact-Checking Data:\\n- Verified by multiple sources\",\n",
    "    \"TF-IDF Outliers\": json.dumps([\"AI\", \"breakthrough\", \"hunger crisis\"]),  # Use json.dumps here\n",
    "    \"Grammar Errors\": 2,\n",
    "    \"Sentence Count\": 25,\n",
    "    \"Sentiment Polarity\": 0.7,\n",
    "    \"Sentiment Subjectivity\": 0.4,\n",
    "    \"fact_checking_summary\": \"Verified by multiple sources.\"\n",
    "}\n",
    "\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "\n",
    "if prompt_text:\n",
    "    print(prompt_text)\n",
    "else:\n",
    "    print(\"Failed to generate prompt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, I'm looking at this article about AI solving world hunger. The title is pretty catchy and promising, which makes me cautious because breakthroughs in such a critical area aren't common. The URL points to an example site from 2025, so it's up-to-date but not tied to any real source. The author is John Doe, who isn't well-known, so that‚Äôs another red flag.\n",
      "\n",
      "The summary says AI has made significant advancements, which makes sense given the technology trend of recent years. However, the article is from 2025, and while AI has advanced, solving world hunger on its own seems a bit far-fetched. It's more likely an announcement or a hypothetical scenario rather than a proven solution.\n",
      "\n",
      "The linguistic analysis shows some grammar errors, which might indicate it wasn't proofread. The TF-IDF outlier words like \"AI,\" \"breakthrough,\" and \"hunger crisis\" suggest the article is trying to emphasize these key points but could be overusing them to create drama or sensationalism. \n",
      "\n",
      "The sentiment score of 0.7 on polarity is positive, which might mean the writer is optimistic, perhaps to encourage investment in AI. The subjectivity score at 0.4 isn't too high, so it doesn't seem like it's biased against any group, but it could still be biased towards AI benefits over other solutions.\n",
      "\n",
      "The readability issues and grammar errors make me think this might not have been fact-checked thoroughly or published by a reputable source. Without comparing it to reliable news, I can't confirm the facts, but the structure seems fishy with too much focus on AI and lack of context about how exactly it's supposed to solve hunger.\n",
      "\n",
      "So, putting it all together, the article is catchy, has some errors, uses strong words to emphasize points, positive tone that might be biased, and doesn‚Äôt seem credible without further evidence.\n",
      "</think>\n",
      "\n",
      "**Final Response:**\n",
      "\n",
      "- **Credibility Score:** 20/100  \n",
      "- **Verdict:** False/Misleading  \n",
      "- **Explanation:** The article is alarmingly sensational and may aim to stir fear or promote investment in AI over traditional solutions. While the use of strong words like \"breakthrough\" and \"AI\" suggests intentionality, the lack of concrete evidence and grammar errors raise significant doubts about its factual accuracy. Given these factors, it's more plausible that this article is misleading rather than truthful.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Generate the fact-checking prompt\n",
    "prompt_text = construct_fact_checking_prompt(example_entry)\n",
    "\n",
    "# Send the prompt to DeepSeek LLM using Ollama\n",
    "response = ollama.chat(\n",
    "    model=\"deepseek-r1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Presidents Are Often Judged By History Through The Lens Of Morality\",\n",
      "        \"url\": \"https://newsone.com/5939034/presidents-are-judged-by-history-through-the-lens-of-morality/\",\n",
      "        \"published_date\": \"2025-02-17T14:33:46+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"George R. Goethals, University of Richmond\",\n",
      "        \"fact_check_result\": \"<think>\\nAlright, let's try to figure out the credibility of this article. The title is about presidents being judged by history through morality, which sounds pretty heavy-handed. I'm not sure if that's a common way historians look at things.\\n\\nThe URL points to newsone.com with an ID, but I don't know much about them. The author is George R. Goethals from the University of Richmond, so maybe he has some academic background. But the category is just general, which doesn't give me much info.\\n\\nLooking at the summary, it's cut off mid-sentence and only mentions Lincoln and Biden. It seems like the article might be about evaluating past presidents' legacies. The author probably discussed how historical perspectives shape our view of leaders, possibly in a scholarly context.\\n\\nThe linguistic analysis shows no grammar errors or unusual words. So, it looks clean. But without knowing what the rest of the article says, it's hard to tell if there are any hidden issues like misinformation or bias.\\n\\nI'm not sure about Goethals' specific stance on morality and history. If he argues that moral judgment is subjective in shaping historical assessments, that could be an interesting point but might also be a controversial view among historians who see more objectivity based on facts rather than morals.\\n\\nThe readability seems okay for someone with some academic background, but without the full text or context from reliable sources like major news outlets, it's tough to judge if it's credible. If Goethals is introducing an unconventional viewpoint that isn't widely accepted in historical studies, then the article might be misleading.\\n\\nSo, putting it all together, I think there's some basis for evaluating credibility because the approach seems unique but without more context or sources, it's hard to say definitively.\\n</think>\\n\\n**Final Response:**\\n\\n- **Credibility Score:** 60\\n- **Verdict:** Misleading\\n- **Explanation:** The article introduces an unconventional perspective on how presidents are judged through historical morality, which may not be widely accepted in academic circles. Without full context or sources from reputable outlets, it's challenging to assess factual accuracy conclusively.\\n\\nThe author presents an interesting viewpoint but the lack of supporting evidence from credible sources raises concerns about the article's reliability.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"The Prince and Princess of Wales' ¬£33k-per-week Mustique villa has a 60ft pool\",\n",
      "        \"url\": \"https://www.hellomagazine.com/travel/814676/kate-middleton-prince-william-villa-antilles-mustique/\",\n",
      "        \"published_date\": \"2025-02-17T14:33:35+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Georgia Brown\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so I need to assess the credibility of a news article based on the information provided. Let's go through each task step by step.\\n\\nFirst, looking at the article title: \\\"The Prince and Princess of Wales' ¬£33k-per-week Mustique villa has a 60ft pool.\\\" That sounds plausible enough‚ÄîÊä•ÈÅì about a luxury property owned by the couple in Mustique. The URL is given but it's from Hellomagazine.com, which I know occasionally covers celebrity news and travel, so that might be a reliable source.\\n\\nThe article summary says \\\"Error: Page returned status code 404,\\\" which means when I tried to access the link, it couldn't find the page. This could mean two things: either the article is no longer available on that website or there was an error in the URL provided. Since the user didn't provide a working link, I can‚Äôt check the actual content for accuracy.\\n\\nMoving on to linguistic analysis. The TF-IDF outlier keywords are none, so all the words seem relevant and not unusual. There were no grammar errors mentioned, which is good because it suggests that if the article exists, it's well-written without obvious mistakes.\\n\\nThe sentiment analysis shows polarity as unknown and subjectivity as unknown. Without specific text, I can't determine if there's positive or negative bias. Maybe the article is neutral in presenting facts.\\n\\nLooking at readability and grammar: The user mentioned no issues here, but since the summary was an error, it's hard to assess based on the given info. Professional writing usually has clear sentences, proper structure, and correct grammar. Without content, I can't evaluate this.\\n\\nLastly, comparing against reliable sources isn't possible because the URL didn't work. However, knowing that Hellomagazine is a reputable source for travel and celebrity news gives some weight to the credibility if the page exists.\\n\\nPutting it all together: The article's URL doesn‚Äôt work, so without seeing the actual content, I can‚Äôt confirm its factual accuracy. But considering the source's reputation and assuming the information in the title makes sense (a known couple owning a pricey villa with a pool), there's some basis for believing it could be true. However, because I can't verify, I'll have to assign a lower credibility score.\\n\\nI think the credibility might be on the lower side because without access to the article, I'm relying solely on the provided title and source's general reputation. There's also the possibility that the source might be incorrect or the information outdated since it was published in 2025. \\n\\nSo, my final assessment would rate this as somewhat credible but with reservations due to the lack of verifiable content.\\n</think>\\n\\n**Final Answer:**\\n\\n- **Credibility Score:** 60\\n- **Verdict:** False\\n- **Explanation:** The article's credibility is questionable because accessing its source resulted in a 404 error, suggesting the page may not exist. Although the source is reputable for travel and celebrity news, without verifying the content directly, it's uncertain whether the information is accurate or if there are misinformation elements present.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Scottish museum to undergo 'bold transformation' after ¬£2.6m government funding boost\",\n",
      "        \"url\": \"https://www.scotsman.com/news/scottish-news/scottish-museum-to-undergo-bold-transformation-after-ps26m-government-funding-boost-4994958\",\n",
      "        \"published_date\": \"2025-02-17T14:32:22+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Jane Bradley\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so I need to assess the credibility of this news article about a Scottish museum undergoing transformation with government funding. Let me go through each step carefully.\\n\\nFirst, looking at the title: it says \\\"bold transformation\\\" and mentions \\\"government funding boost.\\\" The source is unknown, which might be a red flag because credible sources are usually known. Jane Bradley is the author, but without more info on her credentials, I can't say much about her reliability.\\n\\nThe summary from AI just shows an error message, so it's not helpful for content analysis. The linguistic analysis says there are no TF-IDF outlier keywords or grammar issues. So, it looks clean in that aspect. No sentences to evaluate either.\\n\\nFor sentiment, polarity is unknown because the text isn't here, but subjectivity is also unknown. That means we don't know if it's biased or just stating facts.\\n\\nReadability and grammar are both at zero errors, so no typos or formatting issues. The article seems professionally written on the surface.\\n\\nComparing to reliable sources like the Scotsman often provides context. If this article from 2025 is about a museum renovation happening now, it might be outdated or fake. Also, government funding for transformations could be a common practice, so without more specifics, I can't confirm if it's credible.\\n\\nPutting it all together: The article itself looks okay on the surface, but since the source is unknown and published in 2025, it seems suspicious. Without concrete evidence or comparison from reliable sources, I have to assume some level of distrust.\\n</think>\\n\\n**Final Response:**\\n\\n- **Credibility Score:** 30\\n- **Verdict:** False/Misleading\\n- **Explanation:** The article's credibility is questionable due to the unknown source and publication date. Without corroborating information from a reputable source, it's difficult to assess its accuracy. The mention of government funding for a renovation aligns with common public projects but lacks specific details that would confirm its truthfulness.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"The Lathums bring forward release of new album ‚ÄòMatter Does Not Define‚Äô\",\n",
      "        \"url\": \"https://www.nme.com/news/music/the-lathums-bring-forward-release-of-new-album-matter-does-not-define-3838653?utm_source=rss&utm_medium=rss&utm_campaign=the-lathums-bring-forward-release-of-new-album-matter-does-not-define\",\n",
      "        \"published_date\": \"2025-02-17T14:32:14+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Laura Molloy\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so I need to figure out how credible the article about The Lathums releasing their new album is. Let's break it down step by step.\\n\\nFirst, looking at the title and URL: It says they're bringing forward the release of \\\"Matter Does Not Define\\\" from March 7 to February 28. That seems plausible but a bit sudden without more info on why they pulled the release. Maybe they had some issues or wanted to push it back?\\n\\nThe summary mentions that after pulling some strings, their new album is out in February with an Instagram post expressing excitement. The use of \\\"pulled a lot of strings\\\" sounds like they're manipulating time or media for attention, which could be a red flag.\\n\\nLinguistic analysis shows no grammar errors and no TF-IDF outlier words, so the writing looks professional. But the lack of sentence count makes it hard to assess coherence. However, since there are no issues mentioned here, maybe the article is trying to hide something by not providing more details.\\n\\nSentiment-wise, they're excited but didn't specify if this album is better than previous ones or why it's being released earlier. Without context on their music style or public reception, it's hard to gauge bias or objectivity.\\n\\nReadability and grammar are good, no errors spotted. But the lack of detailed information makes me skeptical. Reliable sources often provide more context about album releases, so missing that info is a concern.\\n\\nComparing against reliable sources, if I check The Guardian or Pitchfork for similar announcements, they usually give more details about why an album is delayed or advanced. Since this article doesn't mention any issues and just changes the release date abruptly, it's hard to verify its accuracy without more context.\\n\\nSo, putting it all together: the article mentions a new album with excitement but lacks detailed reasons for the change in release date and doesn't provide necessary context about their music or why fans should care. This makes me cautious that the information might be incomplete or misleading.\\n</think>\\n\\n**Final Response:**\\n\\n- **Credibility Score:** 60  \\n- **Verdict:** False  \\n- **Explanation:** The article is intriguing but lacks crucial details such as the reasons behind the album's delayed release and context about The Lathums' music. Without this information, it's challenging to assess the accuracy of the release date change compared to reliable sources, which often provide more context on album announcements.\\n\\nThe article's excitement tone without necessary context makes it misleading, as fans might not have adequate reasons to believe in the new album's quality or timing.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Trump begins firings of FAA air traffic control staff just weeks after fatal DC plane crash\",\n",
      "        \"url\": \"https://www.toledoblade.com/news/nation/2025/02/17/trump-begins-firings-of-faa-air-traffic-control-staff-after-fatal-dc-plane-crash/stories/20250217084\",\n",
      "        \"published_date\": \"2025-02-17T14:31:22+00:00\",\n",
      "        \"source\": \"Unknown source\",\n",
      "        \"author\": \"Unknown author\",\n",
      "        \"fact_check_result\": \"<think>\\nOkay, so I'm looking at this news article about Trump firing FAA air traffic control staff right after a fatal DC plane crash. First off, the title says something pretty alarming: Trump is beginning firings just weeks after a crash that resulted in deaths.\\n\\nThe URL points to a Toledoblade.com article from February 17, 2025. I don't recognize this source as reliable because it's not one of the major newspapers or well-known outlets. That already makes me cautious because lesser-known sources are often less fact-checked and could be spreading misinformation.\\n\\nThe summary provided by the AI is a bit odd with all those strange text codes, which might mean the summary wasn't properly extracted from the actual article. It looks like it's some kind of formatting error or maybe the user pasted the wrong content. The actual content isn't clear, so I can't really assess its credibility based on that.\\n\\nLooking at the linguistic analysis, there are no TF-IDF outlier keywords, which probably means all the important words used in a normal article aren't flagging anything unusual here. Also, there are no grammar issues or sentence count problems, suggesting it's well-written without any obvious errors. However, the lack of specific content makes this part not very helpful.\\n\\nFor sentiment analysis, both polarity and subjectivity are unknown because the text is unclear. If I could read between the lines, maybe there's some fear-mongering about job losses or political instability, but without clear language, it's hard to tell if that's the case or just speculation based on the triggering words like \\\"firings.\\\"\\n\\nReadability-wise, since the summary seems garbled and doesn't provide real information, it probably isn't professionally written. It likely contains errors typical of misinformation, such as mixing unrelated facts about a crash with policy changes.\\n\\nComparing against reliable sources, I know that Trump hasn't fired FAA staff in recent history, especially after a crash involving crashed planes at major airports like Chicago O'Hare and LaGuardia. This specific event is extremely rare and doesn't align with known actions of the president. Reliable news outlets would likely explain any connection between policy changes and safety incidents clearly.\\n\\nSo putting it all together, this article seems to be spreading misinformation through unclear text and lack of evidence. The credibility score should reflect that it's not trustworthy.\\n</think>\\n\\n**Credibility Score:** 10  \\n**Verdict:** False  \\n**Explanation:** The article is based on unclear and improperly formatted content extracted from an unidentified source, likely a pseudonym or unknown outlet. It mixes unrelated political actions with data about the DC flight crash in a manner that appears fabricated or misleading without supporting evidence. Reliable sources would not conflate such distinct events, indicating a lack of factual basis. The narrative lacks clarity, credibility, and substantial detail to be considered true.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "import chromadb\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_entries_from_chromadb(collection_name=\"news_articles\", max_entries=5):\n",
    "    # ... (No changes to this function)\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")  # Update path as needed\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "\n",
    "    results = collection.get(include=[\"metadatas\", \"documents\"], limit=max_entries)\n",
    "\n",
    "    entries = []\n",
    "    for i in range(len(results[\"documents\"])):\n",
    "        entry = {\n",
    "            \"title\": results[\"metadatas\"][i].get(\"title\", \"Unknown Title\"),\n",
    "            \"url\": results[\"metadatas\"][i].get(\"url\", \"Unknown URL\"),\n",
    "            \"published_date\": results[\"metadatas\"][i].get(\"published_date\", \"Unknown Date\"),\n",
    "            \"source\": results[\"metadatas\"][i].get(\"source\", \"Unknown Source\"),\n",
    "            \"author\": results[\"metadatas\"][i].get(\"author\", \"Unknown Author\"),\n",
    "            \"category\": results[\"metadatas\"][i].get(\"category\", \"Unknown Category\"),\n",
    "            \"enriched_content\": results[\"documents\"][i],\n",
    "            \"TF-IDF Outliers\": json.dumps(results[\"metadatas\"][i].get(\"tfidf_outliers\", [])),\n",
    "            \"Grammar Errors\": results[\"metadatas\"][i].get(\"grammar_errors\", 0),\n",
    "            \"Sentence Count\": results[\"metadatas\"][i].get(\"sentence_count\", 0),\n",
    "            \"Sentiment Polarity\": results[\"metadatas\"][i].get(\"sentiment_polarity\", \"Unknown\"),\n",
    "            \"Sentiment Subjectivity\": results[\"metadatas\"][i].get(\"sentiment_subjectivity\", \"Unknown\"),\n",
    "            \"fact_checking_summary\": results[\"metadatas\"][i].get(\"fact_checking_summary\", \"No fact-checking data available.\"),\n",
    "        }\n",
    "        entries.append(entry)\n",
    "\n",
    "    return entries\n",
    "\n",
    "def analyze_multiple_articles(chroma_entries, model=\"deepseek-r1\", output_dir=\"PROMT_RESULTS\"):  # Changed output_file to output_dir\n",
    "    responses = []\n",
    "\n",
    "    for entry in chroma_entries:\n",
    "        prompt_text = construct_fact_checking_prompt(entry)\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        )\n",
    "        response_content = response['message']['content']\n",
    "        responses.append({\n",
    "            \"title\": entry.get(\"title\", \"Unknown Title\"),\n",
    "            \"url\": entry.get(\"url\", \"Unknown URL\"),\n",
    "            \"published_date\": entry.get(\"published_date\", \"Unknown Date\"),\n",
    "            \"source\": entry.get(\"source\", \"Unknown Source\"),\n",
    "            \"author\": entry.get(\"author\", \"Unknown Author\"),\n",
    "            \"fact_check_result\": response_content\n",
    "        })\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "     # Generate filename with specified format\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    json_files = [f for f in os.listdir(output_dir) if f.startswith(f\"Prompt_Return_{date_str}\")] # Modified to match the new file name format\n",
    "    file_number = len(json_files) + 1\n",
    "    filename = f\"Prompt_Return_{date_str}_{file_number}.json\"  # Modified filename prefix\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    while os.path.exists(filepath): # Check if the file exists and increment counter\n",
    "        counter += 1\n",
    "        filename = f\"Prompt_Return_{timestamp}_{counter}.json\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Save responses to a JSON file\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(responses, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "# ** Fetch limited entries from ChromaDB and send them for analysis **\n",
    "chroma_entries = fetch_entries_from_chromadb(max_entries=5)\n",
    "results = analyze_multiple_articles(chroma_entries)\n",
    "\n",
    "# Output results (optional - you might just want to save to file)\n",
    "print(json.dumps(results, indent=4, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
